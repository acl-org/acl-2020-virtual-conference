[{"card_image_alt_text":"A representative figure from paper main.173","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.173.png","content":{"abstract":"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.","authors":["Joshua Maynez","Shashi Narayan","Bernd Bohnet","Ryan McDonald"],"demo_url":"","keywords":["Abstractive Summarization","likelihood objectives","open-ended tasks","language modeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.173.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.173","similar_paper_uids":["main.173","main.454","main.457","main.458","main.124"],"title":"On Faithfulness and Factuality in Abstractive Summarization","tldr":"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have a...","track":"Summarization"},"forum":"main.173","id":"main.173","presentation_id":"38929071"},{"card_image_alt_text":"A representative figure from paper main.172","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.172.png","content":{"abstract":"Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients\u2019 welfare.","authors":["Sajad Sotudeh Gharebagh","Nazli Goharian","Ross Filice"],"demo_url":"","keywords":["Content Selection","Clinical Summarization","text task","content problem"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.172.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.172","similar_paper_uids":["main.172","main.228","main.705","main.556","main.124"],"title":"Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization","tldr":"Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the con...","track":"Summarization"},"forum":"main.172","id":"main.172","presentation_id":"38929326"},{"card_image_alt_text":"A representative figure from paper main.175","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.175.png","content":{"abstract":"The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input. We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review. At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.","authors":["Reinald Kim Amplayo","Mirella Lapata"],"demo_url":"","keywords":["Unsupervised Summarization","supervised models","abstractive summarization","Noising"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.175.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.175","similar_paper_uids":["main.175","main.461","main.513","main.457","main.555"],"title":"Unsupervised Opinion Summarization with Noising and Denoising","tldr":"The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (...","track":"Summarization"},"forum":"main.175","id":"main.175","presentation_id":"38928750"},{"card_image_alt_text":"A representative figure from paper main.174","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.174.png","content":{"abstract":"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.","authors":["Pinelopi Papalampidi","Frank Keller","Lea Frermann","Mirella Lapata"],"demo_url":"","keywords":["Screenplay Summarization","summarization","general-purpose models","position heuristics"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.174.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.174","similar_paper_uids":["main.174","main.453","main.451","main.173","main.556"],"title":"Screenplay Summarization Using Latent Narrative Structure","tldr":"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from th...","track":"Summarization"},"forum":"main.174","id":"main.174","presentation_id":"38928784"},{"card_image_alt_text":"A representative figure from paper main.460","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.460.png","content":{"abstract":"This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.","authors":["Philippe Laban","Andrew Hsi","John Canny","Marti A. Hearst"],"demo_url":"","keywords":["unsupervised summarization","coverage model","unsupervised procedure","fluency model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.460.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.460","similar_paper_uids":["main.460","main.452","main.175","main.454","main.458"],"title":"The Summary Loop: Learning to Write Abstractive Summaries Without Examples","tldr":"This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the orig...","track":"Summarization"},"forum":"main.460","id":"main.460","presentation_id":"38929183"},{"card_image_alt_text":"A representative figure from paper main.461","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.461.png","content":{"abstract":"Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the \u201camount of novelty\u201d going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (\u201cdecoder\u201d) has direct access to the text of input reviews through the pointer-generator mechanism. Experiments on Amazon and Yelp datasets, show that setting at test time the review\u2019s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.","authors":["Arthur Bra\u017einskas","Mirella Lapata","Ivan Titov"],"demo_url":"","keywords":["Unsupervised Summarization","Copycat-Review Generation","Opinion summarization","automatically summaries"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.461.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.461","similar_paper_uids":["main.461","main.175","main.513","main.555","main.173"],"title":"Unsupervised Opinion Summarization as Copycat-Review Generation","tldr":"Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selec...","track":"Summarization"},"forum":"main.461","id":"main.461","presentation_id":"38928963"},{"card_image_alt_text":"A representative figure from paper main.449","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.449.png","content":{"abstract":"Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available (https://github.com/wasiahmad/NeuralCodeSum) to facilitate future research.","authors":["Wasi Ahmad","Saikat Chakraborty","Baishakhi Ray","Kai-Wei Chang"],"demo_url":"","keywords":["Source Summarization","summarization","ablation studies","Transformer-based Approach"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.449.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.449","similar_paper_uids":["main.449","main.538","main.443","main.135","main.552"],"title":"A Transformer-based Approach for Source Code Summarization","tldr":"Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range depend...","track":"Summarization"},"forum":"main.449","id":"main.449","presentation_id":"38929076"},{"card_image_alt_text":"A representative figure from paper main.459","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.459.png","content":{"abstract":"This paper describes the {Critical Role Dungeons and Dragons Dataset} ({CRD3}) and related analyses. {Critical Role} is an unscripted, live-streamed show where a fixed group of people play {Dungeons and Dragons}, an open-ended role-playing game. The dataset is collected from 159 {Critical Role} episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the {Fandom} wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. {I}n addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural {ML} approaches, and we provide an abstractive summarization benchmark and evaluation.","authors":["Revanth Rameshkumar","Peter Bailey"],"demo_url":"","keywords":["abstractive evaluation","CRD3","open-ended game","data method"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.459.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.459","similar_paper_uids":["main.459","main.44","main.552","main.556","main.454"],"title":"Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset","tldr":"This paper describes the {Critical Role Dungeons and Dragons Dataset} ({CRD3}) and related analyses. {Critical Role} is an unscripted, live-streamed show where a fixed group of people play {Dungeons and Dragons}, an open-ended role-playing game. The ...","track":"Summarization"},"forum":"main.459","id":"main.459","presentation_id":"38928758"},{"card_image_alt_text":"A representative figure from paper main.458","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.458.png","content":{"abstract":"Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.","authors":["Yuhao Zhang","Derek Merck","Emily Tsai","Christopher D. Manning","Curtis Langlotz"],"demo_url":"","keywords":["Summarizing Reports","real-world applications","summarization reports","Neural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.458.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.458","similar_paper_uids":["main.458","main.454","main.173","main.450","main.452"],"title":"Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports","tldr":"Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we d...","track":"Summarization"},"forum":"main.458","id":"main.458","presentation_id":"38928902"},{"card_image_alt_text":"A representative figure from paper main.455","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.455.png","content":{"abstract":"Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).","authors":["Xinnuo Xu","Ond\u0159ej Du\u0161ek","Jingyi Li","Verena Rieser","Ioannis Konstas"],"demo_url":"","keywords":["Fact-based Weighting","Evaluating Summarisation","Abstractive Summarisation","fact-level weighting"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.455.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.455","similar_paper_uids":["main.455","main.454","main.124","main.450","main.333"],"title":"Fact-based Content Weighting for Evaluating Abstractive Summarisation","tldr":"Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the ...","track":"Summarization"},"forum":"main.455","id":"main.455","presentation_id":"38929274"},{"card_image_alt_text":"A representative figure from paper main.125","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.125.png","content":{"abstract":"Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. In this work, we propose a Transformer-based model to enhance the copy mechanism. Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer. We use the centrality of each source word to guide the copy process explicitly. Experimental results show that the self-attention graph provides useful guidance for the copy distribution. Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.","authors":["Song Xu","Haoran Li","Peng Yuan","Youzheng Wu","Xiaodong He","Bowen Zhou"],"demo_url":"","keywords":["Abstractive Summarization","copy mechanism","copy process","copy distribution"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.125.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.125","similar_paper_uids":["main.125","main.121","main.411","main.521","main.687"],"title":"Self-Attention Guided Copy Mechanism for Abstractive Summarization","tldr":"Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while h...","track":"Summarization"},"forum":"main.125","id":"main.125","presentation_id":"38929451"},{"card_image_alt_text":"A representative figure from paper main.124","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.124.png","content":{"abstract":"We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.","authors":["Yang Gao","Wei Zhao","Steffen Eger"],"demo_url":"","keywords":["Multi-Document Summarization","SUPERT","contextualized embeddings","soft techniques"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.124.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.124","similar_paper_uids":["main.124","main.445","main.4","main.173","demo.69"],"title":"SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization","tldr":"We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuri...","track":"Summarization"},"forum":"main.124","id":"main.124","presentation_id":"38929051"},{"card_image_alt_text":"A representative figure from paper main.454","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.454.png","content":{"abstract":"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.","authors":["Esin Durmus","He He","Mona Diab"],"demo_url":"","keywords":["Faithfulness Assessment","Abstractive Summarization","evaluating summary","reading comprehension"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.454.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.454","similar_paper_uids":["main.454","main.173","main.450","main.458","main.386"],"title":"FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization","tldr":"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a g...","track":"Summarization"},"forum":"main.454","id":"main.454","presentation_id":"38929353"},{"card_image_alt_text":"A representative figure from paper main.456","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.456.png","content":{"abstract":"Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.","authors":["Di Jin","Zhijing Jin","Joey Tianyi Zhou","Lisa Orii","Peter Szolovits"],"demo_url":"","keywords":["Stylistic Generation","summarization tasks","automatic evaluation","summarization systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.456.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.456","similar_paper_uids":["main.456","main.123","demo.96","main.639"],"title":"Hooks in the Headline: Learning to Generate Headlines with Controlled Styles","tldr":"Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style...","track":"Summarization"},"forum":"main.456","id":"main.456","presentation_id":"38929352"},{"card_image_alt_text":"A representative figure from paper main.457","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.457.png","content":{"abstract":"Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders---a sequential document encoder and a graph-structured encoder---to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.","authors":["Luyang Huang","Lingfei Wu","Lu Wang"],"demo_url":"","keywords":["Knowledge Summarization","abstractive summarization","semantic interpretation","generation summaries"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.457.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.457","similar_paper_uids":["main.457","main.173","main.555","main.175","main.454"],"title":"Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward","tldr":"Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summa...","track":"Summarization"},"forum":"main.457","id":"main.457","presentation_id":"38929235"},{"card_image_alt_text":"A representative figure from paper main.453","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.453.png","content":{"abstract":"We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.","authors":["Faisal Ladhak","Bryan Li","Yaser Al-Onaizan","Kathy McKeown"],"demo_url":"","keywords":["Exploring Selection","Content Selection","Summarization Chapters","summarization task"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.453.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.453","similar_paper_uids":["main.453","main.445","main.174","main.452","main.173"],"title":"Exploring Content Selection in Summarization of Novel Chapters","tldr":"We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and...","track":"Summarization"},"forum":"main.453","id":"main.453","presentation_id":"38929346"},{"card_image_alt_text":"A representative figure from paper main.123","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.123.png","content":{"abstract":"Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines.","authors":["Kazuki Matsumaru","Sho Takase","Naoaki Okazaki"],"demo_url":"","keywords":["Truthfulness Generation","abstractive summarization","headline generation","automatic headlines"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.123.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.123","similar_paper_uids":["main.123","main.456","main.454","main.458","main.173"],"title":"Improving Truthfulness of Headline Generation","tldr":"Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text....","track":"Summarization"},"forum":"main.123","id":"main.123","presentation_id":"38929335"},{"card_image_alt_text":"A representative figure from paper main.122","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.122.png","content":{"abstract":"Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task. In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets.","authors":["Demian Gholipour Ghalandari","Georgiana Ifrim"],"demo_url":"","keywords":["News Summarization","automatic summarization","automatic TLS","date selection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.122.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.122","similar_paper_uids":["main.122","main.268","tacl.1853","main.480","main.710"],"title":"Examining the State-of-the-Art in News Timeline Summarization","tldr":"Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection...","track":"Summarization"},"forum":"main.122","id":"main.122","presentation_id":"38929106"},{"card_image_alt_text":"A representative figure from paper main.452","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.452.png","content":{"abstract":"Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.","authors":["Raphael Schumann","Lili Mou","Yao Lu","Olga Vechtomova","Katja Markert"],"demo_url":"","keywords":["Unsupervised Summarization","Word-Level Extraction","Automatic summarization","Discrete Optimization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.452.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.452","similar_paper_uids":["main.452","main.460","main.458","main.453","main.173"],"title":"Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction","tldr":"Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two ...","track":"Summarization"},"forum":"main.452","id":"main.452","presentation_id":"38929184"},{"card_image_alt_text":"A representative figure from paper main.450","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.450.png","content":{"abstract":"Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced ``kags''), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github.com/W4ngatang/qags.","authors":["Alex Wang","Kyunghyun Cho","Mike Lewis"],"demo_url":"","keywords":["summarization","automatic protocol","automatically text","abstractive models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.450.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.450","similar_paper_uids":["main.450","main.454","main.413","main.19","main.20"],"title":"Asking and Answering Questions to Evaluate the Factual Consistency of Summaries","tldr":"Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (p...","track":"Summarization"},"forum":"main.450","id":"main.450","presentation_id":"38929318"},{"card_image_alt_text":"A representative figure from paper main.120","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.120.png","content":{"abstract":"Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP), which provides concise and neutral human-written summaries of news events, with links to external source articles. We also automatically extend these source articles by looking for related articles in the Common Crawl archive. We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.","authors":["Demian Gholipour Ghalandari","Chris Hokamp","Nghia The Pham","John Glover","Georgiana Ifrim"],"demo_url":"","keywords":["Multi-document summarization","story clustering","presentation results","timeline generation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.120.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.120","similar_paper_uids":["main.120","demo.58","main.478","main.331","main.555"],"title":"A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal","tldr":"Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, the...","track":"Summarization"},"forum":"main.120","id":"main.120","presentation_id":"38929109"},{"card_image_alt_text":"A representative figure from paper main.121","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.121.png","content":{"abstract":"Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.","authors":["Junnan Zhu","Yu Zhou","Jiajun Zhang","Chengqing Zong"],"demo_url":"","keywords":["Translate Summarize","Neural Summarization","Cross-lingual summarization","Chinese-to-English tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.121.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.121","similar_paper_uids":["main.121","main.125","main.554","main.153","main.581"],"title":"Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization","tldr":"Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual...","track":"Summarization"},"forum":"main.121","id":"main.121","presentation_id":"38928726"},{"card_image_alt_text":"A representative figure from paper main.451","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.451.png","content":{"abstract":"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.","authors":["Jiacheng Xu","Zhe Gan","Yu Cheng","Jingjing Liu"],"demo_url":"","keywords":["Discourse-Aware Summarization","document encoding","extractive selection","text models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.451.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.451","similar_paper_uids":["main.451","main.555","main.552","main.141","main.553"],"title":"Discourse-Aware Neural Extractive Text Summarization","tldr":"Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies...","track":"Summarization"},"forum":"main.451","id":"main.451","presentation_id":"38929202"},{"card_image_alt_text":"A representative figure from paper main.556","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.556.png","content":{"abstract":"In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.","authors":["Hanqi Jin","Tianming Wang","Xiaojun Wan"],"demo_url":"","keywords":["Extractive Summarization","Extractive ","abstractive summarization","Multi-Granularity Network"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.556.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.556","similar_paper_uids":["main.556","main.555","main.273","main.445","main.458"],"title":"Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization","tldr":"In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to gene...","track":"Summarization"},"forum":"main.556","id":"main.556","presentation_id":"38929014"},{"card_image_alt_text":"A representative figure from paper main.555","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.555.png","content":{"abstract":"Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.","authors":["Wei Li","Xinyan Xiao","Jiachen Liu","Hua Wu","Haifeng Wang","Junping Du"],"demo_url":"","keywords":["Abstractive Summarization","detecting information","generating summaries","graph documents"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.555.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.555","similar_paper_uids":["main.555","main.556","main.457","main.451","main.175"],"title":"Leveraging Graph to Improve Abstractive Multi-Document Summarization","tldr":"Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (...","track":"Summarization"},"forum":"main.555","id":"main.555","presentation_id":"38929010"},{"card_image_alt_text":"A representative figure from paper main.554","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.554.png","content":{"abstract":"Cross-lingual summarization is the task of generating a summary in one language given a text in a different language. Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data. However, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time. In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize. We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages. Experimental results show that our model can outperform competitive models in most cases. In addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus.","authors":["Yue Cao","Hui Liu","Xiaojun Wan"],"demo_url":"","keywords":["Neural Summarization","Cross-lingual summarization","cross-lingual training","pipeline methods"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.554.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.554","similar_paper_uids":["main.554","main.421","main.658","main.109","main.153"],"title":"Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization","tldr":"Cross-lingual summarization is the task of generating a summary in one language given a text in a different language. Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the trans...","track":"Summarization"},"forum":"main.554","id":"main.554","presentation_id":"38928847"},{"card_image_alt_text":"A representative figure from paper main.550","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.550.png","content":{"abstract":"In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A. One big challenge for addressing this task is the lack of training data. Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers. We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task. Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation. Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model. This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers.","authors":["Xinyu Xing","Xiaosheng Fan","Xiaojun Wan"],"demo_url":"","keywords":["Automatic Texts","citation task","citation generation","automatically texts"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.550.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.550","similar_paper_uids":["main.550","main.464","main.702","main.354","main.26"],"title":"Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study","tldr":"In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A. ...","track":"Summarization"},"forum":"main.550","id":"main.550","presentation_id":"38928700"},{"card_image_alt_text":"A representative figure from paper main.551","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.551.png","content":{"abstract":"In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.","authors":["Zhenwen Li","Wenhao Wu","Sujian Li"],"demo_url":"","keywords":["Abstractive Summarization","content selection","summarization","summarization method"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.551.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.551","similar_paper_uids":["main.551","main.457","main.445","main.451","main.513"],"title":"Composing Elementary Discourse Units in Abstractive Summarization","tldr":"In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summ...","track":"Summarization"},"forum":"main.551","id":"main.551","presentation_id":"38928858"},{"card_image_alt_text":"A representative figure from paper main.553","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.553.png","content":{"abstract":"As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.","authors":["Danqing Wang","Pengfei Liu","Yining Zheng","Xipeng Qiu","Xuanjing Huang"],"demo_url":"","keywords":["Extractive Summarization","learning relations","heterogeneous summarization","Heterogeneous Networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.553.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.553","similar_paper_uids":["main.553","main.640","main.241","main.552","main.555"],"title":"Heterogeneous Graph Neural Networks for Extractive Document Summarization","tldr":"As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturi...","track":"Summarization"},"forum":"main.553","id":"main.553","presentation_id":"38929003"},{"card_image_alt_text":"A representative figure from paper main.552","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.552.png","content":{"abstract":"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in {https://github.com/maszhongming/MatchSum}.","authors":["Ming Zhong","Pengfei Liu","Yiran Chen","Danqing Wang","Xipeng Qiu","Xuanjing Huang"],"demo_url":"","keywords":["Extractive Summarization","Text Matching","extractive task","semantic problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.552.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.552","similar_paper_uids":["main.552","main.445","main.451","main.449","main.124"],"title":"Extractive Summarization as Text Matching","tldr":"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we for...","track":"Summarization"},"forum":"main.552","id":"main.552","presentation_id":"38929382"}]
