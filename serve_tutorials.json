[{"abstract":"While deep learning has transformed the natural language processing (NLP) field and impacted the larger\ncomputational linguistics community, the rise of neural networks is stained by their opaque nature: It is\nchallenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in\nthe last few years, an increasingly large body of work has been devoted to the analysis and interpretation of\nneural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover,\napproaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims\nto fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The\ntutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers,\nbehavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly\napplied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to\ninform participants where to focus future efforts.\n","id":"T1","material":"","organizers":["Yonatan Belinkov, Sebastian Gehrmann and Ellie Pavlick"],"prerecorded":38928626,"rocketchat_channel":"","sessions":[{"end_time":"Sun, 05 Jul 2020 16:30:00 GMT","livestream_id":38931660,"session_name":"Live Session 1","start_time":"Sun, 05 Jul 2020 13:00:00 GMT","zoom_link":null},{"end_time":"Sun, 05 Jul 2020 21:00:00 GMT","livestream_id":38931661,"session_name":"Live Session 2","start_time":"Sun, 05 Jul 2020 17:30:00 GMT","zoom_link":null}],"slides":"","title":"Interpretability and Analysis in Neural NLP","virtual_format_description":"This tutorial has a prerecorded talk on this page (see below) that you can watch anytime during the conference. It\nalso has two live sessions that will be conducted on Zoom and will be livestreamed on this page. Additionally, it has\na chat window that you can use to have discussions with the tutorial teachers and other attendees anytime during the\nconference.\n","website":""},{"abstract":"The World Wide Web contains vast quantities of textual information in\nseveral forms: unstructured text, template-based semi-structured webpages\n(which present data in key-value pairs and lists), and tables. Methods for\nextracting information from these sources and converting it to a structured\nform have been a target of research from the natural language processing\n(NLP), data mining, and database communities. While these researchers have\nlargely separated extraction from web data into different problems based on\nthe modality of the data, they have faced similar problems such as learning\nwith limited labeled data, defining (or avoiding defining) ontologies, making\nuse of prior knowledge, and scaling solutions to deal with the size of the\nWeb. In this tutorial we take a holistic view toward information extraction,\nexploring the commonalities in the challenges and solutions developed to\naddress these different forms of text. We will explore the approaches\ntargeted at unstructured text that largely rely on learning syntactic or\nsemantic textual patterns, approaches targeted at semi-structured documents\nthat learn to identify structural patterns in the template, and approaches\ntargeting web tables which rely heavily on entity linking and type\ninformation. While these different data modalities have largely been\nconsidered separately in the past, recent research has started taking a more\ninclusive approach toward textual extraction, in which the multiple signals\noffered by textual, layout, and visual clues are combined into a single\nextraction model made possible by new deep learning approaches. At the same\ntime, trends within purely textual extraction have shifted toward\nfull-document understanding rather than considering sentences as independent\nunits. With this in mind, it is worth considering the information extraction\nproblem as a whole to motivate solutions that harness textual semantics along\nwith visual and semi-structured layout information. We will discuss these\napproaches and suggest avenues for future\nwork.\n","id":"T2","material":"","organizers":["Xin Luna Dong, Hannaneh Hajishirzi, Colin Lockard and Prashant Shiralkar"],"prerecorded":"","rocketchat_channel":"","sessions":[{"end_time":"Mon, 06 Jul 2020 01:30:00 GMT","livestream_id":38931662,"session_name":"Live Session","start_time":"Sun, 05 Jul 2020 22:00:00 GMT","zoom_link":null}],"slides":"static/pdf/T2-Slides.pdf","title":"Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web","virtual_format_description":"This tutorial has slides that you see can anytime (It does not have any\nprerecorded talk). It will be conducted entirely live on Zoom and will be\nlivestreamed on this page. It has a chat window that you can use to have\ndiscussions with the tutorial teachers and other attendees anytime during the\nconference.\n","website":"https://sites.google.com/view/acl-2020-multi-modal-ie"},{"abstract":"This tutorial will cover the theory and practice of reviewing research in\nnatural language processing. Heavy reviewing burdens on natural language\nprocessing researchers have made it clear that our community needs to\nincrease the size of our pool of potential reviewers. Simultaneously,\nnotable false negatives--rejection by our conferences of work that was\nlater shown to be tremendously important after acceptance by other\nconferences--have raised awareness of the fact that our reviewing practices\nleave something to be desired. We do not often talk about false positives\nwith respect to conference papers, but leaders in the field have noted that\nwe seem to have a publication bias towards papers that report high\nperformance, with perhaps not much else of interest in them. It need not be\nthis way. Reviewing is a learnable skill, and you will learn it here via\nlectures and a considerable amount of hands-on\npractice.\n","id":"T3","material":"https://github.com/reviewingNLP/ACL2020T3material","organizers":["Kevin Cohen, Karen Fort, Margot Mieskes and Aurelie Neveol"],"prerecorded":38928627,"rocketchat_channel":"","sessions":[{"end_time":"Sun, 05 Jul 2020 21:00:00 GMT","livestream_id":38931665,"session_name":"Live Session","start_time":"Sun, 05 Jul 2020 17:30:00 GMT","zoom_link":null}],"slides":"","title":"Reviewing Natural Language Processing Research","virtual_format_description":"This tutorial has a prerecorded talk on this page (see below) that\nyou can watch anytime during the conference. It also has a live session\nthat will be conducted on Zoom and will be livestreamed on this page.\nAdditionally, it has a chat window that you can use to have discussions\nwith the tutorial teachers and other attendees anytime during the\nconference.\n","website":""},{"abstract":"Text generation has played an important role in various applications of natural\nlanguage processing (NLP), and recent studies, researchers are paying\nincreasing attention to modeling and manipulating the style of the generation\ntext, which we call stylized text generation. In this tutorial, we will\nprovide a comprehensive literature review in this direction. We start from\nthe definition of style and different settings of stylized text generation,\nillustrated with various applications. Then, we present different settings of\nstylized generation, such as style-conditioned generation, style-transfer\ngeneration, and style-adversarial generation. In each setting, we delve deep\ninto machine learning methods, including embedding learning techniques to\nrepresent style, adversarial learning, and reinforcement learning with cycle\nconsistency to match content but to distinguish different styles. We also\nintroduce current approaches to evaluating stylized text generation systems.\nWe conclude our tutorial by presenting the challenges of stylized text\ngeneration and discussing future directions, such as small-data training,\nnon-categorical style modeling, and a generalized scope of style transfer\n(e.g., controlling the syntax as a style).\n","id":"T4","material":"","organizers":["Lili Mou and Olga Vechtomova"],"prerecorded":38928628,"rocketchat_channel":"","sessions":[{"end_time":"Sun, 05 Jul 2020 21:00:00 GMT","livestream_id":38931666,"session_name":"Live Session","start_time":"Sun, 05 Jul 2020 17:30:00 GMT","zoom_link":null}],"slides":"","title":"Stylized Text Generation Approaches and Applications","virtual_format_description":"This tutorial has a prerecorded talk on this page (see below) that you\ncan watch anytime during the conference. It also has a live session that will\nbe conducted on Zoom and will be livestreamed on this page. Additionally, it\nhas a chat window that you can use to have discussions with the tutorial\nteachers and other attendees anytime during the conference.\n","website":"https://sites.google.com/view/2020-stylized-text-generation/tutorial"},{"abstract":"All communication aims at achieving common ground (grounding):\ninterlocutors can work together effectively only with mutual beliefs about\nwhat the state of the world is, about what their goals are, and about how they\nplan to make their goals a reality. Computational dialogue research offers some\nclassic results on grouding, which unfortunately offer scant guidance to the\ndesign of grounding modules and behaviors in cutting-edge systems. In this\ntutorial, we focus on three main topic areas: 1) grounding in human-human\ncommunication; 2) grounding in dialogue systems; and 3) grounding in\nmulti-modal interactive systems, including image-oriented conversations and\nhuman-robot interactions. We highlight a number of achievements of recent\ncomputational research in coordinating complex content, show how these results\nlead to rich and challenging opportunities for doing grounding in more flexible\nand powerful ways, and canvass relevant insights from the literature on\nhuman-human conversation. We expect that the tutorial will be of interest to\nresearchers in dialogue systems, computational semantics and cognitive\nmodeling, and hope that it will catalyze research and system building that more\ndirectly explores the creative, strategic ways conversational agents might be\nable to seek and offer evidence about their understanding of their\ninterlocutors.\n","id":"T5","material":"","organizers":["Malihe Alikhani and Matthew Stone"],"prerecorded":38928631,"rocketchat_channel":"","sessions":[{"end_time":"Sun, 05 Jul 2020 16:30:00 GMT","livestream_id":38931663,"session_name":"Live Session 1","start_time":"Sun, 05 Jul 2020 13:00:00 GMT","zoom_link":null},{"end_time":"Mon, 06 Jul 2020 01:30:00 GMT","livestream_id":38931664,"session_name":"Live Session 2","start_time":"Sun, 05 Jul 2020 22:00:00 GMT","zoom_link":null}],"slides":"","title":"Achieving Common Ground in Multi-modal Dialogue","virtual_format_description":"This tutorial has a prerecorded talk on this page (see below) that you can watch anytime during the conference.\nIt also has two live sessions that will be conducted on Zoom and will be\nlivestreamed on this page. Additionally, it has a chat window that you can use\nto have discussions with the tutorial teachers and other attendees anytime during the conference.\n","website":"https://github.com/malihealikhani/Grounding_in_Dialogue"},{"abstract":"Commonsense knowledge, such as knowing that 'bumping into people annoys\nthem' or 'rain makes the road slippery', helps humans navigate everyday\nsituations seamlessly. Yet, endowing machines with such human-like\ncommonsense reasoning capabilities has remained an elusive goal of\nartificial intelligence research for decades. In recent years, commonsense\nknowledge and reasoning have received renewed attention from the natural\nlanguage processing (NLP) community, yielding exploratory studies in\nautomated commonsense understanding. We organize this tutorial to provide\nresearchers with the critical foundations and recent advances in\ncommonsense representation and reasoning, in the hopes of casting a\nbrighter light on this promising area of future research. In our tutorial,\nwe will (1) outline the various types of commonsense (e.g., physical,\nsocial), and (2) discuss techniques to gather and represent commonsense\nknowledge, while highlighting the challenges specific to this type of\nknowledge (e.g., reporting bias). We will then (3) discuss the types of\ncommonsense knowledge captured by modern NLP systems (e.g., large\npretrained language models), and (4) present ways to measure systems'\ncommonsense reasoning abilities. We will finish with (5) a discussion of\nvarious ways in which commonsense reasoning can be used to improve\nperformance on NLP tasks, exemplified by an (6) interactive session on\nintegrating commonsense into a downstream\ntask.\n","id":"T6","material":"","organizers":["Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, Dan Roth"],"prerecorded":"","rocketchat_channel":"","sessions":[{"end_time":"Mon, 06 Jul 2020 01:30:00 GMT","livestream_id":38931667,"session_name":"Live Session","start_time":"Sun, 05 Jul 2020 22:00:00 GMT","zoom_link":null}],"slides":"","title":"Commonsense Reasoning for Natural Language Processing","virtual_format_description":"This tutorial has slides that you can see anytime (It does not have\nany prerecorded talk). It will be conducted entirely live on Zoom and will\nbe livestreamed on this page. It has a chat window that you can use to have\ndiscussions with the tutorial teachers and other attendees anytime during\nthe conference.\n","website":"https://tinyurl.com/acl2020-commonsense"},{"abstract":"To raise awareness among future NLP practitioners and\nprevent inertia in the field, we need to place ethics in the curriculum for all\nNLP student--not as an elective, but as a core part of their education. Our\ngoal in this tutorial is to empower NLP researchers and practitioners with\ntools and resources to teach others about how to ethically apply NLP\ntechniques. We will present both high-level strategies for developing an\nethics-oriented curriculum, based on experience and best practices, as well as\nspecific sample exercises that can be brought to a classroom. This highly\ninteractive work session will culminate in a shared online resource page that\npools lesson plans, assignments, exercise ideas, reading suggestions, and ideas\nfrom the attendees. Though the tutorial will focus particularly on examples for\nuniversity classrooms, we believe these ideas can extend to company-internal\nworkshops or tutorials in a variety of organizations. In this setting, a key\nlesson is that there is no single approach to ethical NLP: each project\nrequires thoughtful consideration about what steps can be taken to best support\npeople affected by that project. However, we can learn (and teach) what issues\nto be aware of, what questions to ask, and what strategies are available to\nmitigate harm.\n","id":"T7","material":"","organizers":["Emily M. Bender, Dirk Hovy and Alexandra Schofield"],"prerecorded":"","rocketchat_channel":"","sessions":[{"end_time":"Sun, 05 Jul 2020 16:30:00 GMT","livestream_id":null,"session_name":"Live Session 1","start_time":"Sun, 05 Jul 2020 13:00:00 GMT","zoom_link":null},{"end_time":"Sun, 05 Jul 2020 21:00:00 GMT","livestream_id":null,"session_name":"Live Session 2","start_time":"Sun, 05 Jul 2020 17:30:00 GMT","zoom_link":null}],"slides":"static/pdf/T7-Slides.pdf","title":"Integrating Ethics into the NLP Curriculum","virtual_format_description":"This tutorial has slides that you can see\nanytime (It does not have any prerecorded talk). The two live sessions will be\nconducted on Zoom (It will not be livestreamed). It has a chat window that you\ncan use to have discussions with the tutorial teachers and other attendees\nanytime during the conference.\n","website":""},{"abstract":"This tutorial provides a comprehensive and coherent overview of cutting-edge\nresearch in open-domain question answering (QA), the task of answering\nquestions using a large collection of documents of diversified topics. We\nwill start by first giving a brief historical background, discussing the\nbasic setup and core technical challenges of the research problem, and then\ndescribe modern datasets with the common evaluation metrics and benchmarks.\nThe focus will then shift to cutting-edge models proposed for open-domain QA,\nincluding two-stage retriever-reader approaches, dense retriever and\nend-to-end training, and retriever-free methods. Finally, we will cover some\nhybrid approaches using both text and large knowledge bases and conclude the\ntutorial with important open questions. We hope that the tutorial will not\nonly help the audience to acquire up-to-date knowledge but also provide new\nperspectives to stimulate the advances of open-domain QA research in the next\nphase.\n","id":"T8","material":"","organizers":["Danqi Chen and Scott Wen-tau Yih"],"prerecorded":"","rocketchat_channel":"","sessions":[{"end_time":"Mon, 06 Jul 2020 01:30:00 GMT","livestream_id":38931668,"session_name":"Live Session","start_time":"Sun, 05 Jul 2020 22:00:00 GMT","zoom_link":null}],"slides":"","title":"Open-Domain Question Answering","virtual_format_description":"This tutorial has slides on its website that you can see anytime (It does not have any\nprerecorded talk). It will be conducted entirely live on Zoom and will be\nlivestreamed on this page. It has a chat window that you can use to have\ndiscussions with the tutorial teachers and other attendees anytime during the\nconference.\n","website":"https://github.com/danqi/acl2020-openqa-tutorial"}]
