


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>

    <!-- https://developer.snapappointments.com/bootstrap-select/ -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>

    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />

    <title>ACL2020: Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms</title>
    
<meta name="citation_title" content="Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms" />

<meta name="citation_author" content="Goro Kobayashi" />

<meta name="citation_author" content="Tatsuki Kuribayashi" />

<meta name="citation_author" content="Sho Yokoi" />

<meta name="citation_author" content="Kentaro Inui" />

<meta name="citation_publication_date" content="July 2020" />
<meta name="citation_conference_title" content="The 58th Annual Meeting Of The Association For Computational Linguistics" />
<meta name="citation_inbook_title" content="Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics" />
<meta name="citation_abstract" content="Self-attention modules are essential building blocks of Transformer-based language models and hence are the subject of a large number of studies aiming to discover which linguistic capabilities these models possess (Rogers et al., 2020). Such studies are commonly conducted by analyzing correlations of attention weights with specific linguistic phenomena. In this paper, we show that attention weights alone are only one of two factors determining the output of self-attention modules and propose to incorporate the other factor, namely the norm of the transformed input vectors, into the analysis, as well. Our analysis of self-attention modules in BERT (Devlin et al., 2019) shows that the proposed method produces insights that better agree with linguistic intuitions than an analysis based on attention-weights alone. Our analysis further reveals that BERT controls the amount of the contribution from frequent informative and less informative tokens not by attention weights but via vector norms." />

<meta name="citation_keywords" content="BERT" />

<meta name="citation_keywords" content="Self-attention modules" />

<meta name="citation_keywords" content="Transformer-based models" />

<meta name="citation_keywords" content="output modules" />

<meta name="citation_pdf_url" content="" />


    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="static/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="static/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="static/favicon/favicon-16x16.png">
    <link rel="manifest" href="static/favicon/site.webmanifest">
    <link rel="mask-icon" href="static/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="static/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="static/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
  </head>

  <body>
    <!-- NAV -->
    
    

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="index.html">
          <img
             class="logo" src="static/images/acl2020/acl-logo.png"
             height="45px"
             width="auto"
          />
        </a>
        
        <a class="navbar-brand" href="index.html">ACL2020</a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="schedule.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="livestream.html">Livestream</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="plenary_sessions.html">Plenary</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="tutorials.html">Tutorials</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="workshops.html">Workshops</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="sponsors.html">Sponsors</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="organizers.html">Organizers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="about.html">Help</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Heading -->
      <div class="heading">
         
      </div>
      <div class="tabs pt-3">
      <!-- Tabs -->
      <div class="tabs pt-3">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="color: black">
      Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Goro Kobayashi" class="text-primary"
        >Goro Kobayashi</a
      >,
      
      <a href="papers.html?filter=authors&search=Tatsuki Kuribayashi" class="text-primary"
        >Tatsuki Kuribayashi</a
      >,
      
      <a href="papers.html?filter=authors&search=Sho Yokoi" class="text-primary"
        >Sho Yokoi</a
      >,
      
      <a href="papers.html?filter=authors&search=Kentaro Inui" class="text-primary"
        >Kentaro Inui</a
      >
      
    </h3>
   
    <div class="text-center p-3">
      <a class="card-link" data-toggle="collapse" role="button" href="#details">
        Abstract
      </a>
      
      
      
      
    </div>
    <p class="card-text text-center h5">

     <a href="papers.html?track=Student Research Workshop" class ="badge badge-pill badge-primary" target="_blank">Student Research Workshop</a>
     <span class="badge badge-secondary">SRW Paper</span>
    </p>

    
  <div class="text-center text-muted text-monospace">
  <div class="paper-session-times">
  Session 6A: Jul 7
  <span class="session_times">(05:00-06:00 GMT)</span>

  

  
  </div>
  </div>

  <div class="text-center text-muted text-monospace">
  <div class="paper-cal-links">
  <img src="static/images/calendar.svg" class="paper-detail-calendar" height="24px">
  <span id="session-cal-link-1"></span>
  </div>
  </div>

  <div class="text-center text-muted text-monospace">
  <div class="paper-session-times">
  Session 12B: Jul 8
  <span class="session_times">(09:00-10:00 GMT)</span>

  

  
  </div>
  </div>

  <div class="text-center text-muted text-monospace">
  <div class="paper-cal-links">
  <img src="static/images/calendar.svg" class="paper-detail-calendar" height="24px">
  <span id="session-cal-link-2"></span>
  </div>
  </div>


<script src="static/js/add-to-calendar.js"></script>
<script>

  let ouicalData;
  let targetNode;
  let calendarNodeInner;
  let calendarNodeSpan;
  let parser = new DOMParser();
  let calendarNames = Array("google", "off365", "outlook", "ical");

  
    // create a calendar by hand
    ouicalData = addToCalendarData({
      options: {
        class: 'my-class',
        id: 'my-id'                               // If you don't pass an id, one will be generated for you.
      },
      data: {
        title: 'Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms'.replace("#", " "),

        // Event start date
        start: new Date("2020-07-07T05:00:00"),
        end: new Date("2020-07-07T06:00:00"),
        // duration: 120,                           // Event duration (IN MINUTES)
        // allday: true,														// Override end time, duration and timezone, triggers 'all day'

        // Event timezone. Will convert the given time to that zone
        timezone: 'UTC',
        // Event Address
        

        // Event Description
        // NOTE: Cannot use abstract because of it is multi-line. Need to format it.
        description: 'https://virtual.acl2020.org/paper_srw.115.html'
      }
    });

    targetNode = document.querySelector('#session-cal-link-1');
    for (const name of calendarNames) {
      calendarNodeInner = parser.parseFromString(ouicalData[name], "text/html");
      calendarNodeSpan = document.createElement("span");
      calendarNodeSpan.setAttribute("style", "padding: 2px");

      const openBracket = document.createElement("span");
      openBracket.setAttribute("aria-hidden","true");
      openBracket.innerHTML = "[";
      calendarNodeSpan.appendChild(openBracket);

      calendarNodeSpan.appendChild(calendarNodeInner.getElementsByTagName("a")[0]);

      const closeBracket = document.createElement("span");
      closeBracket.setAttribute("aria-hidden","true");
      closeBracket.innerHTML = "]";
      calendarNodeSpan.appendChild(closeBracket);

      targetNode.appendChild(calendarNodeSpan);
    }
  
    // create a calendar by hand
    ouicalData = addToCalendarData({
      options: {
        class: 'my-class',
        id: 'my-id'                               // If you don't pass an id, one will be generated for you.
      },
      data: {
        title: 'Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms'.replace("#", " "),

        // Event start date
        start: new Date("2020-07-08T09:00:00"),
        end: new Date("2020-07-08T10:00:00"),
        // duration: 120,                           // Event duration (IN MINUTES)
        // allday: true,														// Override end time, duration and timezone, triggers 'all day'

        // Event timezone. Will convert the given time to that zone
        timezone: 'UTC',
        // Event Address
        

        // Event Description
        // NOTE: Cannot use abstract because of it is multi-line. Need to format it.
        description: 'https://virtual.acl2020.org/paper_srw.115.html'
      }
    });

    targetNode = document.querySelector('#session-cal-link-2');
    for (const name of calendarNames) {
      calendarNodeInner = parser.parseFromString(ouicalData[name], "text/html");
      calendarNodeSpan = document.createElement("span");
      calendarNodeSpan.setAttribute("style", "padding: 2px");

      const openBracket = document.createElement("span");
      openBracket.setAttribute("aria-hidden","true");
      openBracket.innerHTML = "[";
      calendarNodeSpan.appendChild(openBracket);

      calendarNodeSpan.appendChild(calendarNodeInner.getElementsByTagName("a")[0]);

      const closeBracket = document.createElement("span");
      closeBracket.setAttribute("aria-hidden","true");
      closeBracket.innerHTML = "]";
      calendarNodeSpan.appendChild(closeBracket);

      targetNode.appendChild(calendarNodeSpan);
    }
  
</script>

  </div>
</div>

<div id="details" class="pp-card m-3 collapse">
  <div class="card-body">
    <div class="card-text">
      <div id="abstractExample">
        <span class="font-weight-bold">Abstract:</span>
        Self-attention modules are essential building blocks of Transformer-based language models and hence are the subject of a large number of studies aiming to discover which linguistic capabilities these models possess (Rogers et al., 2020). Such studies are commonly conducted by analyzing correlations of attention weights with specific linguistic phenomena. In this paper, we show that attention weights alone are only one of two factors determining the output of self-attention modules and propose to incorporate the other factor, namely the norm of the transformed input vectors, into the analysis, as well. Our analysis of self-attention modules in BERT (Devlin et al., 2019) shows that the proposed method produces insights that better agree with linguistic intuitions than an analysis based on attention-weights alone. Our analysis further reveals that BERT controls the amount of the contribution from frequent informative and less informative tokens not by attention weights but via vector norms.
      </div>
    </div>
  </div>
</div>

<div class="container" style="background-color:white; padding: 0px;">
  <div class="text-muted text-center">
    You can open the
    <a href="https://slideslive.com/38928669" target="_blank">pre-recorded video</a>
    
    in a separate window.
    

  </div>
  <div class="row m-2">
    <!-- Slides Live-->
    <div class="col-md-12 col-xs-12 my-auto p-2" >
      <div id="presentation-embed" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed', {
        presentationId: '38928669',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 500,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>

      <div class="text-muted">
      NOTE: The SlidesLive video may display a random order of the authors.
      The correct author list is shown at the top of this webpage.
      </div>
    </div>

    <!-- Chat (disabled) -->
    
  </div>
</div>




<div class="my-3"></div>
<div class="row p-4">
  <div class="col-12 bd-content">
    <h1 class="text-center">Similar Papers</h1>
  </div>
</div>
<p></p>
<div class="container" >
  <div class="row">
  
    <div class="col-md-4 col-xs-6">
      <div class="pp-card" >
        <div class="pp-card-header" class="text-muted">
          <a href="paper_main.385.html" class="text-muted">
            <h5 class="card-title" align="center">Quantifying Attention Flow in Transformers</h5>
          </a>
          <h6 class="card-subtitle text-muted" align="center">
             
             Samira Abnar,
             
             Willem Zuidema,
             
          </h6>
          <center><img class="cards_img" src="https://acl2020-public.s3.amazonaws.com/papers/main.385.png" alt="A representative figure from paper main.385" width="80%"/></center>
        </div>
      </div>
    </div>
  
    <div class="col-md-4 col-xs-6">
      <div class="pp-card" >
        <div class="pp-card-header" class="text-muted">
          <a href="paper_main.687.html" class="text-muted">
            <h5 class="card-title" align="center">Hard-Coded Gaussian Attention for Neural Machine Translation</h5>
          </a>
          <h6 class="card-subtitle text-muted" align="center">
             
             Weiqiu You,
             
             Simeng Sun,
             
             Mohit Iyyer,
             
          </h6>
          <center><img class="cards_img" src="https://acl2020-public.s3.amazonaws.com/papers/main.687.png" alt="A representative figure from paper main.687" width="80%"/></center>
        </div>
      </div>
    </div>
  
    <div class="col-md-4 col-xs-6">
      <div class="pp-card" >
        <div class="pp-card-header" class="text-muted">
          <a href="paper_tacl.1815.html" class="text-muted">
            <h5 class="card-title" align="center">Theoretical Limitations of Self-Attention in Neural Sequence Models</h5>
          </a>
          <h6 class="card-subtitle text-muted" align="center">
             
             Michael Hahn,
             
          </h6>
          <center><img class="cards_img" src="https://acl2020-public.s3.amazonaws.com/papers/tacl.1815.png" alt="A representative figure from paper tacl.1815" width="80%"/></center>
        </div>
      </div>
    </div>
  
    <div class="col-md-4 col-xs-6">
      <div class="pp-card" >
        <div class="pp-card-header" class="text-muted">
          <a href="paper_main.432.html" class="text-muted">
            <h5 class="card-title" align="center">Learning to Deceive with Attention-Based Explanations</h5>
          </a>
          <h6 class="card-subtitle text-muted" align="center">
             
             Danish Pruthi,
             
             Mansi Gupta,
             
             Bhuwan Dhingra,
             
             Graham Neubig,
             
             Zachary C. Lipton,
             
          </h6>
          <center><img class="cards_img" src="https://acl2020-public.s3.amazonaws.com/papers/main.432.png" alt="A representative figure from paper main.432" width="80%"/></center>
        </div>
      </div>
    </div>
  
  </div>
</div>

<script src="static/js/time-extend.js"></script>
<script>
  $(document).ready(()=>{
    add_local_tz('.session_times');
  })
</script>


      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-171494682-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-171494682-1");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">Â© 2020 Association for Computational Linguistics</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>