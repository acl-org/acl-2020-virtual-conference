[{"card_image_alt_text":"A representative figure from paper main.749","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.749.png","content":{"abstract":"We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.","authors":["Tongfei Chen","Yunmo Chen","Benjamin Van Durme"],"demo_url":"","keywords":["Hierarchical Typing","hierarchical classification","prediction","Multi-level Learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.749.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.749","similar_paper_uids":["main.749","main.27","main.524","main.241","main.572"],"title":"Hierarchical Entity Typing via Multi-level Learning to Rank","tldr":"We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings ac...","track":"Information Extraction"},"forum":"main.749","id":"main.749","presentation_id":"38929149"},{"card_image_alt_text":"A representative figure from paper main.577","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.577.png","content":{"abstract":"Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.","authors":["Juntao Yu","Bernd Bohnet","Massimo Poesio"],"demo_url":"","keywords":["Named Recognition","NER","Natural Processing","NER research"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.577.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.577","similar_paper_uids":["main.577","main.519","main.571","main.525","main.611"],"title":"Named Entity Recognition as Dependency Parsing","tldr":"Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that en...","track":"Information Extraction"},"forum":"main.577","id":"main.577","presentation_id":"38928891"},{"card_image_alt_text":"A representative figure from paper main.576","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.576.png","content":{"abstract":"Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.","authors":["Yuanzhe Zhang","Zhongtao Jiang","Tao Zhang","Shiwan Liu","Jiarun Cao","Kang Liu","Shengping Liu","Jun Zhao"],"demo_url":"","keywords":["medical systems","MIE","Medical Extractor","EMRs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.576.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.576","similar_paper_uids":["main.576","main.719","main.62","main.282","main.567"],"title":"MIE: A Medical Information Extractor towards Medical Dialogues","tldr":"Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dial...","track":"Information Extraction"},"forum":"main.576","id":"main.576","presentation_id":"38929356"},{"card_image_alt_text":"A representative figure from paper main.748","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.748.png","content":{"abstract":"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.","authors":["Dongfang Xu","Zeyu Zhang","Steven Bethard"],"demo_url":"","keywords":["Biomedical Normalization","Concept normalization","Generate-and-Rank Framework","Semantic Regularization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.748.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.748","similar_paper_uids":["main.748","main.717","main.760","main.184","main.292"],"title":"A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization","tldr":"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are ...","track":"Information Extraction"},"forum":"main.748","id":"main.748","presentation_id":"38929206"},{"card_image_alt_text":"A representative figure from paper main.574","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.574.png","content":{"abstract":"One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation. In this work, we propose approaches to address this problem. For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations. For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations. Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results. Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.","authors":["Yangming Li","Han Li","Kaisheng Yao","Xiaolong Li"],"demo_url":"","keywords":["Neural Labeling","data problem","delexicalized identification","local reconstruction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.574.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.574","similar_paper_uids":["main.574","main.58","main.722","main.612","main.725"],"title":"Handling Rare Entities for Neural Sequence Labeling","tldr":"One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) a...","track":"Information Extraction"},"forum":"main.574","id":"main.574","presentation_id":"38928929"},{"card_image_alt_text":"A representative figure from paper main.575","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.575.png","content":{"abstract":"Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.","authors":["Hiroki Ouchi","Jun Suzuki","Sosuke Kobayashi","Sho Yokoi","Tatsuki Kuribayashi","Ryuto Konno","Kentaro Inui"],"demo_url":"","keywords":["Named Recognition","Interpretable rationales","model predictions","structured prediction"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.575.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.575","similar_paper_uids":["main.575","main.278","main.543","srw.16","tacl.1853"],"title":"Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition","tldr":"Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based ...","track":"Information Extraction"},"forum":"main.575","id":"main.575","presentation_id":"38928957"},{"card_image_alt_text":"A representative figure from paper main.571","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.571.png","content":{"abstract":"In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies. Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them. We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module. The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions. Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.","authors":["Ying Luo","Hai Zhao"],"demo_url":"","keywords":["Nested Recognition","NER","Bipartite Network","bipartite"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.571.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.571","similar_paper_uids":["main.571","main.577","main.519","main.512","main.67"],"title":"Bipartite Flat-Graph Network for Nested Named Entity Recognition","tldr":"In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner ...","track":"Information Extraction"},"forum":"main.571","id":"main.571","presentation_id":"38929316"},{"card_image_alt_text":"A representative figure from paper main.570","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.570.png","content":{"abstract":"An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI. As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50). Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance. Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets. A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets. The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM_PPI_NLP.","authors":["Pratik Dutta","Sriparna Saha"],"demo_url":"","keywords":["protein-protein identification","in-depth interactions","PPI tasks","automatic PPI"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.570.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.570","similar_paper_uids":["main.570","main.273","main.343","main.402","main.401"],"title":"Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification","tldr":"An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. Most of the recent PPI tasks in BioNLP domain h...","track":"Information Extraction"},"forum":"main.570","id":"main.570","presentation_id":"38929411"},{"card_image_alt_text":"A representative figure from paper main.612","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.612.png","content":{"abstract":"Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities. Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality. FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation. Extensive experiments show the effectiveness of such embeddings. Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.","authors":["Feng Hou","Ruili Wang","Jun He","Yi Zhou"],"demo_url":"","keywords":["Entity Linking","learning commonality","Semantic Embeddings","Entity embeddings"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.612.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.612","similar_paper_uids":["main.612","main.722","main.760","main.238","main.525"],"title":"Improving Entity Linking through Semantic Reinforced Entity Embeddings","tldr":"Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local con...","track":"Information Extraction"},"forum":"main.612","id":"main.612","presentation_id":"38928767"},{"card_image_alt_text":"A representative figure from paper main.610","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.610.png","content":{"abstract":"We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. Previous approaches to the TSE task can be characterized as either distributional or pattern-based. We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches. Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM. The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns. Our method outperforms state-of-the-art TSE algorithms. Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/","authors":["Guy Kushilevitz","Shaul Markovitch","Yoav Goldberg"],"demo_url":"","keywords":["Term Expansion","TSE task","Two-Stage Method","TSE"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.610.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.610","similar_paper_uids":["main.610","main.391","main.194","demo.69","main.522"],"title":"A Two-Stage Masked LM Method for Term Set Expansion","tldr":"We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization f...","track":"Information Extraction"},"forum":"main.610","id":"main.610","presentation_id":"38929105"},{"card_image_alt_text":"A representative figure from paper main.572","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.572.png","content":{"abstract":"Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs. Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. Accordingly, we build two novel embedding models to realize the mechanisms. Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs. Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing. The source code and data of this paper can be obtained from: https://github.com/Adam1679/ConnectE .","authors":["Yu Zhao","anxiang zhang","Ruobing Xie","Kang Liu","Xiaojie WANG"],"demo_url":"","keywords":["Connecting Embeddings","Knowledge Typing","knowledge completion","KG typing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.572.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.572","similar_paper_uids":["main.572","main.412","main.238","main.655","main.526"],"title":"Connecting Embeddings for Knowledge Graph Entity Typing","tldr":"Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity ty...","track":"Information Extraction"},"forum":"main.572","id":"main.572","presentation_id":"38928952"},{"card_image_alt_text":"A representative figure from paper main.573","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.573.png","content":{"abstract":"Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations. Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning. Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.","authors":["Xu Han","Yi Dai","Tianyu Gao","Yankai Lin","Zhiyuan Liu","Peng Li","Maosong Sun","Jie Zhou"],"demo_url":"","keywords":["Continual Learning","Reconsolidation","human formation","memory exercise"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.573.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.573","similar_paper_uids":["main.573","main.481","main.61","main.583","main.719"],"title":"Continual Relation Learning via Episodic Memory Activation and Reconsolidation","tldr":"Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical rel...","track":"Information Extraction"},"forum":"main.573","id":"main.573","presentation_id":"38929363"},{"card_image_alt_text":"A representative figure from paper main.611","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.611.png","content":{"abstract":"Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.","authors":["Xiaonan Li","Hang Yan","Xipeng Qiu","Xuanjing Huang"],"demo_url":"","keywords":["Chinese NER","Chinese recognition","NER","FLAT"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.611.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.611","similar_paper_uids":["main.611","main.528","main.519","main.571","main.577"],"title":"FLAT: Chinese NER Using Flat-Lattice Transformer","tldr":"Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are...","track":"Information Extraction"},"forum":"main.611","id":"main.611","presentation_id":"38929311"},{"card_image_alt_text":"A representative figure from paper main.138","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.138.png","content":{"abstract":"Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs---as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.","authors":["Marcin Namysl","Sven Behnke","Joachim K\u00f6hler"],"demo_url":"","keywords":["Noise-Aware Training","Robust Labeling","Sequence systems","noisy problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.138.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.138","similar_paper_uids":["main.138","demo.89","srw.116","main.322","main.777"],"title":"NAT: Noise-Aware Training for Robust Neural Sequence Labeling","tldr":"Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs---as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy seq...","track":"Information Extraction"},"forum":"main.138","id":"main.138","presentation_id":"38928783"},{"card_image_alt_text":"A representative figure from paper main.716","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.716.png","content":{"abstract":"Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing. The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular. This is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms. In this paper, we aim at adapting monolingual models to code-switched text in various tasks. Specifically, we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs (i.e., Nepali-English, Spanish-English, and Hindi-English) using the task of language identification. Our method, CS-ELMo, is an extension of ELMo with a simple yet effective position-aware attention mechanism inside its character convolutions. We show the effectiveness of this transfer learning step by outperforming multilingual BERT and homologous CS-unaware ELMo models and establishing a new state of the art in CS tasks, such as NER and POS tagging. Our technique can be expanded to more English-paired code-switched languages, providing more resources to the CS community.","authors":["Gustavo Aguilar","Thamar Solorio"],"demo_url":"","keywords":["natural processing","CS","language identification","CS tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.716.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.716","similar_paper_uids":["main.716","main.329","main.348","main.421","main.747"],"title":"From English to Code-Switching: Transfer Learning with Strong Morphological Clues","tldr":"Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing. The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular. This is partl...","track":"Information Extraction"},"forum":"main.716","id":"main.716","presentation_id":"38929261"},{"card_image_alt_text":"A representative figure from paper main.528","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.528.png","content":{"abstract":"Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.","authors":["Ruotian Ma","Minlong Peng","Qi Zhang","Zhongyu Wei","Xuanjing Huang"],"demo_url":"","keywords":["Chinese recognition","NER","Lattice-LSTM","complex architecture"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.528.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.528","similar_paper_uids":["main.528","main.519","main.611","main.525","main.315"],"title":"Simplify the Usage of Lexicon in Chinese NER","tldr":"Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LS...","track":"Information Extraction"},"forum":"main.528","id":"main.528","presentation_id":"38928863"},{"card_image_alt_text":"A representative figure from paper main.717","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.717.png","content":{"abstract":"Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs. The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities. We conduct extensive experiments on public English and Chinese datasets. Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets.","authors":["Jingyuan Zhang","Mingming Sun","Yue Feng","Ping Li"],"demo_url":"","keywords":["text understanding","learning relationships","identification concepts","Bayesian Learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.717.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.717","similar_paper_uids":["main.717","main.748","main.184","main.292","main.760"],"title":"Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts","tldr":"Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of ...","track":"Information Extraction"},"forum":"main.717","id":"main.717","presentation_id":"38928762"},{"card_image_alt_text":"A representative figure from paper main.139","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.139.png","content":{"abstract":"Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.","authors":["Pierre Lison","Jeremy Barnes","Aliaksandr Hubin","Samia Touileb"],"demo_url":"","keywords":["Named Recognition","NER","Weak Approach","transfer techniques"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.139.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.139","similar_paper_uids":["main.139","main.581","main.520","main.519","main.752"],"title":"Named Entity Recognition without Labelled Data: A Weak Supervision Approach","tldr":"Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existin...","track":"Information Extraction"},"forum":"main.139","id":"main.139","presentation_id":"38929328"},{"card_image_alt_text":"A representative figure from paper main.715","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.715.png","content":{"abstract":"This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text. In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. The major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE. In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization. In particular, we leverage Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.","authors":["Amir Pouran Ben Veyseh","Franck Dernoncourt","Dejing Dou","Thien Huu Nguyen"],"demo_url":"","keywords":["Neural Extraction","Relation Extraction","RE","syntactic injection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.715.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.715","similar_paper_uids":["main.715","main.642","main.527","main.375","main.591"],"title":"Exploiting the Syntax-Model Consistency for Neural Relation Extraction","tldr":"This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text. In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the depe...","track":"Information Extraction"},"forum":"main.715","id":"main.715","presentation_id":"38929107"},{"card_image_alt_text":"A representative figure from paper main.714","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.714.png","content":{"abstract":"Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a dif\ufb01cult task since it requires a view of a larger context to determine which spans of text correspond to event role \ufb01llers. We \ufb01rst investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role \ufb01ller extraction, as well as how the length of context captured affects the models\u2019 performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report \ufb01ndings on the relationship between context length and neural model performance on the task.","authors":["Xinya Du","Claire Cardie"],"demo_url":"","keywords":["Document-Level Extraction","event extraction","extraction decisions","Multi-Granularity Encoding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.714.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.714","similar_paper_uids":["main.714","main.718","main.141","main.321","main.230"],"title":"Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding","tldr":"Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that docu...","track":"Information Extraction"},"forum":"main.714","id":"main.714","presentation_id":"38928745"},{"card_image_alt_text":"A representative figure from paper main.670","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.670.png","content":{"abstract":"Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .","authors":["Sarthak Jain","Madeleine van Zuylen","Hannaneh Hajishirzi","Iz Beltagy"],"demo_url":"","keywords":["Document-Level Extraction","IE tasks","salient identification","document identification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.670.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.670","similar_paper_uids":["main.670","main.141","main.207","main.135","main.137"],"title":"SciREX: A Challenge Dataset for Document-Level Information Extraction","tldr":"Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) data...","track":"Information Extraction"},"forum":"main.670","id":"main.670","presentation_id":"38929290"},{"card_image_alt_text":"A representative figure from paper main.667","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.667.png","content":{"abstract":"In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.","authors":["Zhisong Zhang","Xiang Kong","Zhengzhong Liu","Xuezhe Ma","Eduard Hovy"],"demo_url":"","keywords":["Implicit Detection","implicit task","modeling","argument detection"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.667.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.667","similar_paper_uids":["main.667","main.718","main.371","main.714","main.82"],"title":"A Two-Step Approach for Implicit Event Argument Detection","tldr":"In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidate...","track":"Information Extraction"},"forum":"main.667","id":"main.667","presentation_id":"38928918"},{"card_image_alt_text":"A representative figure from paper main.713","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.713.png","content":{"abstract":"Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.","authors":["Ying Lin","Heng Ji","Fei Huang","Lingfei Wu"],"demo_url":"","keywords":["Information Extraction","Information IE","IE","end-to-end IE"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.713.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.713","similar_paper_uids":["main.713","main.758","main.421","main.554","main.137"],"title":"A Joint Neural Model for Information Extraction with Global Features","tldr":"Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likel...","track":"Information Extraction"},"forum":"main.713","id":"main.713","presentation_id":"38929257"},{"card_image_alt_text":"A representative figure from paper main.680","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.680.png","content":{"abstract":"Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets annotated with named entities. We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.","authors":["Shruti Rijhwani","Daniel Preotiuc-Pietro"],"demo_url":"","keywords":["named recognition","NLP tasks","Natural models","language use"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.680.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.680","similar_paper_uids":["main.680","main.722","main.764","main.645","main.678"],"title":"Temporally-Informed Analysis of Named Entity Recognition","tldr":"Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom report...","track":"Information Extraction"},"forum":"main.680","id":"main.680","presentation_id":"38929195"},{"card_image_alt_text":"A representative figure from paper main.521","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.521.png","content":{"abstract":"While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.","authors":["Keshav Kolluru","Samarth Aggarwal","Vipul Rathore","Mausam -","Soumen Chakrabarti"],"demo_url":"","keywords":["Iterative Extraction","Open Extraction","IMoJIE","Iterative "],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.521.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.521","similar_paper_uids":["main.521","main.125","tacl.1815","main.710","main.137"],"title":"IMoJIE: Iterative Memory-Based Joint Open Information Extraction","tldr":"While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis...","track":"Information Extraction"},"forum":"main.521","id":"main.521","presentation_id":"38929035"},{"card_image_alt_text":"A representative figure from paper main.520","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.520.png","content":{"abstract":"Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.","authors":["Xiang Dai","Sarvnaz Karimi","Ben Hachey","Cecile Paris"],"demo_url":"","keywords":["Discontinuous NER","Transition-based Model","sequence techniques","generic encoding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.520.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.520","similar_paper_uids":["main.520","main.519","main.139","main.528","main.752"],"title":"An Effective Transition-based Model for Discontinuous NER","tldr":"Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient b...","track":"Information Extraction"},"forum":"main.520","id":"main.520","presentation_id":"38928958"},{"card_image_alt_text":"A representative figure from paper main.722","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.722.png","content":{"abstract":"Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of ``soft gazetteers'' that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.","authors":["Shruti Rijhwani","Shuyan Zhou","Graham Neubig","Jaime Carbonell"],"demo_url":"","keywords":["Low-Resource Recognition","named recognition","'","Soft Gazetteers"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.722.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.722","similar_paper_uids":["main.722","tacl.1906","main.612","main.574","main.680"],"title":"Soft Gazetteers for Low-Resource Named Entity Recognition","tldr":"Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated th...","track":"Information Extraction"},"forum":"main.722","id":"main.722","presentation_id":"38929323"},{"card_image_alt_text":"A representative figure from paper main.681","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.681.png","content":{"abstract":"We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example's domain. It requires no labeled data from the target domain, making it completely unsupervised. Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data. Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data. Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively.","authors":["Aakanksha Naik","Carolyn Rose"],"demo_url":"","keywords":["Open Identification","trigger identification","Adversarial Adaptation","supervised models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.681.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.681","similar_paper_uids":["main.681","main.370","main.595","main.565","main.139"],"title":"Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation","tldr":"We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial traini...","track":"Information Extraction"},"forum":"main.681","id":"main.681","presentation_id":"38929304"},{"card_image_alt_text":"A representative figure from paper main.668","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.668.png","content":{"abstract":"Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities. Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it. Given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from Wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification. Furthermore, we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature, showing they also provide a challenging test case for machine reading algorithms.","authors":["Or Honovich","Lucas Torroba Hennigen","Omri Abend","Shay B. Cohen"],"demo_url":"","keywords":["Machine Events","Machine reading","NLP","classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.668.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.668","similar_paper_uids":["main.668","main.162","main.507","main.544","main.678"],"title":"Machine Reading of Historical Events","tldr":"Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities. Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a...","track":"Information Extraction"},"forum":"main.668","id":"main.668","presentation_id":"38929364"},{"card_image_alt_text":"A representative figure from paper main.720","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.720.png","content":{"abstract":"Named-entities are inherently multilingual, and annotations in any given language may be limited. This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language. However, a straightforward implementation of this simple idea does not always work in practice: naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more training data. The starting point of this paper is a simple solution to this problem, in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts. To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters.","authors":["David Mueller","Nicholas Andrews","Mark Dredze"],"demo_url":"","keywords":["Multilingual Recognition","polyglot recognition","multilingual transfer","naive models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.720.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.720","similar_paper_uids":["main.720","main.490","main.329","main.421","main.536"],"title":"Sources of Transfer in Multilingual Named Entity Recognition","tldr":"Named-entities are inherently multilingual, and annotations in any given language may be limited. This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language...","track":"Information Extraction"},"forum":"main.720","id":"main.720","presentation_id":"38929424"},{"card_image_alt_text":"A representative figure from paper main.522","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.522.png","content":{"abstract":"Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.","authors":["Meihan Tong","Bin Xu","Shuai Wang","Yixin Cao","Lei Hou","Juanzi Li","Jun Xie"],"demo_url":"","keywords":["Event Detection","ED","automatically texts","structuring texts"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.522.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.522","similar_paper_uids":["main.522","main.194","main.769","main.347","main.586"],"title":"Improving Event Detection via Open-domain Trigger Knowledge","tldr":"Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger word...","track":"Information Extraction"},"forum":"main.522","id":"main.522","presentation_id":"38928727"},{"card_image_alt_text":"A representative figure from paper main.523","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.523.png","content":{"abstract":"Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and token labeling are limited to binary classification. We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.","authors":["Canasai Kruengkrai","Thien Hai Nguyen","Sharifah Mahani Aljunied","Lidong Bing"],"demo_url":"","keywords":["Low-Resource Recognition","low-resource NER","NER","binary classification"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.523.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.523","similar_paper_uids":["main.523","main.519","main.336","srw.123","demo.87"],"title":"Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling","tldr":"Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and toke...","track":"Information Extraction"},"forum":"main.523","id":"main.523","presentation_id":"38929237"},{"card_image_alt_text":"A representative figure from paper main.721","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.721.png","content":{"abstract":"In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for \"zero-shot\" open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.","authors":["Colin Lockard","Prashant Shiralkar","Xin Luna Dong","Hannaneh Hajishirzi"],"demo_url":"","keywords":["Zero-Shot Extraction","information extraction","distant supervision","generalization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.721.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.721","similar_paper_uids":["main.721","main.148","main.531","main.128","main.272"],"title":"ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages","tldr":"In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites...","track":"Information Extraction"},"forum":"main.721","id":"main.721","presentation_id":"38929243"},{"card_image_alt_text":"A representative figure from paper main.669","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.669.png","content":{"abstract":"Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE.","authors":["Thy Thy Tran","Phong Le","Sophia Ananiadou"],"demo_url":"","keywords":["Unsupervised Extraction","URE","URE","URE methods"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.669.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.669","similar_paper_uids":["main.669","main.335","main.722","main.600","demo.59"],"title":"Revisiting Unsupervised Relation Extraction","tldr":"Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely...","track":"Information Extraction"},"forum":"main.669","id":"main.669","presentation_id":"38929063"},{"card_image_alt_text":"A representative figure from paper main.137","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.137.png","content":{"abstract":"Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents. But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts. In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new task that we explore in the Biology domain. First, we present the Focused Open Biological Information Extraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow scientific IE system to extract trade-off relations and arguments that are central to biology texts. We then run both the narrow IE system and a state-of-the-art Open IE system on a corpus of 10K open-access scientific biological texts. We show that a significant amount (65%) of erroneous and uninformative Open IE extractions can be filtered using narrow IE extractions. Furthermore, we show that the retained extractions are significantly more often informative to a reader.","authors":["Ruben Kruiper","Julian Vincent","Jessica Chen-Burger","Marc Desmulliez","Ioannis Konstas"],"demo_url":"","keywords":["Semi-Open Extraction","Information","IE","narrow systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.137.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.137","similar_paper_uids":["main.137","main.670","main.714","main.713","main.521"],"title":"In Layman\u2019s Terms: Semi-Open Relation Extraction from Scientific Texts","tldr":"Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents. But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on t...","track":"Information Extraction"},"forum":"main.137","id":"main.137","presentation_id":"38928870"},{"card_image_alt_text":"A representative figure from paper main.719","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.719.png","content":{"abstract":"Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making.","authors":["Zhen Wang","Jennifer Lee","Simon Lin","Huan Sun"],"demo_url":"","keywords":["Medical Prediction","recognition","clinical making","machine models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.719.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.719","similar_paper_uids":["main.719","demo.31","main.408","main.576","main.286"],"title":"Rationalizing Medical Relation Prediction from Corpus-level Statistics","tldr":"Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework insp...","track":"Information Extraction"},"forum":"main.719","id":"main.719","presentation_id":"38929313"},{"card_image_alt_text":"A representative figure from paper main.527","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.527.png","content":{"abstract":"Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts. In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation. We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents. To handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; To leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels. Further, the confidences are used to adjust the training losses of extractors. Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods.","authors":["Daoyuan Chen","Yaliang Li","Kai Lei","Ying Shen"],"demo_url":"","keywords":["entity extraction","re-labeling instances","extraction tasks","re-labeling instance"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.527.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.527","similar_paper_uids":["main.527","main.579","main.572","main.142","main.526"],"title":"Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents","tldr":"Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts. In this paper, we consider the problem of shifted label distribution, ...","track":"Information Extraction"},"forum":"main.527","id":"main.527","presentation_id":"38928879"},{"card_image_alt_text":"A representative figure from paper main.526","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.526.png","content":{"abstract":"The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.","authors":["Zhiwen Xie","Guangyou Zhou","Jin Liu","Jimmy Xiangji Huang"],"demo_url":"","keywords":["Relation-Aware Network","Knowledge Embedding","ReInceptionE","Knowledge embedding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.526.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.526","similar_paper_uids":["main.526","main.572","main.238","main.617","main.241"],"title":"ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding","tldr":"The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al....","track":"Information Extraction"},"forum":"main.526","id":"main.526","presentation_id":"38928714"},{"card_image_alt_text":"A representative figure from paper main.718","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.718.png","content":{"abstract":"We present a novel document-level model for finding argument spans that fill an event's roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.","authors":["Seth Ebner","Patrick Xia","Ryan Culkin","Kyle Rawlins","Benjamin Van Durme"],"demo_url":"","keywords":["sentence-level labeling","coreference resolution","cross-sentence linking","Multi-Sentence Linking"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.718.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.718","similar_paper_uids":["main.718","main.714","main.667","main.141","main.207"],"title":"Multi-Sentence Argument Linking","tldr":"We present a novel document-level model for finding argument spans that fill an event's roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are smal...","track":"Information Extraction"},"forum":"main.718","id":"main.718","presentation_id":"38928805"},{"card_image_alt_text":"A representative figure from paper main.136","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.136.png","content":{"abstract":"Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.","authors":["Zhepei Wei","Jianlin Su","Yue Wang","Yuan Tian","Yi Chang"],"demo_url":"","keywords":["Relational Extraction","large-scale construction","overlapping problem","relational task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.136.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.136","similar_paper_uids":["main.136","main.247","main.574","main.197","main.230"],"title":"A Novel Cascade Binary Tagging Framework for Relational Triple Extraction","tldr":"Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the...","track":"Information Extraction"},"forum":"main.136","id":"main.136","presentation_id":"38928881"},{"card_image_alt_text":"A representative figure from paper main.524","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.524.png","content":{"abstract":"Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state. With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer. Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.","authors":["Chen Jia","Yue Zhang"],"demo_url":"","keywords":["NER Adaptation","Cross-domain NER","multi-task learning","cross-domain transfer"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.524.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.524","similar_paper_uids":["main.524","main.12","main.572","main.578","main.421"],"title":"Multi-Cell Compositional LSTM for NER Domain Adaptation","tldr":"Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LS...","track":"Information Extraction"},"forum":"main.524","id":"main.524","presentation_id":"38929285"},{"card_image_alt_text":"A representative figure from paper main.519","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.519.png","content":{"abstract":"The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question ``which person is mentioned in the text\".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation. We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.","authors":["Xiaoya Li","Jingrong Feng","Yuxian Meng","Qinghong Han","Fei Wu","Jiwei Li"],"demo_url":"","keywords":["Named Recognition","NER","flat NER","flat tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.519.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.519","similar_paper_uids":["main.519","main.577","main.525","main.571","main.528"],"title":"A Unified MRC Framework for Named Entity Recognition","tldr":"The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most wide...","track":"Information Extraction"},"forum":"main.519","id":"main.519","presentation_id":"38928689"},{"card_image_alt_text":"A representative figure from paper main.525","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.525.png","content":{"abstract":"This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER.","authors":["Jue Wang","Lidan Shou","Ke Chen","Gang Chen"],"demo_url":"","keywords":["Nested Recognition","nested NER","Overlapping Recognition","Named Recognition"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.525.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.525","similar_paper_uids":["main.525","main.519","main.528","main.571","main.612"],"title":"Pyramid: A Layered Model for Nested Named Entity Recognition","tldr":"This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Eac...","track":"Information Extraction"},"forum":"main.525","id":"main.525","presentation_id":"38929230"},{"card_image_alt_text":"A representative figure from paper main.581","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.581.png","content":{"abstract":"To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data. Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer. However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language. In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language. The proposed method works for both single-source and multi-source cross-lingual NER. For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models. Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER.","authors":["Qianhui Wu","Zijia Lin","B\u00f6rje Karlsson","Jian-Guang Lou","Biqing Huang"],"demo_url":"","keywords":["Single-/Multi-Source NER","named problem","cross-lingual NER","single-source NER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.581.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.581","similar_paper_uids":["main.581","main.554","main.519","main.139","main.528"],"title":"Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language","tldr":"To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data. Previous works on cross-lingual NER are ...","track":"Information Extraction"},"forum":"main.581","id":"main.581","presentation_id":"38929366"},{"card_image_alt_text":"A representative figure from paper main.580","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.580.png","content":{"abstract":"We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.","authors":["Bodhisattwa Prasad Majumder","Navneet Potti","Sandeep Tata","James Bradley Wendt","Qi Zhao","Marc Najork"],"demo_url":"","keywords":["Information Extraction","extraction task","Representation Learning","extraction system"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.580.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.580","similar_paper_uids":["main.580","main.289","tacl.2001","main.31","main.670"],"title":"Representation Learning for Information Extraction from Form-like Documents","tldr":"We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate ...","track":"Information Extraction"},"forum":"main.580","id":"main.580","presentation_id":"38929320"},{"card_image_alt_text":"A representative figure from paper main.582","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.582.png","content":{"abstract":"Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs. To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit. The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels. To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks. Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.","authors":["Shaowei Chen","Jie Liu","Yu Wang","Wenzheng Zhang","Ziming Chi"],"demo_url":"","keywords":["Aspect-Opinion Extraction","Opinion extraction","fine-grained mining","sentiment classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.582.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.582","similar_paper_uids":["main.582","main.296","main.513","main.295","main.340"],"title":"Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction","tldr":"Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tas...","track":"Information Extraction"},"forum":"main.582","id":"main.582","presentation_id":"38928949"},{"card_image_alt_text":"A representative figure from paper main.140","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.140.png","content":{"abstract":"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.","authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"demo_url":"","keywords":["Relation Extraction","probing tasks","RE","probing task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.140.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.140","similar_paper_uids":["main.140","main.383","cl.1547","cl.1482","main.158"],"title":"Probing Linguistic Features of Sentence-Level Representations in Relation Extraction","tldr":"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. Howev...","track":"Information Extraction"},"forum":"main.140","id":"main.140","presentation_id":"38929091"},{"card_image_alt_text":"A representative figure from paper main.752","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.752.png","content":{"abstract":"Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce \"entity triggers,\" an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.","authors":["Bill Yuchen Lin","Dong-Ho Lee","Ming Shen","Ryan Moreno","Xiao Huang","Prashant Shiralkar","Xiang Ren"],"demo_url":"","keywords":["Named Recognition","NER","supervision","label-efficient models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.752.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.752","similar_paper_uids":["main.752","main.519","main.528","main.139","main.522"],"title":"TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition","tldr":"Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research qu...","track":"Information Extraction"},"forum":"main.752","id":"main.752","presentation_id":"38929157"},{"card_image_alt_text":"A representative figure from paper main.578","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.578.png","content":{"abstract":"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.","authors":["Yuting Wu","Xiao Liu","Yansong Feng","Zheng Wang","Dongyan Zhao"],"demo_url":"","keywords":["Entity Alignment","structural challenge","matching-oriented representations","alignment task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.578.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.578","similar_paper_uids":["main.578","main.524","main.612","main.572","main.512"],"title":"Neighborhood Matching Network for Entity Alignment","tldr":"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN e...","track":"Information Extraction"},"forum":"main.578","id":"main.578","presentation_id":"38929380"},{"card_image_alt_text":"A representative figure from paper main.579","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.579.png","content":{"abstract":"Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate ``distractor'' sentences to augment the bags and train the model to ignore the distractors. Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.","authors":["Hamed Shahbazi","Xiaoli Fern","Reza Ghaeini","Prasad Tadepalli"],"demo_url":"","keywords":["relation extraction","Explanation","neural models","relation models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.579.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.579","similar_paper_uids":["main.579","main.527","main.382","main.771","main.142"],"title":"Relation Extraction with Explanation","tldr":"Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is...","track":"Information Extraction"},"forum":"main.579","id":"main.579","presentation_id":"38929354"},{"card_image_alt_text":"A representative figure from paper main.141","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.141.png","content":{"abstract":"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","authors":["Guoshun Nan","Zhijiang Guo","Ivan Sekulic","Wei Lu"],"demo_url":"","keywords":["Reasoning","Document-Level Extraction","aggregation information","inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.141.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.141","similar_paper_uids":["main.141","main.670","main.135","main.207","main.321"],"title":"Reasoning with Latent Structure Refinement for Document-Level Relation Extraction","tldr":"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the d...","track":"Information Extraction"},"forum":"main.141","id":"main.141","presentation_id":"38929374"},{"card_image_alt_text":"A representative figure from paper main.751","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.751.png","content":{"abstract":"Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.","authors":["Giannis Karamanolakis","Jun Ma","Xin Luna Dong"],"demo_url":"","keywords":["Taxonomy-Aware Extraction","e-Commerce","knowledge extraction","TXtract"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.751.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.751","similar_paper_uids":["main.751","main.199","main.515","main.26","main.33"],"title":"TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories","tldr":"Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Co...","track":"Information Extraction"},"forum":"main.751","id":"main.751","presentation_id":"38929154"},{"card_image_alt_text":"A representative figure from paper main.750","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.750.png","content":{"abstract":"Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been widely studied. To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches. These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues.","authors":["Jing Wang","Mayank Kulkarni","Daniel Preotiuc-Pietro"],"demo_url":"","keywords":["Multi-Domain Recognition","Named recognition","domain models","NER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.750.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.750","similar_paper_uids":["main.750","main.165","main.524","main.681","main.139"],"title":"Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference","tldr":"Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been wid...","track":"Information Extraction"},"forum":"main.750","id":"main.750","presentation_id":"38929187"},{"card_image_alt_text":"A representative figure from paper main.142","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.142.png","content":{"abstract":"TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"demo_url":"","keywords":["TACRED Task","Relation Extraction","Relation RE","unsupervised RE"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.142.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.142","similar_paper_uids":["main.142","main.527","main.444","main.140","main.159"],"title":"TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task","tldr":"TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we inv...","track":"Information Extraction"},"forum":"main.142","id":"main.142","presentation_id":"38928889"},{"card_image_alt_text":"A representative figure from paper tacl.1906","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1906.png","content":{"abstract":"Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages (HRL), but these do not extend well to low-resource languages (LRL) with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the LRL by utilizing resources in closely-related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: we experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared to state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL.","authors":["Shuyan Zhou","Shruti Rijhwani","John Wieting","Jaime Carbonell","Graham Neubig"],"demo_url":"","keywords":["Candidate Generation","Low-resource Linking","Cross-lingual linking","Cross-lingual XEL"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00303","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00303","similar_paper_uids":["tacl.1906","main.336","main.722","main.760","main.252"],"title":"Improving Candidate Generation for Low-resource Cross-lingual Entity Linking","tldr":"Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candi...","track":"Information Extraction"},"forum":"tacl.1906","id":"tacl.1906","presentation_id":"38929509"}]
