[{"card_image_alt_text":"A representative figure from paper main.614","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.614.png","content":{"abstract":"Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries.","authors":["Thanh Nguyen","Nikhil Rao","Karthik Subbian"],"demo_url":"","keywords":["e-Commerce Search","Mitigating problem","ranking algorithms","deep model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.614.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.614","similar_paper_uids":["main.614","main.217","main.208","main.762","main.565"],"title":"Learning Robust Models for e-Commerce Product Search","tldr":"Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search lo...","track":"Information Retrieval and Text Mining"},"forum":"main.614","id":"main.614","presentation_id":"38928718"},{"card_image_alt_text":"A representative figure from paper main.613","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.613.png","content":{"abstract":"We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013\u20132015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.","authors":["Shadi Saleh","Pavel Pecina"],"demo_url":"","keywords":["Document Translation","Query Translation","Cross-Lingual Retrieval","DT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.613.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.613","similar_paper_uids":["main.613","demo.116","main.153","main.554","demo.66"],"title":"Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain","tldr":"We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF ...","track":"Information Retrieval and Text Mining"},"forum":"main.613","id":"main.613","presentation_id":"38929450"},{"card_image_alt_text":"A representative figure from paper main.31","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.31.png","content":{"abstract":"Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.","authors":["Yufeng Zhang","Xueli Yu","Zeyu Cui","Shu Wu","Zhongzhen Wen","Liang Wang"],"demo_url":"","keywords":["Inductive Classification","Text classification","natural processing","NLP"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.31.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.31","similar_paper_uids":["main.31","main.199","main.553","main.207","main.366"],"title":"Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks","tldr":"Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each docum...","track":"Information Retrieval and Text Mining"},"forum":"main.31","id":"main.31","presentation_id":"38929373"},{"card_image_alt_text":"A representative figure from paper main.104","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.104.png","content":{"abstract":"Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.","authors":["Jie Zhou","Chunping Ma","Dingkun Long","Guangwei Xu","Ning Ding","Haoyu Zhang","Pengjun Xie","Gongshen Liu"],"demo_url":"","keywords":["Hierarchical Classification","multi-label classification","inductive features","Hierarchy-Aware Model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.104.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.104","similar_paper_uids":["main.104","main.58","main.27","main.283","main.128"],"title":"Hierarchy-Aware Global Model for Hierarchical Text Classification","tldr":"Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, the...","track":"Information Retrieval and Text Mining"},"forum":"main.104","id":"main.104","presentation_id":"38929004"},{"card_image_alt_text":"A representative figure from paper main.105","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.105.png","content":{"abstract":"Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg","authors":["Florian Boudin","Ygor Gallina","Akiko Aizawa"],"demo_url":"","keywords":["Keyphrase Generation","Scientific Retrieval","document retrieval","retrieval"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.105.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.105","similar_paper_uids":["main.105","demo.24","main.100","main.538","demo.69"],"title":"Keyphrase Generation for Scientific Document Retrieval","tldr":"Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can signif...","track":"Information Retrieval and Text Mining"},"forum":"main.105","id":"main.105","presentation_id":"38928736"},{"card_image_alt_text":"A representative figure from paper main.30","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.30.png","content":{"abstract":"Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.","authors":["Dheeraj Mekala","Jingbo Shang"],"demo_url":"","keywords":["Text Classification","Weakly classification","string matching","Contextualized Supervision"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.30.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.30","similar_paper_uids":["main.30","main.346","main.618","main.431","main.272"],"title":"Contextualized Weak Supervision for Text Classification","tldr":"Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambi...","track":"Information Retrieval and Text Mining"},"forum":"main.30","id":"main.30","presentation_id":"38929298"},{"card_image_alt_text":"A representative figure from paper main.32","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.32.png","content":{"abstract":"Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.","authors":["Rui Wang","Xuemeng Hu","Deyu Zhou","Yulan He","Yuxuan Xiong","Chenchen Ye","Haiyang Xu"],"demo_url":"","keywords":["automatic extraction","model inference","neural modeling","topic inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.32.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.32","similar_paper_uids":["main.32","main.548","main.73","srw.5","main.724"],"title":"Neural Topic Modeling with Bidirectional Adversarial Training","tldr":"Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirich...","track":"Information Retrieval and Text Mining"},"forum":"main.32","id":"main.32","presentation_id":"38928999"},{"card_image_alt_text":"A representative figure from paper main.33","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.33.png","content":{"abstract":"Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different labels to have distinct representations. Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.","authors":["Sora Ohashi","Junya Takayama","Tomoyuki Kajiwara","Chenhui Chu","Yuki Arase"],"demo_url":"","keywords":["Text Classification","text representation","text tasks","single- classifications"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.33.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.33","similar_paper_uids":["main.33","main.197","main.451","tacl.1849","main.374"],"title":"Text Classification with Negative Supervision","tldr":"Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. l...","track":"Information Retrieval and Text Mining"},"forum":"main.33","id":"main.33","presentation_id":"38929209"},{"card_image_alt_text":"A representative figure from paper main.102","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.102.png","content":{"abstract":"This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.","authors":["Ruiying Geng","Binhua Li","Yongbin Li","Jian Sun","Xiaodan Zhu"],"demo_url":"","keywords":["Few-Shot Classification","few-short classification","Dynamic Networks","Dynamic DMIN"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.102.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.102","similar_paper_uids":["main.102","main.128","main.481","main.58","main.99"],"title":"Dynamic Memory Induction Networks for Few-Shot Text Classification","tldr":"This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short clas...","track":"Information Retrieval and Text Mining"},"forum":"main.102","id":"main.102","presentation_id":"38928941"},{"card_image_alt_text":"A representative figure from paper main.103","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.103.png","content":{"abstract":"Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.","authors":["Wang Chen","Hou Pong Chan","Piji Li","Irwin King"],"demo_url":"","keywords":["Deep Generation","Keyphrase generation","Exclusive Decoding","KG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.103.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.103","similar_paper_uids":["main.103","main.710","main.105","main.724","main.617"],"title":"Exclusive Hierarchical Decoding for Deep Keyphrase Generation","tldr":"Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously d...","track":"Information Retrieval and Text Mining"},"forum":"main.103","id":"main.103","presentation_id":"38928959"},{"card_image_alt_text":"A representative figure from paper main.723","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.723.png","content":{"abstract":"We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts.","authors":["Han-Chin Shing","Philip Resnik","Douglas Oard"],"demo_url":"","keywords":["Suicidality Assessment","suicide assessment","ranking problem","resource-bounded retrieval"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.723.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.723","similar_paper_uids":["main.723","main.486","main.49","main.50","main.488"],"title":"A Prioritization Model for Suicidality Risk Assessment","tldr":"We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. Building on measures developed for resource-bounded document retrieval, we introduc...","track":"Information Retrieval and Text Mining"},"forum":"main.723","id":"main.723","presentation_id":"38929150"},{"card_image_alt_text":"A representative figure from paper main.725","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.725.png","content":{"abstract":"Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.","authors":["Yunyi Zhang","Jiaming Shen","Jingbo Shang","Jiawei Han"],"demo_url":"","keywords":["Empower Expansion","Entity expansion","NLP applications","question answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.725.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.725","similar_paper_uids":["main.725","main.574","main.527","main.578","main.612"],"title":"Empower Entity Set Expansion via Language Model Probing","tldr":"Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and t...","track":"Information Retrieval and Text Mining"},"forum":"main.725","id":"main.725","presentation_id":"38929134"},{"card_image_alt_text":"A representative figure from paper main.724","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.724.png","content":{"abstract":"Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the ``ideal'' number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM's novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information -- when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the ``shape'' the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.","authors":["Felipe Viegas","Washington Cunha","Christian Gomes","Ant\u00f4nio Pereira","Leonardo Rocha","Marcos Goncalves"],"demo_url":"","keywords":["data analysis","data exploration","exploration","HTM"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.724.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.724","similar_paper_uids":["main.724","main.32","main.103","main.65","main.104"],"title":"CluHTM - Semantic Hierarchical Topic Modeling based on CluWords","tldr":"Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherenc...","track":"Information Retrieval and Text Mining"},"forum":"main.724","id":"main.724","presentation_id":"38929077"},{"card_image_alt_text":"A representative figure from paper main.29","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.29.png","content":{"abstract":"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","authors":["Joe Barrow","Rajiv Jain","Vlad Morariu","Varun Manjunatha","Douglas Oard","Philip Resnik"],"demo_url":"","keywords":["Document Labeling","Text segmentation","document segmentation","segment labeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.29.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.29","similar_paper_uids":["main.29","main.603","main.277","tacl.1876","main.170"],"title":"A Joint Model for Document Segmentation and Segment Labeling","tldr":"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks ...","track":"Information Retrieval and Text Mining"},"forum":"main.29","id":"main.29","presentation_id":"38929073"},{"card_image_alt_text":"A representative figure from paper main.726","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.726.png","content":{"abstract":"In classification, there are usually some good features that are indicative of class labels. For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment. However, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification). Although deep learning has made significant progresses in generating discriminative features through its powerful representation learning, we believe there is still room for improvement. In this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection. This method projects existing features into the orthogonal space of the common features. The resulting projection is thus perpendicular to the common features and more discriminative for classification. We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results.","authors":["Qi Qin","Wenpeng Hu","Bing Liu"],"demo_url":"","keywords":["Text Classification","classification","sentiment classification","Bert classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.726.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.726","similar_paper_uids":["main.726","main.374","main.341","main.283","main.338"],"title":"Feature Projection for Improved Text Classification","tldr":"In classification, there are usually some good features that are indicative of class labels. For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of...","track":"Information Retrieval and Text Mining"},"forum":"main.726","id":"main.726","presentation_id":"38928956"},{"card_image_alt_text":"A representative figure from paper main.73","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.73.png","content":{"abstract":"This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.","authors":["Masaru Isonuma","Junichiro Mori","Danushka Bollegala","Ichiro Sakata"],"demo_url":"","keywords":["inducing structures","downstream tasks","Tree-Structured Model","doubly-recurrent networks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.73.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.73","similar_paper_uids":["main.73","main.32","main.630","main.548","srw.5"],"title":"Tree-Structured Neural Topic Model","tldr":"This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neu...","track":"Information Retrieval and Text Mining"},"forum":"main.73","id":"main.73","presentation_id":"38928818"},{"card_image_alt_text":"A representative figure from paper main.72","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.72.png","content":{"abstract":"We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst's intention starting from only a small number of sample terms. Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.","authors":["Ryosuke Kohita","Issei Yoshida","Hiroshi Kanayama","Tetsuya Nasukawa"],"demo_url":"","keywords":["Interactive Dictionary","Text Analytics","text analysis","interactive construction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.72.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.72","similar_paper_uids":["main.72","main.143","main.248","main.201","srw.5"],"title":"Interactive Construction of User-Centric Dictionary for Text Analytics","tldr":"We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. Thi...","track":"Information Retrieval and Text Mining"},"forum":"main.72","id":"main.72","presentation_id":"38928823"},{"card_image_alt_text":"A representative figure from paper main.70","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.70.png","content":{"abstract":"Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. In addition, traditional independent word representation in graphical model tends to cause ``term ambiguity\" problem in short text clustering. Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way. Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.","authors":["Jay Kumar","Junming Shao","Salah Uddin","Wazir Ali"],"demo_url":"","keywords":["Short Clustering","Clustering streams","Online Model","sparse representation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.70.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.70","similar_paper_uids":["main.70","main.366","main.548","main.291","main.227"],"title":"An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering","tldr":"Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal b...","track":"Information Retrieval and Text Mining"},"forum":"main.70","id":"main.70","presentation_id":"38928978"},{"card_image_alt_text":"A representative figure from paper main.71","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.71.png","content":{"abstract":"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","authors":["Lin Zheng","Qinliang Su","Dinghan Shen","Changyou Chen"],"demo_url":"","keywords":["Generative Hashing","large-scale retrieval","training","Boltzmann Machines"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.71.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.71","similar_paper_uids":["main.71","main.235","main.364","main.694","main.753"],"title":"Generative Semantic Hashing Enhanced via Boltzmann Machines","tldr":"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized...","track":"Information Retrieval and Text Mining"},"forum":"main.71","id":"main.71","presentation_id":"38928915"},{"card_image_alt_text":"A representative figure from paper main.74","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.74.png","content":{"abstract":"We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. The two models match user queries to FAQ answers and questions, respectively. We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. We show that our model is on par and even outperforms supervised models on existing datasets.","authors":["Yosi Mass","Boaz Carmeli","Haggai Roitman","David Konopnicki"],"demo_url":"","keywords":["Unsupervised Retrieval","Question Generation","Frequently retrieval","fully method"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.74.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.74","similar_paper_uids":["main.74","main.498","main.414","main.652","main.600"],"title":"Unsupervised FAQ Retrieval with Question Generation and BERT","tldr":"We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. Th...","track":"Information Retrieval and Text Mining"},"forum":"main.74","id":"main.74","presentation_id":"38928954"}]
