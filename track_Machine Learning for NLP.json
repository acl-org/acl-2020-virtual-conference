[{"card_image_alt_text":"A representative figure from paper main.615","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.615.png","content":{"abstract":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.","authors":["Clara Meister","Elizabeth Salesky","Ryan Cotterell"],"demo_url":"","keywords":["label smoothing","language tasks","Generalized Regularization","Label Smoothing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.615.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.615","similar_paper_uids":["main.615","main.646","main.316","main.128","main.764"],"title":"Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing","tldr":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connec...","track":"Machine Learning for NLP"},"forum":"main.615","id":"main.615","presentation_id":"38928899"},{"card_image_alt_text":"A representative figure from paper main.198","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.198.png","content":{"abstract":"Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot-product distance metric forms part of the inductive bias of NNLMs. Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability. We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.","authors":["David Demeter","Gregory Kimmel","Doug Downey"],"demo_url":"","keywords":["Neural Models","Neural NNLMs","NNLMs","softmax function"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.198.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.198","similar_paper_uids":["main.198","srw.39","main.264","main.773","main.257"],"title":"Stolen Probability: A Structural Weakness of Neural Language Models","tldr":"Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot...","track":"Machine Learning for NLP"},"forum":"main.198","id":"main.198","presentation_id":"38929181"},{"card_image_alt_text":"A representative figure from paper main.239","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.239.png","content":{"abstract":"This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.","authors":["Tal Perl","Sriram Chaudhury","Raja Giryes"],"demo_url":"","keywords":["sentence reconstruction","transfer learning","Low Tagging","Sentence Reconstruction"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.239.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.239","similar_paper_uids":["main.239","main.688","main.528","main.336","main.750"],"title":"Low Resource Sequence Tagging using Sentence Reconstruction","tldr":"This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from n...","track":"Machine Learning for NLP"},"forum":"main.239","id":"main.239","presentation_id":"38928819"},{"card_image_alt_text":"A representative figure from paper main.588","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.588.png","content":{"abstract":"Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.","authors":["Hao Tang","Donghong Ji","Chenliang Li","Qiji Zhou"],"demo_url":"","keywords":["Aspect-based Classification","Dependency Structure","attention mechanism","Convolutional CNN"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.588.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.588","similar_paper_uids":["main.588","main.295","main.67","main.605","main.224"],"title":"Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification","tldr":"Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and...","track":"Machine Learning for NLP"},"forum":"main.588","id":"main.588","presentation_id":"38928780"},{"card_image_alt_text":"A representative figure from paper main.589","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.589.png","content":{"abstract":"We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.","authors":["Thanh-Tung Nguyen","Xuan-Phi Nguyen","Shafiq Joty","Xiaoli Li"],"demo_url":"","keywords":["Dynamic Attention","dynamic selection","NLP tasks","machine translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.589.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.589","similar_paper_uids":["main.589","main.311","main.147","main.312","main.517"],"title":"Differentiable Window for Dynamic Local Attention","tldr":"We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modu...","track":"Machine Learning for NLP"},"forum":"main.589","id":"main.589","presentation_id":"38928814"},{"card_image_alt_text":"A representative figure from paper main.238","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.238.png","content":{"abstract":"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.","authors":["Mrinmaya Sachan"],"demo_url":"","keywords":["AI applications","reasoning tasks","KG inference","Knowledge Compression"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.238.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.238","similar_paper_uids":["main.238","main.412","main.617","main.572","main.526"],"title":"Knowledge Graph Embedding Compression","tldr":"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. Th...","track":"Machine Learning for NLP"},"forum":"main.238","id":"main.238","presentation_id":"38928878"},{"card_image_alt_text":"A representative figure from paper main.199","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.199.png","content":{"abstract":"Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.","authors":["Chao Shang","Sarthak Dash","Md. Faisal Mahbub Chowdhury","Nandana Mihindukulasooriya","Alfio Gliozzo"],"demo_url":"","keywords":["Taxonomy Domains","Graph-based Transfer","Extracting relations","taxonomy construction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.199.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.199","similar_paper_uids":["main.199","main.640","main.362","main.31","main.224"],"title":"Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer","tldr":"Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many...","track":"Machine Learning for NLP"},"forum":"main.199","id":"main.199","presentation_id":"38928766"},{"card_image_alt_text":"A representative figure from paper main.358","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.358.png","content":{"abstract":"In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity. Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations. It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases. Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework. Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.","authors":["Wentao Xu","Shun Zheng","Liang He","Bin Shao","Jian Yin","Tie-Yan Liu"],"demo_url":"","keywords":["Segmented Graphs","knowledge embedding","artificial intelligence","recommendation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.358.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.358","similar_paper_uids":["main.358","main.347","main.204","main.302","main.290"],"title":"SEEK: Segmented Embedding of Knowledge Graphs","tldr":"In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods ...","track":"Machine Learning for NLP"},"forum":"main.358","id":"main.358","presentation_id":"38929038"},{"card_image_alt_text":"A representative figure from paper main.616","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.616.png","content":{"abstract":"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.","authors":["Yekun Chai","Shuo Jin","Xinwen Hou"],"demo_url":"","keywords":["sequence tasks","optimization process","Highway Transformer","Self-Gating Networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.616.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.616","similar_paper_uids":["main.616","main.385","tacl.1815","main.687","main.411"],"title":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks","tldr":"Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo info...","track":"Machine Learning for NLP"},"forum":"main.616","id":"main.616","presentation_id":"38928904"},{"card_image_alt_text":"A representative figure from paper main.617","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.617.png","content":{"abstract":"Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.","authors":["Ines Chami","Adva Wolf","Da-Cheng Juan","Frederic Sala","Sujith Ravi","Christopher R\u00e9"],"demo_url":"","keywords":["high-fidelity representations","Low-Dimensional Embeddings","Knowledge embeddings","KGs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.617.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.617","similar_paper_uids":["main.617","main.238","main.412","main.526","main.241"],"title":"Low-Dimensional Hyperbolic Knowledge Graph Embeddings","tldr":"Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hype...","track":"Machine Learning for NLP"},"forum":"main.617","id":"main.617","presentation_id":"38928761"},{"card_image_alt_text":"A representative figure from paper main.439","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.439.png","content":{"abstract":"Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse. We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).","authors":["Dan Iter","Kelvin Guu","Larry Lansing","Dan Jurafsky"],"demo_url":"","keywords":["Discourse","unsupervised text","contextual representations","discourse-level representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.439.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.439","similar_paper_uids":["main.439","main.247","main.250","main.451","tacl.1853"],"title":"Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models","tldr":"Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objecti...","track":"Machine Learning for NLP"},"forum":"main.439","id":"main.439","presentation_id":"38928972"},{"card_image_alt_text":"A representative figure from paper main.388","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.388.png","content":{"abstract":"Multi-task Learning methods have achieved great progress in text classification. However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications. To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption. The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.","authors":["Yuren Mao","Shuang Yun","Weiwei Liu","Bo Du"],"demo_url":"","keywords":["Multi-task Classification","text classification","multi-task problems","convex problems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.388.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.388","similar_paper_uids":["main.388","main.402","main.273","main.222","main.86"],"title":"Tchebycheff Procedure for Multi-task Text Classification","tldr":"Multi-task Learning methods have achieved great progress in text classification. However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applica...","track":"Machine Learning for NLP"},"forum":"main.388","id":"main.388","presentation_id":"38928751"},{"card_image_alt_text":"A representative figure from paper main.202","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.202.png","content":{"abstract":"Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.","authors":["Subhabrata Mukherjee","Ahmed Hassan Awadallah"],"demo_url":"","keywords":["natural tasks","knowledge distillation","multilingual Recognition","multilingual NER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.202.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.202","similar_paper_uids":["main.202","main.528","main.250","main.89","main.195"],"title":"XtremeDistil: Multi-stage Distillation for Massive Multilingual Models","tldr":"Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to ...","track":"Machine Learning for NLP"},"forum":"main.202","id":"main.202","presentation_id":"38929189"},{"card_image_alt_text":"A representative figure from paper main.438","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.438.png","content":{"abstract":"Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.","authors":["Xingyuan Pan","Maitrey Mehta","Vivek Srikumar"],"demo_url":"","keywords":["Structured Prediction","natural tasks","structured problems","construction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.438.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.438","similar_paper_uids":["main.438","main.744","main.325","main.188","main.200"],"title":"Learning Constraints for Structured Prediction Using Rectifier Networks","tldr":"Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improv...","track":"Machine Learning for NLP"},"forum":"main.438","id":"main.438","presentation_id":"38928961"},{"card_image_alt_text":"A representative figure from paper main.189","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.189.png","content":{"abstract":"Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.","authors":["Kiant\u00e9 Brantley","Hal Daum\u00e9 III","Amr Sharaf"],"demo_url":"","keywords":["Active Learning","structured tasks","sequence tasks","Imitation algorithms"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.189.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.189","similar_paper_uids":["main.189","main.390","main.62","main.215","demo.87"],"title":"Active Imitation Learning with Noisy Guidance","tldr":"Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried s...","track":"Machine Learning for NLP"},"forum":"main.189","id":"main.189","presentation_id":"38929087"},{"card_image_alt_text":"A representative figure from paper main.200","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.200.png","content":{"abstract":"Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%. Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.","authors":["Sinong Wang","Madian Khabsa","Hao Ma"],"demo_url":"","keywords":["text tasks","Pretrainng","Pretraining models","NLP models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.200.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.200","similar_paper_uids":["main.200","main.45","main.705","main.24","main.76"],"title":"To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks","tldr":"Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in...","track":"Machine Learning for NLP"},"forum":"main.200","id":"main.200","presentation_id":"38929086"},{"card_image_alt_text":"A representative figure from paper main.201","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.201.png","content":{"abstract":"Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.","authors":["Mozhi Zhang","Yoshinari Fujinuma","Michael J. Paul","Jordan Boyd-Graber"],"demo_url":"","keywords":["Dictionaries","BLI","generalization","downstream tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.201.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.201","similar_paper_uids":["main.201","main.143","main.618","tacl.1766","main.766"],"title":"Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries","tldr":"Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization t...","track":"Machine Learning for NLP"},"forum":"main.201","id":"main.201","presentation_id":"38928729"},{"card_image_alt_text":"A representative figure from paper main.188","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.188.png","content":{"abstract":"We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.","authors":["Abhyuday Jagannatha","Hong Yu"],"demo_url":"","keywords":["Natural Processing","natural applications","NLP applications","named recognition"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.188.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.188","similar_paper_uids":["main.188","main.242","main.278","main.438","main.575"],"title":"Calibrating Structured Output Predictors for Natural Language Processing","tldr":"We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calib...","track":"Machine Learning for NLP"},"forum":"main.188","id":"main.188","presentation_id":"38929172"},{"card_image_alt_text":"A representative figure from paper main.266","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.266.png","content":{"abstract":"We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing. Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures. We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.","authors":["Kartik Goyal","Chris Dyer","Christopher Warren","Maxwell G'Sell","Taylor Berg-Kirkpatrick"],"demo_url":"","keywords":["Typographical Printing","clustering images","archiving process","Early printing"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.266.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.266","similar_paper_uids":["main.266","main.694","main.316","main.753","main.437"],"title":"A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing","tldr":"We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of v...","track":"Machine Learning for NLP"},"forum":"main.266","id":"main.266","presentation_id":"38929423"},{"card_image_alt_text":"A representative figure from paper main.272","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.272.png","content":{"abstract":"Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification","authors":["Zhiquan Ye","Yuxia Geng","Jiaoyan Chen","Jingmin Chen","Xiaoxiao Xu","Suhang Zheng","Feng Wang","Jun Zhang","Huajun Chen"],"demo_url":"","keywords":["Zero-shot Classification","Reinforced Self-training","Zero-shot learning","self-training method"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.272.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.272","similar_paper_uids":["main.272","tacl.1801","main.581","main.148","main.128"],"title":"Zero-shot Text Classification via Reinforced Self-training","tldr":"Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To t...","track":"Machine Learning for NLP"},"forum":"main.272","id":"main.272","presentation_id":"38929229"},{"card_image_alt_text":"A representative figure from paper main.267","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.267.png","content":{"abstract":"Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L\u221e norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.","authors":["Chuhan Wu","Fangzhao Wu","Tao Qi","Xiaohui Cui","Yongfeng Huang"],"demo_url":"","keywords":["Text Representation","text representations","model training","Pooling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.267.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.267","similar_paper_uids":["main.267","main.283","main.517","main.431","main.90"],"title":"Attentive Pooling with Learnable Norms for Text Representation","tldr":"Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L\u221e norm of input feat...","track":"Machine Learning for NLP"},"forum":"main.267","id":"main.267","presentation_id":"38929028"},{"card_image_alt_text":"A representative figure from paper main.313","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.313.png","content":{"abstract":"Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples. R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple. Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.","authors":["Dai Quoc Nguyen","Tu Nguyen","Dinh Phung"],"demo_url":"","keywords":["Triple Classification","Search Personalization","search problems","SEARCH17"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.313.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.313","similar_paper_uids":["main.313","main.526","main.643","main.481","main.385"],"title":"A Relational Memory-based Embedding Model for Triple Classification and Search Personalization","tldr":"Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explo...","track":"Machine Learning for NLP"},"forum":"main.313","id":"main.313","presentation_id":"38928911"},{"card_image_alt_text":"A representative figure from paper main.271","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.271.png","content":{"abstract":"Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.","authors":["Ryosuke Kuwabara","Jun Suzuki","Hideki Nakayama"],"demo_url":"","keywords":["text tasks","Single Ensemble","Distinct Vectors","Model techniques"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.271.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.271","similar_paper_uids":["main.271","main.625","main.304","main.593","main.517"],"title":"Single Model Ensemble using Pseudo-Tags and Distinct Vectors","tldr":"Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single ...","track":"Machine Learning for NLP"},"forum":"main.271","id":"main.271","presentation_id":"38929422"},{"card_image_alt_text":"A representative figure from paper main.270","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.270.png","content":{"abstract":"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.","authors":["Ofir Press","Noah A. Smith","Omer Levy"],"demo_url":"","keywords":["task-specific reorderings","Transformer Models","Multilayer networks","randomly transformers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.270.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.270","similar_paper_uids":["main.270","main.145","srw.9","tacl.1815","main.411"],"title":"Improving Transformer Models by Reordering their Sublayers","tldr":"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the languag...","track":"Machine Learning for NLP"},"forum":"main.270","id":"main.270","presentation_id":"38928925"},{"card_image_alt_text":"A representative figure from paper main.314","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.314.png","content":{"abstract":"It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.","authors":["Ning Miao","Yuxuan Song","Hao Zhou","Lei Li"],"demo_url":"","keywords":["over- problem","text tasks","Tailoring Models","Monte-Carlo Methods"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.314.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.314","similar_paper_uids":["main.314","main.197","main.357","main.214","main.249"],"title":"Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods","tldr":"It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In ...","track":"Machine Learning for NLP"},"forum":"main.314","id":"main.314","presentation_id":"38928919"},{"card_image_alt_text":"A representative figure from paper main.248","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.248.png","content":{"abstract":"Sentence ordering is the task of arranging the sentences of a given text in the correct order. Recent work using deep neural networks for this task has framed it as a sequence prediction problem. In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it. Additionally, we propose a human evaluation for this task. The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents.","authors":["Shrimai Prabhumoye","Ruslan Salakhutdinov","Alan W Black"],"demo_url":"","keywords":["Sentence Ordering","sequence problem","constraint problem","Topological Sort"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.248.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.248","similar_paper_uids":["main.248","main.453","main.710","main.709","main.767"],"title":"Topological Sort for Sentence Ordering","tldr":"Sentence ordering is the task of arranging the sentences of a given text in the correct order. Recent work using deep neural networks for this task has framed it as a sequence prediction problem. In this paper, we propose a new framing of this task a...","track":"Machine Learning for NLP"},"forum":"main.248","id":"main.248","presentation_id":"38929117"},{"card_image_alt_text":"A representative figure from paper main.249","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.249.png","content":{"abstract":"Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.","authors":["Keita Kurita","Paul Michel","Graham Neubig"],"demo_url":"","keywords":["Weight Attacks","sentiment classification","toxicity detection","spam detection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.249.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.249","similar_paper_uids":["main.249","main.197","main.357","main.540","main.314"],"title":"Weight Poisoning Attacks on Pretrained Models","tldr":"Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted p...","track":"Machine Learning for NLP"},"forum":"main.249","id":"main.249","presentation_id":"38928910"},{"card_image_alt_text":"A representative figure from paper main.671","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.671.png","content":{"abstract":"We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called ``trigger'' words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.","authors":["Tassilo Klein","Moin Nabi"],"demo_url":"","keywords":["Commonsense Reasoning","Pronoun problems","pronoun disambiguation","commonsense tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.671.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.671","similar_paper_uids":["main.671","main.357","main.247","main.508","main.687"],"title":"Contrastive Self-Supervised Learning for Commonsense Reasoning","tldr":"We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called ``trigger'' words, which are responsible for flipp...","track":"Machine Learning for NLP"},"forum":"main.671","id":"main.671","presentation_id":"38929108"},{"card_image_alt_text":"A representative figure from paper main.315","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.315.png","content":{"abstract":"Most Chinese pre-trained models take character as the basic unit and learn representation according to character's external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. As a result, word and character information are explicitly integrated at the fine-tuning procedure. Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.","authors":["Yanzeng Li","Bowen Yu","Xue Mengge","Tingwen Liu"],"demo_url":"","keywords":["segmentation propagation","Pre-trained Representation","Chinese models","word-aligned attention"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.315.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.315","similar_paper_uids":["main.315","tacl.1876","main.528","main.311","main.82"],"title":"Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention","tldr":"Most Chinese pre-trained models take character as the basic unit and learn representation according to character's external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propo...","track":"Machine Learning for NLP"},"forum":"main.315","id":"main.315","presentation_id":"38928721"},{"card_image_alt_text":"A representative figure from paper main.317","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.317.png","content":{"abstract":"State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level). Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks. To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.","authors":["Mao Ye","Chengyue Gong","Qiang Liu"],"demo_url":"","keywords":["Certified Robustness","Adversarial Substitutions","human-unaware transformations","ensemble"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.317.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.317","similar_paper_uids":["main.317","main.540","main.244","main.245","main.197"],"title":"SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions","tldr":"State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the...","track":"Machine Learning for NLP"},"forum":"main.317","id":"main.317","presentation_id":"38928757"},{"card_image_alt_text":"A representative figure from paper main.673","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.673.png","content":{"abstract":"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.","authors":["Pengyu Cheng","Martin Renqiang Min","Dinghan Shen","Christopher Malon","Yizhe Zhang","Yitong Li","Lawrence Carin"],"demo_url":"","keywords":["Learning language","NLP tasks","conditional generation","style transfer"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.673.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.673","similar_paper_uids":["main.673","main.639","main.169","srw.17","main.354"],"title":"Improving Disentangled Text Representation Learning with Information-Theoretic Guidance","tldr":"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, ...","track":"Machine Learning for NLP"},"forum":"main.673","id":"main.673","presentation_id":"38929080"},{"card_image_alt_text":"A representative figure from paper main.672","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.672.png","content":{"abstract":"Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL --- a Transformer augmented with a long-range memory of past activations --- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.","authors":["Jack Rae","Ali Razavi"],"demo_url":"","keywords":["modelling data","language modelling","Transformers","Deep models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.672.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.672","similar_paper_uids":["main.672","main.37","main.687","main.250","main.38"],"title":"Do Transformers Need Deep Long-Range Memory?","tldr":"Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL --- a Transformer augmented with a long-range memory of past activations --- has been shown to be state...","track":"Machine Learning for NLP"},"forum":"main.672","id":"main.672","presentation_id":"38928897"},{"card_image_alt_text":"A representative figure from paper main.316","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.316.png","content":{"abstract":"Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy. Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space. We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.","authors":["Chen Wu","Prince Zizhuang Wang","William Yang Wang"],"demo_url":"","keywords":["Encoder-Decoder Incompatibility","Variational Modeling","text modeling","probability estimation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.316.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.316","similar_paper_uids":["main.316","main.235","main.753","main.694","main.36"],"title":"On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond","tldr":"Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dyna...","track":"Machine Learning for NLP"},"forum":"main.316","id":"main.316","presentation_id":"38928764"},{"card_image_alt_text":"A representative figure from paper main.247","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.247.png","content":{"abstract":"BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model\u2019s parameters, it is selected from a relevant passage. We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets. Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.","authors":["Michael Glass","Alfio Gliozzo","Rishav Chakravarti","Anthony Ferritto","Lin Pan","G P Shrivatsa Bhargav","Dinesh Garg","Avi Sil"],"demo_url":"","keywords":["Question Answering","language tasks","Next Prediction","pre-training task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.247.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.247","similar_paper_uids":["main.247","tacl.1853","main.705","tacl.1849","main.195"],"title":"Span Selection Pre-training for Question Answering","tldr":"BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks...","track":"Machine Learning for NLP"},"forum":"main.247","id":"main.247","presentation_id":"38929129"},{"card_image_alt_text":"A representative figure from paper main.246","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.246.png","content":{"abstract":"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled \"Show Your Work: Improved Reporting of Experimental Results\" (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. We analytically show that their estimator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/castorini/meanmax.","authors":["Raphael Tang","Jaejun Lee","Ji Xin","Xinyu Liu","Yaoliang Yu","Jimmy Lin"],"demo_url":"","keywords":["natural processing","neural networks","statistical simulation","statistical generalizability"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.246.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.246","similar_paper_uids":["main.246","main.391","main.764","demo.86","main.4"],"title":"Showing Your Work Doesn't Always Work","tldr":"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled \"Show Your Work: Improved Reporting of Experimental Results\" (Dodge et al., 2019...","track":"Machine Learning for NLP"},"forum":"main.246","id":"main.246","presentation_id":"38929111"},{"card_image_alt_text":"A representative figure from paper main.250","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.250.png","content":{"abstract":"Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.","authors":["Ashish Khetan","Zohar Karnin"],"demo_url":"","keywords":["Optimizing BERT","natural tasks","schuBERT","Transformers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.250.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.250","similar_paper_uids":["main.250","main.195","main.247","main.38","main.204"],"title":"schuBERT: Optimizing Elements of BERT","tldr":"Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE,...","track":"Machine Learning for NLP"},"forum":"main.250","id":"main.250","presentation_id":"38929349"},{"card_image_alt_text":"A representative figure from paper main.244","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.244.png","content":{"abstract":"Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","authors":["Dan Hendrycks","Xiaoyuan Liu","Eric Wallace","Adam Dziedzic","Rishabh Krishnan","Dawn Song"],"demo_url":"","keywords":["out-of-distribution generalization","detecting examples","distillation","Pretrained"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.244.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.244","similar_paper_uids":["main.244","main.769","main.770","main.245","main.247"],"title":"Pretrained Transformers Improve Out-of-Distribution Robustness","tldr":"Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new ...","track":"Machine Learning for NLP"},"forum":"main.244","id":"main.244","presentation_id":"38929340"},{"card_image_alt_text":"A representative figure from paper main.245","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.245.png","content":{"abstract":"Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.","authors":["Erik Jones","Robin Jia","Aditi Raghunathan","Percy Liang"],"demo_url":"","keywords":["Robust Encodings","NLP systems","RobEn","model architecture"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.245.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.245","similar_paper_uids":["main.245","main.540","srw.105","main.244","main.317"],"title":"Robust Encodings: A Framework for Combating Adversarial Typos","tldr":"Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks o...","track":"Machine Learning for NLP"},"forum":"main.245","id":"main.245","presentation_id":"38929357"},{"card_image_alt_text":"A representative figure from paper main.645","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.645.png","content":{"abstract":"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.","authors":["Louis Martin","Benjamin Muller","Pedro Javier Ortiz Su\u00e1rez","Yoann Dupont","Laurent Romary","\u00c9ric de la Clergerie","Djam\u00e9 Seddah","Beno\u00eet Sagot"],"demo_url":"","keywords":["Natural Processing","part-of-speech tagging","dependency parsing","named recognition"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.645.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.645","similar_paper_uids":["main.645","main.156","main.80","main.722","srw.79"],"title":"CamemBERT: a Tasty French Language Model","tldr":"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of suc...","track":"Machine Learning for NLP"},"forum":"main.645","id":"main.645","presentation_id":"38929193"},{"card_image_alt_text":"A representative figure from paper main.241","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.241.png","content":{"abstract":"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.","authors":["Yun Tang","Jing Huang","Guangtao Wang","Xiaodong He","Bowen Zhou"],"demo_url":"","keywords":["Knowledge Embedding","knowledge task","knowledge prediction","Orthogonal Transforms"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.241.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.241","similar_paper_uids":["main.241","main.640","main.67","tacl.1805","main.553"],"title":"Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding","tldr":"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain cha...","track":"Machine Learning for NLP"},"forum":"main.241","id":"main.241","presentation_id":"38928836"},{"card_image_alt_text":"A representative figure from paper main.269","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.269.png","content":{"abstract":"Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.","authors":["Xinwei Geng","Longyue Wang","Xing Wang","Bing Qin","Ting Liu","Zhaopeng Tu"],"demo_url":"","keywords":["NLP tasks","natural inference","semantic labelling","machine translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.269.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.269","similar_paper_uids":["main.269","main.153","cl.1552","main.311","tacl.1815"],"title":"How Does Selective Mechanism Improve Self-Attention Networks?","tldr":"Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained....","track":"Machine Learning for NLP"},"forum":"main.269","id":"main.269","presentation_id":"38929046"},{"card_image_alt_text":"A representative figure from paper main.268","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.268.png","content":{"abstract":"Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, \\ie they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation.","authors":["Fynn Schr\u00f6der","Chris Biemann"],"demo_url":"","keywords":["multi-task tasks","MTL","TL","MTL setups"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.268.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.268","similar_paper_uids":["main.268","main.581","main.297","main.122","main.283"],"title":"Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks","tldr":"Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming ...","track":"Machine Learning for NLP"},"forum":"main.268","id":"main.268","presentation_id":"38929002"},{"card_image_alt_text":"A representative figure from paper main.240","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.240.png","content":{"abstract":"Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.","authors":["Julian Salazar","Davis Liang","Toan Q. Nguyen","Katrin Kirchhoff"],"demo_url":"","keywords":["Masked Scoring","NLP tasks","domain adaptation","language scoring"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.240.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.240","similar_paper_uids":["main.240","main.391","main.18","demo.69","demo.67"],"title":"Masked Language Model Scoring","tldr":"Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scor...","track":"Machine Learning for NLP"},"forum":"main.240","id":"main.240","presentation_id":"38929391"},{"card_image_alt_text":"A representative figure from paper main.646","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.646.png","content":{"abstract":"Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively. We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language. This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources. Still, a favourite can be named based on convenience. We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field.","authors":["Tom Pelsmaeker","Wilker Aziz"],"demo_url":"","keywords":["Estimation Models","parameterisation models","posterior collapse","language modelling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.646.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.646","similar_paper_uids":["main.646","main.506","main.177","main.615","main.468"],"title":"Effective Estimation of Deep Generative Language Models","tldr":"Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, due ...","track":"Machine Learning for NLP"},"forum":"main.646","id":"main.646","presentation_id":"38929441"},{"card_image_alt_text":"A representative figure from paper main.242","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.242.png","content":{"abstract":"Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.","authors":["Taehee Jung","Dongyeop Kang","Hua Cheng","Lucas Mentch","Thomas Schaaf"],"demo_url":"","keywords":["Sentence Tasks","classifications","xSLUE","classification tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.242.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.242","similar_paper_uids":["main.242","main.188","main.278","main.503","main.235"],"title":"Posterior Calibrated Training on Sentence Classification Tasks","tldr":"Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g...","track":"Machine Learning for NLP"},"forum":"main.242","id":"main.242","presentation_id":"38929102"},{"card_image_alt_text":"A representative figure from paper main.243","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.243.png","content":{"abstract":"Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. Experiments consider applications of this approach for text generation. We find that this method improves over standard benchmarks, while also providing fine-grained control.","authors":["Xiang Lisa Li","Alexander Rush"],"demo_url":"","keywords":["Posterior Generation","Text generation","deep models","neural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.243.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.243","similar_paper_uids":["main.243","main.314","main.544","main.18","main.354"],"title":"Posterior Control of Blackbox Generation","tldr":"Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discret...","track":"Machine Learning for NLP"},"forum":"main.243","id":"main.243","presentation_id":"38929245"},{"card_image_alt_text":"A representative figure from paper main.647","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.647.png","content":{"abstract":"The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.","authors":["Shauli Ravfogel","Yanai Elazar","Hila Gonen","Michael Twiton","Yoav Goldberg"],"demo_url":"","keywords":["multi-class classification","Iterative Projection","Iterative ","neural representation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.647.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.647","similar_paper_uids":["main.647","main.326","main.262","main.404","main.260"],"title":"Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection","tldr":"The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for ...","track":"Machine Learning for NLP"},"forum":"main.647","id":"main.647","presentation_id":"38929453"},{"card_image_alt_text":"A representative figure from paper main.191","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.191.png","content":{"abstract":"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.","authors":["Danilo Croce","Giuseppe Castellucci","Roberto Basili"],"demo_url":"","keywords":["Robust Classification","Natural tasks","image processing","generative setting"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.191.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.191","similar_paper_uids":["main.191","srw.105","main.263","main.590","main.200"],"title":"GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples","tldr":"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtai...","track":"Machine Learning for NLP"},"forum":"main.191","id":"main.191","presentation_id":"38928799"},{"card_image_alt_text":"A representative figure from paper main.190","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.190.png","content":{"abstract":"Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to \"interpret\" these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3--20x less labeled data and improves on the baseline by 3--10 F1 points with the same amount of labeled data.","authors":["Shikhar Murty","Pang Wei Koh","Percy Liang"],"demo_url":"","keywords":["relation tasks","ExpBERT","Natural Explanations","explanation-guided representations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.190.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.190","similar_paper_uids":["main.190","main.212","main.195","main.768","main.771"],"title":"ExpBERT: Representation Engineering with Natural Language Explanations","tldr":"Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural langua...","track":"Machine Learning for NLP"},"forum":"main.190","id":"main.190","presentation_id":"38928962"},{"card_image_alt_text":"A representative figure from paper main.423","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.423.png","content":{"abstract":"The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the `Word in Context' task.","authors":["Yoav Levine","Barak Lenz","Or Dagan","Ori Ram","Dan Padnos","Or Sharir","Shai Shalev-Shwartz","Amnon Shashua","Yoav Shoham"],"demo_url":"","keywords":["natural understanding","lexical understanding","SemEval Disambiguation","task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.423.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.423","similar_paper_uids":["main.423","main.369","main.197","main.346","main.95"],"title":"SenseBERT: Driving Some Sense into BERT","tldr":"The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate ...","track":"Machine Learning for NLP"},"forum":"main.423","id":"main.423","presentation_id":"38929368"},{"card_image_alt_text":"A representative figure from paper main.437","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.437.png","content":{"abstract":"While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.","authors":["Shuning Jin","Sam Wiseman","Karl Stratos","Karen Livescu"],"demo_url":"","keywords":["Low-Resource Classification","Discrete Representations","discrete models","continuous representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.437.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.437","similar_paper_uids":["main.437","main.694","main.753","main.336","main.2"],"title":"Discrete Latent Variable Representations for Low-Resource Text Classification","tldr":"While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning disc...","track":"Machine Learning for NLP"},"forum":"main.437","id":"main.437","presentation_id":"38929414"},{"card_image_alt_text":"A representative figure from paper main.192","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.192.png","content":{"abstract":"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","authors":["Zhengbao Jiang","Wei Xu","Jun Araki","Graham Neubig"],"demo_url":"","keywords":["Natural Analysis","Natural processing","dependency parsing","semantic labeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.192.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.192","similar_paper_uids":["main.192","main.111","tacl.1853","main.341","srw.123"],"title":"Generalizing Natural Language Analysis through Span-relation Representations","tldr":"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a ...","track":"Machine Learning for NLP"},"forum":"main.192","id":"main.192","presentation_id":"38928920"},{"card_image_alt_text":"A representative figure from paper main.193","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.193.png","content":{"abstract":"Sequence labeling is a fundamental task for a range of natural language processing problems. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders.","authors":["Ouyu Lan","Xiao Huang","Bill Yuchen Lin","He Jiang","Liyuan Liu","Xiang Ren"],"demo_url":"","keywords":["Sequence Labeling","natural problems","crowd annotation","multi-source learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.193.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.193","similar_paper_uids":["main.193","main.581","main.627","main.3","main.136"],"title":"Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling","tldr":"Sequence labeling is a fundamental task for a range of natural language processing problems. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often co...","track":"Machine Learning for NLP"},"forum":"main.193","id":"main.193","presentation_id":"38928969"},{"card_image_alt_text":"A representative figure from paper main.356","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.356.png","content":{"abstract":"This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.","authors":["Oren Barkan","Idan Rejwan","Avi Caciularu","Noam Koenigstein"],"demo_url":"","keywords":["Bayesian modeling","Bayesian Learning","BHWR","Variational learning"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.356.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.356","similar_paper_uids":["main.356","main.368","main.544","main.37","main.65"],"title":"Bayesian Hierarchical Words Representation Learning","tldr":"This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant in...","track":"Machine Learning for NLP"},"forum":"main.356","id":"main.356","presentation_id":"38929155"},{"card_image_alt_text":"A representative figure from paper main.197","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.197.png","content":{"abstract":"Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.","authors":["Haoming Jiang","Pengcheng He","Weizhu Chen","Xiaodong Liu","Jianfeng Gao","Tuo Zhao"],"demo_url":"","keywords":["NLP","generalization","NLP tasks","SMART"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.197.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.197","similar_paper_uids":["main.197","main.249","main.314","main.357","main.214"],"title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization","tldr":"Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources ...","track":"Machine Learning for NLP"},"forum":"main.197","id":"main.197","presentation_id":"38928798"},{"card_image_alt_text":"A representative figure from paper main.236","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.236.png","content":{"abstract":"We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.","authors":["Simran Arora","Avner May","Jian Zhang","Christopher R\u00e9"],"demo_url":"","keywords":["Contextual Embeddings","deep embeddings","pretrained embeddings","GloVe"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.236.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.236","similar_paper_uids":["main.236","main.431","main.434","main.156","main.405"],"title":"Contextual Embeddings: When Are They Worth It?","tldr":"We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of ...","track":"Machine Learning for NLP"},"forum":"main.236","id":"main.236","presentation_id":"38929148"},{"card_image_alt_text":"A representative figure from paper main.593","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.593.png","content":{"abstract":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) \u201cexit\u201d from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.","authors":["Roy Schwartz","Gabriel Stanovsky","Swabha Swayamdipta","Jesse Dodge","Noah A. Smith"],"demo_url":"","keywords":["inference","early decisions","costly retraining","Job Model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.593.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.593","similar_paper_uids":["main.593","main.537","main.204","main.504","main.325"],"title":"The Right Tool for the Job: Matching Model and Instance Complexities","tldr":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tu...","track":"Machine Learning for NLP"},"forum":"main.593","id":"main.593","presentation_id":"38929251"},{"card_image_alt_text":"A representative figure from paper main.587","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.587.png","content":{"abstract":"Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead \u201creallocate\u201d them\u2014the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over \u201ctransformer-base\u201d by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.","authors":["Hao Peng","Roy Schwartz","Dianqi Li","Noah A. Smith"],"demo_url":"","keywords":["natural tasks","machine translation","language modeling","Multi-head architectures"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.587.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.587","similar_paper_uids":["main.587","main.40","srw.16","main.687","main.311"],"title":"A Mixture of h - 1 Heads is Better than h Heads","tldr":"Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss...","track":"Machine Learning for NLP"},"forum":"main.587","id":"main.587","presentation_id":"38929434"},{"card_image_alt_text":"A representative figure from paper main.592","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.592.png","content":{"abstract":"Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.","authors":["Yinqiao Li","Chi Hu","Yuhao Zhang","Nuo Xu","Yufan Jiang","Tong Xiao","Jingbo Zhu","Tongran Liu","Changliang Li"],"demo_url":"","keywords":["Language Modeling","intra-cell NAS","recurrent modeling","CoNLL task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.592.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.592","similar_paper_uids":["main.592","main.197","main.158","main.140","main.581"],"title":"Learning Architectures from an Extended Search Space for Language Modeling","tldr":"Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we prese...","track":"Machine Learning for NLP"},"forum":"main.592","id":"main.592","presentation_id":"38928944"},{"card_image_alt_text":"A representative figure from paper main.237","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.237.png","content":{"abstract":"We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification pre-diction. The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks. We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.","authors":["Lili Yu","Howard Chen","Sida I. Wang","Tao Lei","Yoav Artzi"],"demo_url":"","keywords":["Interactive Classification","natural classification","intent classification","classification pre-diction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.237.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.237","similar_paper_uids":["main.237","main.187","srw.122","demo.61","demo.31"],"title":"Interactive Classification by Asking Informative Questions","tldr":"We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information usi...","track":"Machine Learning for NLP"},"forum":"main.237","id":"main.237","presentation_id":"38929271"},{"card_image_alt_text":"A representative figure from paper main.196","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.196.png","content":{"abstract":"Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.","authors":["Robert L Logan IV","Matt Gardner","Sameer Singh"],"demo_url":"","keywords":["Importance Models","likelihood-based evaluation","Language models","importance sampling"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.196.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.196","similar_paper_uids":["main.196","srw.55","main.437","tacl.1727","main.753"],"title":"On Importance Sampling-Based Evaluation of Latent Language Models","tldr":"Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractab...","track":"Machine Learning for NLP"},"forum":"main.196","id":"main.196","presentation_id":"38929173"},{"card_image_alt_text":"A representative figure from paper main.357","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.357.png","content":{"abstract":"Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.","authors":["Alexandre Tamborrino","Nicola Pellican\u00f2","Baptiste Pannier","Pascal Voitot","Louise Naudin"],"demo_url":"","keywords":["Commonsense Reasoning","common tasks","plausibility task","pre-training phase"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.357.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.357","similar_paper_uids":["main.357","main.197","main.249","main.314","main.214"],"title":"Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning","tldr":"Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure i...","track":"Machine Learning for NLP"},"forum":"main.357","id":"main.357","presentation_id":"38929054"},{"card_image_alt_text":"A representative figure from paper main.194","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.194.png","content":{"abstract":"This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText.","authors":["Jiaao Chen","Zichao Yang","Diyi Yang"],"demo_url":"","keywords":["Semi-Supervised Classification","text classification","data augmentation","supervision"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.194.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.194","similar_paper_uids":["main.194","main.522","main.538","main.449","main.78"],"title":"MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification","tldr":"This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden spac...","track":"Machine Learning for NLP"},"forum":"main.194","id":"main.194","presentation_id":"38929239"},{"card_image_alt_text":"A representative figure from paper main.235","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.235.png","content":{"abstract":"Variational Autoencoder (VAE) is widely used as a generative model to approximate a model's posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as ``posterior collapse''. Previous approaches consider the Kullback\u2013Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior's parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.","authors":["Qile Zhu","Wei Bi","Xiaojiang Liu","Xiyao Ma","Xiaolin Li","Dapeng Wu"],"demo_url":"","keywords":["amortized inference","language modeling","text classification","dialogue generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.235.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.235","similar_paper_uids":["main.235","main.316","main.753","main.694","main.71"],"title":"A Batch Normalized Inference Network Keeps the KL Vanishing Away","tldr":"Variational Autoencoder (VAE) is widely used as a generative model to approximate a model's posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decod...","track":"Machine Learning for NLP"},"forum":"main.235","id":"main.235","presentation_id":"38928824"},{"card_image_alt_text":"A representative figure from paper main.590","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.590.png","content":{"abstract":"Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.","authors":["Xiaoqing Zheng","Jiehang Zeng","Yi Zhou","Cho-Jui Hsieh","Minhao Cheng","Xuanjing Huang"],"demo_url":"","keywords":["semantic tasks","sentiment analysis","question answering","reading comprehension"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.590.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.590","similar_paper_uids":["main.590","srw.105","main.540","cl.1550","main.191"],"title":"Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples","tldr":"Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, questio...","track":"Machine Learning for NLP"},"forum":"main.590","id":"main.590","presentation_id":"38928966"},{"card_image_alt_text":"A representative figure from paper main.591","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.591.png","content":{"abstract":"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \u201csyntactic distances\u201d, where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.","authors":["Wenyu Du","Zhouhan Lin","Yikang Shen","Timothy J. O'Donnell","Yoshua Bengio","Yue Zhang"],"demo_url":"","keywords":["Language Modeling","Syntactic Approach","neural models","intermediate representation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.591.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.591","similar_paper_uids":["main.591","main.303","main.158","main.297","main.778"],"title":"Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach","tldr":"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this p...","track":"Machine Learning for NLP"},"forum":"main.591","id":"main.591","presentation_id":"38929039"},{"card_image_alt_text":"A representative figure from paper main.195","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.195.png","content":{"abstract":"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).","authors":["Zhiqing Sun","Hongkun Yu","Xiaodan Song","Renjie Liu","Yiming Yang","Denny Zhou"],"demo_url":"","keywords":["Natural NLP","NLP tasks","knowledge transfer","natural tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.195.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.195","similar_paper_uids":["main.195","main.204","main.250","main.247","main.76"],"title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices","tldr":"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to ...","track":"Machine Learning for NLP"},"forum":"main.195","id":"main.195","presentation_id":"38928769"},{"card_image_alt_text":"A representative figure from paper tacl.1727","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1727.png","content":{"abstract":"The best solution of structured prediction models in NLP is often inaccurate due to limited expressive power of the model or to non-exact parameter estimation. One way to mitigate this problem is sampling candidate solutions from the model\u2019s solution space, reasoning that effective exploration of this space should yield high quality solutions. Unfortunately, sampling is often computationally hard and many works hence back-off to sub-optimal strategies such as extraction of the best scoring solutions of the model, which are not as diverse as sampled solutions. In this paper we propose a perturbation-based approach where sampling from a probabilistic model is computationally efficient. We present a learning algorithm for the variance of the perturbations, and empirically demonstrate its importance. Moreover, while finding the argmax in our model is intractable, we propose an efficient and effective approximation. We apply our framework to cross-lingual dependency parsing across 72 corpora from 42 languages and to lightly supervised dependency parsing across 13 corpora from 12 languages and demonstrate strong results in terms of both the quality of the entire solution list and of the final solution.","authors":["Amichay Doitch","Ram Yazdi","Tamir Hazan","Roi Reichart"],"demo_url":"","keywords":["Structured tasks","Dependency Parsing","NLP","sampling"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00291","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00291","similar_paper_uids":["tacl.1727","main.554","main.318","main.196","main.358"],"title":"Perturbation Based Learning for Structured NLP tasks with Application to Dependency Parsing ","tldr":"The best solution of structured prediction models in NLP is often inaccurate due to limited expressive power of the model or to non-exact parameter estimation. One way to mitigate this problem is sampling candidate solutions from the model\u2019s solution...","track":"Machine Learning for NLP"},"forum":"tacl.1727","id":"tacl.1727","presentation_id":"38929486"},{"card_image_alt_text":"A representative figure from paper tacl.1853","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1853.png","content":{"abstract":"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-Large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.","authors":["Mandar Joshi","Danqi Chen","Yinhan Liu","Daniel S. Weld","Luke Zettlemoyer","Omer Levy"],"demo_url":"","keywords":["span tasks","question answering","coreference resolution","OntoNotes task"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00300","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00300","similar_paper_uids":["tacl.1853","main.247","main.11","main.622","main.299"],"title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","tldr":"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations...","track":"Machine Learning for NLP"},"forum":"tacl.1853","id":"tacl.1853","presentation_id":"38929502"},{"card_image_alt_text":"A representative figure from paper tacl.1766","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1766.png","content":{"abstract":"Contextual representation models have achieved great success in improving various downstream natural language processing tasks. However, these language-model-based encoders are difficult to train due to their large parameter size and high computational complexity By carefully examining the training procedure, we observe that the softmax layer, which predicts a distribution of the target word, often induces significant overhead, especially when the vocabulary size is large. Therefore, we revisit the design of the output layer and consider directly predicting the pre-trained embedding of the target word for a given context. When applied to ELMo, the proposed approach achieves a 4 times speedup and eliminates 80% trainable parameters while achieving competitive performance on downstream tasks. Further analysis shows that the approach maintains the speed advantage under various settings, even when the sentence encoder is scaled up.","authors":["Liunian Harold Li","Patrick H. Chen","Cho-Jui Hsieh","Kai-Wei Chang"],"demo_url":"","keywords":["natural tasks","Contextual Learning","Contextual models","language-model-based encoders"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00289","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00289","similar_paper_uids":["tacl.1766","main.536","main.250","main.197","main.38"],"title":"Efficient Contextual Representation Learning With Continuous Outputs","tldr":"Contextual representation models have achieved great success in improving various downstream natural language processing tasks. However, these language-model-based encoders are difficult to train due to their large parameter size and high computation...","track":"Machine Learning for NLP"},"forum":"tacl.1766","id":"tacl.1766","presentation_id":"38929490"}]
