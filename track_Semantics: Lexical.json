[{"card_image_alt_text":"A representative figure from paper main.365","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.365.png","content":{"abstract":"This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.","authors":["Mario Giulianelli","Marco Del Tredici","Raquel Fern\u00e1ndez"],"demo_url":"","keywords":["Contextualised Representations","unsupervised approach","BERT model","model representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.365.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.365","similar_paper_uids":["main.365","main.493","main.300","main.630","main.168"],"title":"Analysing Lexical Semantic Change with Contextualised Word Representations","tldr":"This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clu...","track":"Semantics: Lexical"},"forum":"main.365","id":"main.365","presentation_id":"38929048"},{"card_image_alt_text":"A representative figure from paper main.364","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.364.png","content":{"abstract":"Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. The proposed method works in two steps. First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks. After selecting the code length, each word learns discrete codes through a neural network with a binary constraint. To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.","authors":["Yeachan Kim","Kang-Min Kim","SangKeun Lee"],"demo_url":"","keywords":["Adaptive Embeddings","Distributed words","natural tasks","downstream tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.364.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.364","similar_paper_uids":["main.364","main.282","main.238","main.449","main.431"],"title":"Adaptive Compression of Word Embeddings","tldr":"Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g...","track":"Semantics: Lexical"},"forum":"main.364","id":"main.364","presentation_id":"38929006"},{"card_image_alt_text":"A representative figure from paper main.366","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.366.png","content":{"abstract":"Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.","authors":["Billy Chiu","Sunil Kumar Sahu","Derek Thomas","Neha Sengupta","Mohammady Mahdy"],"demo_url":"","keywords":["Document Clustering","inducing classes","clustering","Autoencoding Graph"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.366.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.366","similar_paper_uids":["main.366","main.31","main.67","main.588","main.362"],"title":"Autoencoding Keyword Correlation Graph for Document Clustering","tldr":"Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. ...","track":"Semantics: Lexical"},"forum":"main.366","id":"main.366","presentation_id":"38928850"},{"card_image_alt_text":"A representative figure from paper main.367","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.367.png","content":{"abstract":"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference. This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.","authors":["Guy Emerson"],"demo_url":"","keywords":["Amortised Inference","inference","semantic composition","Autoencoding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.367.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.367","similar_paper_uids":["main.367","main.694","main.593","main.341","main.753"],"title":"Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics","tldr":"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent va...","track":"Semantics: Lexical"},"forum":"main.367","id":"main.367","presentation_id":"38929060"},{"card_image_alt_text":"A representative figure from paper main.95","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.95.png","content":{"abstract":"A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work. This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.","authors":["Terra Blevins","Luke Zettlemoyer"],"demo_url":"","keywords":["Word Disambiguation","Word WSD","WSD","sense disambiguation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.95.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.95","similar_paper_uids":["main.95","main.369","main.255","main.423","main.595"],"title":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","tldr":"A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that in...","track":"Semantics: Lexical"},"forum":"main.95","id":"main.95","presentation_id":"38928744"},{"card_image_alt_text":"A representative figure from paper main.675","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.675.png","content":{"abstract":"We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection. InstaMap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point's nearest neighbours in the training dictionary. We report performance gains with InstaMap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs. We note prominent improvements, especially for more distant language pairs (i.e., languages with non-isomorphic monolingual spaces).","authors":["Goran Glava\u0161","Ivan Vuli\u0107"],"demo_url":"","keywords":["bilingual induction","Non-Linear Mapping","InstaMap","instance-based method"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.675.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.675","similar_paper_uids":["main.675","main.618","main.421","main.766","main.143"],"title":"Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces","tldr":"We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection. InstaMap is a non-parametric model that learns a non-linear pro...","track":"Semantics: Lexical"},"forum":"main.675","id":"main.675","presentation_id":"38928885"},{"card_image_alt_text":"A representative figure from paper main.259","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.259.png","content":{"abstract":"Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs. To the best of our knowledge, this is the first ``MWE-aware\" metaphor identification system paving the way for further experiments on the complex interactions of these phenomena. The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.","authors":["Omid Rohanian","Marek Rei","Shiva Taslimipoor","Le An Ha"],"demo_url":"","keywords":["Verbal Expressions","Identification Metaphor","Identifying expressions","MWEs processing"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.259.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.259","similar_paper_uids":["main.259","tacl.1912","main.775","tacl.1720","main.58"],"title":"Verbal Multiword Expressions for Identification of Metaphor","tldr":"Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are ling...","track":"Semantics: Lexical"},"forum":"main.259","id":"main.259","presentation_id":"38929343"},{"card_image_alt_text":"A representative figure from paper main.258","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.258.png","content":{"abstract":"While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. Both models outperform previous approaches, with the multi-channel model performing best.","authors":["Anna H\u00e4tty","Dominik Schlechtweg","Michael Dorna","Sabine Schulte im Walde"],"demo_url":"","keywords":["Automatic Extraction","technicality prediction","computational approaches","classification approach"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.258.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.258","similar_paper_uids":["main.258","main.625","main.766","main.675","main.653"],"title":"Predicting Degrees of Technicality in Automatic Terminology Extraction","tldr":"While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and il...","track":"Semantics: Lexical"},"forum":"main.258","id":"main.258","presentation_id":"38928698"},{"card_image_alt_text":"A representative figure from paper main.335","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.335.png","content":{"abstract":"Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.","authors":["Mujeen Sung","Hwisang Jeon","Jinhyuk Lee","Jaewoo Kang"],"demo_url":"","keywords":["Biomedical Representations","normalization entities","learning entities","Synonym Marginalization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.335.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.335","similar_paper_uids":["main.335","main.574","main.577","main.571","main.520"],"title":"Biomedical Entity Representations with Synonym Marginalization","tldr":"Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challengin...","track":"Semantics: Lexical"},"forum":"main.335","id":"main.335","presentation_id":"38929043"},{"card_image_alt_text":"A representative figure from paper main.255","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.255.png","content":{"abstract":"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.","authors":["Michele Bevilacqua","Roberto Navigli"],"demo_url":"","keywords":["Word Disambiguation","Word WSD","WSD","Enhanced WSD"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.255.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.255","similar_paper_uids":["main.255","main.156","main.95","main.369","demo.116"],"title":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","tldr":"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset...","track":"Semantics: Lexical"},"forum":"main.255","id":"main.255","presentation_id":"38929222"},{"card_image_alt_text":"A representative figure from paper main.334","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.334.png","content":{"abstract":"The hypernymy detection task has been addressed under various frameworks. Previously, the design of unsupervised hypernymy scores has been extensively studied. In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from ``lexical memorization''. In this work, we revisit supervised distributional models for hypernymy detection. Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE). In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations. A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections. Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.","authors":["Chengyu Wang","Xiaofeng He"],"demo_url":"","keywords":["Supervised Detection","hypernymy task","unsupervised scores","hypernymy detection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.334.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.334","similar_paper_uids":["main.334","tacl.1903","main.340","main.199","main.336"],"title":"BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection","tldr":"The hypernymy detection task has been addressed under various frameworks. Previously, the design of unsupervised hypernymy scores has been extensively studied. In contrast, supervised classifiers, especially distributional models, leverage the global...","track":"Semantics: Lexical"},"forum":"main.334","id":"main.334","presentation_id":"38929383"},{"card_image_alt_text":"A representative figure from paper main.336","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.336.png","content":{"abstract":"Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.","authors":["Changlong Yu","Jialong Han","Haisong Zhang","Wilfred Ng"],"demo_url":"","keywords":["Hypernymy Detection","lexical entailment","natural tasks","monolingual detection"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.336.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.336","similar_paper_uids":["main.336","tacl.1906","main.252","main.523","main.722"],"title":"Hypernymy Detection for Low-Resource Languages via Meta Learning","tldr":"Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investiga...","track":"Semantics: Lexical"},"forum":"main.336","id":"main.336","presentation_id":"38929011"},{"card_image_alt_text":"A representative figure from paper main.256","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.256.png","content":{"abstract":"Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters\u2019 written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios. Experiments across different applications show the significant effectiveness of our model.","authors":["Hong-You Chen","SZ-HAN YU","Shou-de Lin"],"demo_url":"","keywords":["Chinese Embedding","Chinese applications","Chinese problem","out-of-vocabulary embedding"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.256.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.256","similar_paper_uids":["main.256","srw.48","main.96","main.85","main.188"],"title":"Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs","tldr":"Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters\u2019 written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, ...","track":"Semantics: Lexical"},"forum":"main.256","id":"main.256","presentation_id":"38928977"},{"card_image_alt_text":"A representative figure from paper main.257","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.257.png","content":{"abstract":"We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.","authors":["Daniela Gerz","Ivan Vuli\u0107","Marek Rei","Roi Reichart","Anna Korhonen"],"demo_url":"","keywords":["estimating preference","Multidirectional Representations","neural framework","task-independent model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.257.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.257","similar_paper_uids":["main.257","main.250","main.198","main.683","main.200"],"title":"Multidirectional Associative Optimization of Function-Specific Word Representations","tldr":"We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausibl...","track":"Semantics: Lexical"},"forum":"main.257","id":"main.257","presentation_id":"38928982"},{"card_image_alt_text":"A representative figure from paper main.337","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.337.png","content":{"abstract":"This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes \u2013 the semantic class of direct objects of a verb and the semantic class in a thesaurus \u2013 and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.","authors":["Ryohei Sasano","Anna Korhonen"],"demo_url":"","keywords":["modeling distribution","centroid-based model","discriminative models","Word-Class Distributions"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.337.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.337","similar_paper_uids":["main.337","main.51","main.725","main.71","main.575"],"title":"Investigating Word-Class Distributions in Word Vector Spaces","tldr":"This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and ...","track":"Semantics: Lexical"},"forum":"main.337","id":"main.337","presentation_id":"38928833"},{"card_image_alt_text":"A representative figure from paper main.369","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.369.png","content":{"abstract":"Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.","authors":["Tommaso Pasini","Federico Scozzafava","Bianca Scarlini"],"demo_url":"","keywords":["English tasks","disambiguation","multilingual tasks","CluBERT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.369.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.369","similar_paper_uids":["main.369","demo.69","main.95","main.204","main.423"],"title":"CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages","tldr":"Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the ...","track":"Semantics: Lexical"},"forum":"main.369","id":"main.369","presentation_id":"38929342"},{"card_image_alt_text":"A representative figure from paper main.368","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.368.png","content":{"abstract":"Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\u00fctze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.","authors":["Timo Schick","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["NLP","rare task","BERTRAM","Word Embeddings"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.368.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.368","similar_paper_uids":["main.368","main.431","main.705","main.76","main.195"],"title":"BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance","tldr":"Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\u00fctze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been address...","track":"Semantics: Lexical"},"forum":"main.368","id":"main.368","presentation_id":"38928737"},{"card_image_alt_text":"A representative figure from paper cl.1543","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/cl.1543.png","content":{"abstract":"We present LESSLEX, a novel multilingual lexical resource. Different from the vast majority of existing approaches, we ground our embeddings on a sense inventory made available from the BabelNet semantic network. In this setting, multilingual access is governed by the mapping of terms onto their underlying sense descriptions, such that all vectors co-exist in the same semantic space. As a result, for each term we have thus the 'blended' terminological vector along with those describing all senses associated to that term. LessLex has been tested on three tasks relevant to lexical semantics: conceptual similarity, contextual similarity, and semantic text similarity: we experimented over the principal data sets for such tasks in their multilingual and cross-lingual variants, improving on or closely approaching state-of-the-art results. We conclude by arguing that LessLex vectors may be relevant for practical applications and for research on conceptual and lexical access and competence.","authors":["Davide Colla","Enrico Mensa","Daniele P. Radicioni"],"demo_url":"","keywords":["SenSe Items","SenSe ","LESSLEX","Multilingual Embeddings"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00375","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/coli_a_00375","similar_paper_uids":["cl.1543","main.329","demo.116","main.260","main.95"],"title":"LESSLEX: Linking Multilingual Embeddings to SenSe Representations of Lexical Items","tldr":"We present LESSLEX, a novel multilingual lexical resource. Different from the vast majority of existing approaches, we ground our embeddings on a sense inventory made available from the BabelNet semantic network. In this setting, multilingual access ...","track":"Semantics: Lexical"},"forum":"cl.1543","id":"cl.1543","presentation_id":"38929479"},{"card_image_alt_text":"A representative figure from paper tacl.1903","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1903.png","content":{"abstract":"In this paper, we propose LEXSUB, a novel approach towards unifying lexical and distributional semantics. We inject knowledge about lexical-semantic relations into distributional word embeddings by defining subspaces of the distributional vector space in which a lexical relation should hold. Our framework can handle symmetric attract and repel relations (e.g., synonymy and antonymy, respectively), as well as asymmetric relations (e.g., hypernymy and meronomy). In a suite of intrinsic benchmarks, we show that our model outperforms previous post-hoc approaches on relatedness tasks, and on hypernymy classification and detection while being competitive on word similarity tasks. It also outperforms previous systems on extrinsic classification tasks that benefit from exploiting lexical relational cues. We perform a series of analyses to understand the behaviors of our model.","authors":["Kushal Arora","Aishik Chakraborty","Jackie Chi Kit Cheung"],"demo_url":"","keywords":["relatedness tasks","hypernymy classification","detection","word tasks"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00316","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00316","similar_paper_uids":["tacl.1903","main.334","main.336","main.136","main.367"],"title":"Learning Lexical Subspaces in a Distributional Vector Space","tldr":"In this paper, we propose LEXSUB, a novel approach towards unifying lexical and distributional semantics. We inject knowledge about lexical-semantic relations into distributional word embeddings by defining subspaces of the distributional vector spac...","track":"Semantics: Lexical"},"forum":"tacl.1903","id":"tacl.1903","presentation_id":"38929508"}]
