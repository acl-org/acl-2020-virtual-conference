[{"card_image_alt_text":"A representative figure from paper main.91","card_image_path":"static/images/papers/main.91.png","content":{"abstract":"Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. In this paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.","authors":["Yunshi Lan","Jing Jiang"],"demo_url":"","keywords":["Query Generation","Answering Questions","staged method","constraints"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.91.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.91","similar_paper_uids":["main.91","main.74","main.730","main.69","main.67"],"title":"Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases","tldr":"Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. In this paper, we handle both types of complexity at the...","track":"Question Answering"},"forum":"main.91","id":"main.91","presentation_id":"38929263"},{"card_image_alt_text":"A representative figure from paper main.85","card_image_path":"static/images/papers/main.85.png","content":{"abstract":"Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.","authors":["Jinhyuk Lee","Minjoon Seo","Hannaneh Hajishirzi","Jaewoo Kang"],"demo_url":"","keywords":["Real-Time Answering","Open-domain answering","phrase problem","Contextualized Representations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.85.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.85","similar_paper_uids":["main.85","main.537","main.325","main.617","main.37"],"title":"Contextualized Sparse Representations for Real-Time Open-Domain Question Answering","tldr":"Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper...","track":"Question Answering"},"forum":"main.85","id":"main.85","presentation_id":"38929083"},{"card_image_alt_text":"A representative figure from paper main.601","card_image_path":"static/images/papers/main.601.png","content":{"abstract":"This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance. Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text. Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text. Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain. Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics. These patterns can be estimated by the neural hidden semi-Markov model using latent variables. With latent patterns as a prior, we can regularize the generation model and produce the optimal results. Experimental results on the HotpotQA data set demonstrate the effectiveness of our model. Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements.","authors":["Jianxing Yu","Wei Liu","Shuang Qiu","Qinliang Su","Kai Wang","Xiaojun Quan","Jian Yin"],"demo_url":"","keywords":["Low-Resource Questions","generating questions","machine comprehension","multi-hop model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.601.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.601","similar_paper_uids":["main.601","main.499","main.694","main.412","main.26"],"title":"Low-Resource Generation of Multi-hop Reasoning Questions","tldr":"This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance. Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on sev...","track":"Question Answering"},"forum":"main.601","id":"main.601","presentation_id":"38928709"},{"card_image_alt_text":"A representative figure from paper main.600","card_image_path":"static/images/papers/main.600.png","content":{"abstract":"Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA. We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models. We also show the effectiveness of our approach in the few-shot learning setting.","authors":["Zhongli Li","Wenhui Wang","Li Dong","Furu Wei","Ke Xu"],"demo_url":"","keywords":["Unsupervised QA","Question Answering","Question QA","QA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.600.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.600","similar_paper_uids":["main.600","main.413","main.20","main.652","main.19"],"title":"Harvesting and Refining Question-Answer Pairs for Unsupervised QA","tldr":"Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data avai...","track":"Question Answering"},"forum":"main.600","id":"main.600","presentation_id":"38928860"},{"card_image_alt_text":"A representative figure from paper main.84","card_image_path":"static/images/papers/main.84.png","content":{"abstract":"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.","authors":["Jan Deriu","Katsiaryna Mlynchyk","Philippe Schl\u00e4pfer","Alvaro Rodrigo","Dirk von Gr\u00fcnigen","Nicolas Kaiser","Kurt Stockinger","Eneko Agirre","Mark Cieliebak"],"demo_url":"","keywords":["question answering","annotation","Inverse Annotation","intermediate representation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.84.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.84","similar_paper_uids":["main.84","main.496","main.91","main.742","main.398"],"title":"A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation","tldr":"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operati...","track":"Question Answering"},"forum":"main.84","id":"main.84","presentation_id":"38929033"},{"card_image_alt_text":"A representative figure from paper main.90","card_image_path":"static/images/papers/main.90.png","content":{"abstract":"Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. In this paper, we introduce a new follow-up question identification task. We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question. It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.","authors":["Souvik Kundu","Qian Lin","Hwee Tou Ng"],"demo_url":"","keywords":["Conversational Answering","answer finding","follow-up task","conversational systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.90.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.90","similar_paper_uids":["main.90","main.498","srw.122","main.772","main.21"],"title":"Learning to Identify Follow-Up Questions in Conversational Question Answering","tldr":"Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial f...","track":"Question Answering"},"forum":"main.90","id":"main.90","presentation_id":"38929302"},{"card_image_alt_text":"A representative figure from paper main.414","card_image_path":"static/images/papers/main.414.png","content":{"abstract":"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","authors":["Vikas Yadav","Steven Bethard","Mihai Surdeanu"],"demo_url":"","keywords":["Unsupervised Retrieval","Multi-hop Answering","Evidence retrieval","question answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.414.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.414","similar_paper_uids":["main.414","main.413","main.600","main.20","main.772"],"title":"Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering","tldr":"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which...","track":"Question Answering"},"forum":"main.414","id":"main.414","presentation_id":"38929306"},{"card_image_alt_text":"A representative figure from paper main.86","card_image_path":"static/images/papers/main.86.png","content":{"abstract":"Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model's current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear bene\ufb01t in multitask performance over forcing task homogeneity at the epoch or batch level. Our \ufb01nal model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.","authors":["Ananth Gottumukkala","Dheeru Dua","Sameer Singh","Matt Gardner"],"demo_url":"","keywords":["Multi-Task Comprehension","generalization","Dynamic Strategies","general systems"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.86.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.86","similar_paper_uids":["main.86","main.222","main.388","main.689","main.183"],"title":"Dynamic Sampling Strategies for Multi-Task Reading Comprehension","tldr":"Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and l...","track":"Question Answering"},"forum":"main.86","id":"main.86","presentation_id":"38929287"},{"card_image_alt_text":"A representative figure from paper main.602","card_image_path":"static/images/papers/main.602.png","content":{"abstract":"Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we introduce R4C, a new task for evaluating RC systems' internal reasoning. R4C requires giving not only answers but also derivations: explanations that justify predicted answers. We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations). Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.","authors":["Naoya Inoue","Pontus Stenetorp","Kentaro Inui"],"demo_url":"","keywords":["R4C","RC Systems","reading systems","RC reasoning"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.602.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.602","similar_paper_uids":["main.602","main.507","main.408","main.410","main.455"],"title":"R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason","tldr":"Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we...","track":"Question Answering"},"forum":"main.602","id":"main.602","presentation_id":"38928927"},{"card_image_alt_text":"A representative figure from paper main.603","card_image_path":"static/images/papers/main.603.png","content":{"abstract":"In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. However, these transformer models can only take as input a fixed-length (e.g., 512) text. To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering. Moreover, they are less capable of answering questions that need cross-segment information. We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also apply recurrent mechanisms to enable information to flow across segments. Experiments on three MRC tasks -- CoQA, QuAC, and TriviaQA -- demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.","authors":["Hongyu Gong","Yelong Shen","Dian Yu","Jianshu Chen","Dong Yu"],"demo_url":"","keywords":["Long-Text Comprehension","machine comprehension","MRC","question answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.603.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.603","similar_paper_uids":["main.603","main.247","main.29","main.361","main.503"],"title":"Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension","tldr":"In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of ...","track":"Question Answering"},"forum":"main.603","id":"main.603","presentation_id":"38928937"},{"card_image_alt_text":"A representative figure from paper main.87","card_image_path":"static/images/papers/main.87.png","content":{"abstract":"Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages. However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary. In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web. Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.","authors":["Fei Yuan","Linjun Shou","Xuanyu Bai","Ming Gong","Yaobo Liang","Nan Duan","Yan Fu","Daxin Jiang"],"demo_url":"","keywords":["Multilingual Comprehension","multilingual MRC","MRC","sentence tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.87.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.87","similar_paper_uids":["main.87","main.421","main.747","main.554","main.252"],"title":"Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension","tldr":"Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages. However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is sig...","track":"Question Answering"},"forum":"main.87","id":"main.87","presentation_id":"38929253"},{"card_image_alt_text":"A representative figure from paper main.411","card_image_path":"static/images/papers/main.411.png","content":{"abstract":"Transformer-based QA models use input-wide self-attention -- i.e. across both the question and the input passage -- at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.","authors":["Qingqing Cao","Harsh Trivedi","Aruna Balasubramanian","Niranjan Balasubramanian"],"demo_url":"","keywords":["Faster Answering","question-independent processing","DeFormer","Decomposing Transformers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.411.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.411","similar_paper_uids":["main.411","main.204","main.687","tacl.1815","main.385"],"title":"DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering","tldr":"Transformer-based QA models use input-wide self-attention -- i.e. across both the question and the input passage -- at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all ...","track":"Question Answering"},"forum":"main.411","id":"main.411","presentation_id":"38929429"},{"card_image_alt_text":"A representative figure from paper main.83","card_image_path":"static/images/papers/main.83.png","content":{"abstract":"Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.","authors":["Shaoru Guo","Ru Li","Hongye Tan","Xiaoli Li","Yong Guan","Hongyan Zhao","Yueping Zhang"],"demo_url":"","keywords":["Machine Comprehension","Sentence representation","SR","Machine MRC"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.83.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.83","similar_paper_uids":["main.83","main.435","main.247","main.666","main.87"],"title":"A Frame-based Sentence Representation for Machine Reading Comprehension","tldr":"Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledg...","track":"Question Answering"},"forum":"main.83","id":"main.83","presentation_id":"38928862"},{"card_image_alt_text":"A representative figure from paper main.410","card_image_path":"static/images/papers/main.410.png","content":{"abstract":"Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP'18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert's performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets.","authors":["Xiang Yue","Bernal Jimenez Gutierrez","Huan Sun"],"demo_url":"","keywords":["Clinical Comprehension","Machine comprehension","annotation","question answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.410.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.410","similar_paper_uids":["main.410","main.19","main.507","main.600","main.652"],"title":"Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset","tldr":"Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, P...","track":"Question Answering"},"forum":"main.410","id":"main.410","presentation_id":"38929136"},{"card_image_alt_text":"A representative figure from paper main.362","card_image_path":"static/images/papers/main.362.png","content":{"abstract":"While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree's effectiveness in translating the MWP text into solution expressions.","authors":["Jipeng Zhang","Lei Wang","Roy Ka-Wei Lee","Yi Bin","Yan Wang","Jie Shao","Ee-Peng Lim"],"demo_url":"","keywords":["Solving Problems","Math Problems","math problem","quantity representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.362.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.362","similar_paper_uids":["main.362","main.67","main.640","main.588","main.224"],"title":"Graph-to-Tree Learning for Solving Math Word Problems","tldr":"While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. Th...","track":"Question Answering"},"forum":"main.362","id":"main.362","presentation_id":"38929273"},{"card_image_alt_text":"A representative figure from paper main.412","card_image_path":"static/images/papers/main.412.png","content":{"abstract":"Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn't always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA's effectiveness over other state-of-the-art baselines.","authors":["Apoorv Saxena","Aditay Tripathi","Partha Talukdar"],"demo_url":"","keywords":["Multi-hop Answering","Question task","natural queries","multi-hop KGQA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.412.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.412","similar_paper_uids":["main.412","main.238","main.617","main.572","main.653"],"title":"Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings","tldr":"Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requ...","track":"Question Answering"},"forum":"main.412","id":"main.412","presentation_id":"38929421"},{"card_image_alt_text":"A representative figure from paper main.604","card_image_path":"static/images/papers/main.604.png","content":{"abstract":"Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks. To our best knowledge, it is the first single model that outperforms the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.","authors":["Dayiheng Liu","Yeyun Gong","Jie Fu","Yu Yan","Jiusheng Chen","Daxin Jiang","Jiancheng Lv","Nan Duan"],"demo_url":"","keywords":["Natural Answering","natural understanding","long-answer tasks","RikiNet"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.604.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.604","similar_paper_uids":["main.604","main.599","main.498","main.247","main.501"],"title":"RikiNet: Reading Wikipedia Pages for Natural Question Answering","tldr":"Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dyna...","track":"Question Answering"},"forum":"main.604","id":"main.604","presentation_id":"38928913"},{"card_image_alt_text":"A representative figure from paper main.599","card_image_path":"static/images/papers/main.599.png","content":{"abstract":"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.","authors":["Bo Zheng","Haoyang Wen","Yaobo Liang","Nan Duan","Wanxiang Che","Daxin Jiang","Ming Zhou","Ting Liu"],"demo_url":"","keywords":["Document Modeling","Multi-grained Comprehension","machine comprehension","Graph Networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.599.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.599","similar_paper_uids":["main.599","main.604","main.507","main.247","main.37"],"title":"Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension","tldr":"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of ex...","track":"Question Answering"},"forum":"main.599","id":"main.599","presentation_id":"38928861"},{"card_image_alt_text":"A representative figure from paper main.361","card_image_path":"static/images/papers/main.361.png","content":{"abstract":"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC. To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. At each iteration, a base MRC model is trained with golden answers and noisy evidence labels. The trained model will predict pseudo evidence labels as extra supervision in the next iteration. We evaluate STM on seven datasets over three MRC tasks. Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.","authors":["Yilin Niu","Fangkai Jiao","Mantong Zhou","Ting Yao","Jingfang Xu","Minlie Huang"],"demo_url":"","keywords":["Machine Comprehension","Soft Extraction","machine","MRC"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.361.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.361","similar_paper_uids":["main.361","main.211","main.87","main.97","main.414"],"title":"A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction","tldr":"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, ...","track":"Question Answering"},"forum":"main.361","id":"main.361","presentation_id":"38928873"},{"card_image_alt_text":"A representative figure from paper main.413","card_image_path":"static/images/papers/main.413.png","content":{"abstract":"Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.","authors":["Alexander Fabbri","Patrick Ng","Zhiguo Wang","Ramesh Nallapati","Bing Xiang"],"demo_url":"","keywords":["Template-Based Generation","Unsupervised Answering","Question Answering","QA"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.413.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.413","similar_paper_uids":["main.413","main.20","main.600","main.19","main.652"],"title":"Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering","tldr":"Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled ...","track":"Question Answering"},"forum":"main.413","id":"main.413","presentation_id":"38928820"},{"card_image_alt_text":"A representative figure from paper main.500","card_image_path":"static/images/papers/main.500.png","content":{"abstract":"Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.","authors":["Md Arafat Sultan","Shubham Chandel","Ram\u00f3n Fernandez Astudillo","Vittorio Castelli"],"demo_url":"","keywords":["Question Generation","QA","Automatic generation","Automatic QG"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.500.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.500","similar_paper_uids":["main.500","main.20","main.413","main.450","main.772"],"title":"On the Importance of Diversity in Question Generation for QA","tldr":"Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples fro...","track":"Question Answering"},"forum":"main.500","id":"main.500","presentation_id":"38929093"},{"card_image_alt_text":"A representative figure from paper main.501","card_image_path":"static/images/papers/main.501.png","content":{"abstract":"We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.","authors":["Hao Cheng","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"demo_url":"","keywords":["Distantly-Supervised Answering","extractive answering","document-level super-vision","probability assumptions"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.501.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.501","similar_paper_uids":["main.501","main.498","main.604","main.19","main.247"],"title":"Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering","tldr":"We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumpt...","track":"Question Answering"},"forum":"main.501","id":"main.501","presentation_id":"38929404"},{"card_image_alt_text":"A representative figure from paper main.503","card_image_path":"static/images/papers/main.503.png","content":{"abstract":"To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model's training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model's softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model's behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model's probabilities only answers 48% at 80% accuracy.","authors":["Amita Kamath","Robin Jia","Percy Liang"],"demo_url":"","keywords":["Selective Answering","Domain Shift","question models","QA models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.503.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.503","similar_paper_uids":["main.503","main.413","main.652","main.19","main.20"],"title":"Selective Question Answering under Domain Shift","tldr":"To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model's training data, making errors more likely and thus abstention more critical. I...","track":"Question Answering"},"forum":"main.503","id":"main.503","presentation_id":"38929409"},{"card_image_alt_text":"A representative figure from paper main.502","card_image_path":"static/images/papers/main.502.png","content":{"abstract":"We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving and significantly impair each other\u2019s context. Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap.","authors":["Xiang Kong","Varun Gangal","Eduard Hovy"],"demo_url":"","keywords":["SCDE","computational models","sentence prediction","joint solving"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.502.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.502","similar_paper_uids":["main.502","tacl.1882","main.226","tacl.1929","main.499"],"title":"SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations","tldr":"We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up mult...","track":"Question Answering"},"forum":"main.502","id":"main.502","presentation_id":"38928704"},{"card_image_alt_text":"A representative figure from paper main.499","card_image_path":"static/images/papers/main.499.png","content":{"abstract":"Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data.","authors":["Akari Asai","Hannaneh Hajishirzi"],"demo_url":"","keywords":["Logic-Guided Augmentation","Regularization","Consistent Answering","natural questions"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.499.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.499","similar_paper_uids":["main.499","main.601","main.600","main.247","main.19"],"title":"Logic-Guided Data Augmentation and Regularization for Consistent Question Answering","tldr":"Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating lo...","track":"Question Answering"},"forum":"main.499","id":"main.499","presentation_id":"38929217"},{"card_image_alt_text":"A representative figure from paper main.498","card_image_path":"static/images/papers/main.498.png","content":{"abstract":"Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important signals for learning the representations. Existing methods learned semantic representations with dual encoders or dual variational auto-encoders. The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. However, the alignment and semantics were too separate to capture the aligned semantics between question and answer. In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions. Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.","authors":["Wenhao Yu","Lingfei Wu","Qingkai Zeng","Shu Tao","Yu Deng","Meng Jiang"],"demo_url":"","keywords":["Answer Retrieval","vector questions/answers","Question-answer alignment","SQuAD"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.498.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.498","similar_paper_uids":["main.498","main.74","main.21","main.19","main.501"],"title":"Crossing Variational Autoencoders for Answer Retrieval","tldr":"Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important sig...","track":"Question Answering"},"forum":"main.498","id":"main.498","presentation_id":"38929337"},{"card_image_alt_text":"A representative figure from paper main.505","card_image_path":"static/images/papers/main.505.png","content":{"abstract":"We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.","authors":["Changmao Li","Jinho D. Choi"],"demo_url":"","keywords":["Span-based Answering","language tasks","token- modeling","utterance prediction"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.505.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.505","similar_paper_uids":["main.505","tacl.1853","main.247","main.216","main.637"],"title":"Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering","tldr":"We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance ord...","track":"Question Answering"},"forum":"main.505","id":"main.505","presentation_id":"38928874"},{"card_image_alt_text":"A representative figure from paper main.504","card_image_path":"static/images/papers/main.504.png","content":{"abstract":"Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time. Partial encodings from the transformer model are shared among rerankers, providing further speed-up. When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.","authors":["Luca Soldaini","Alessandro Moschitti"],"demo_url":"","keywords":["Efficient Selection","Answer Selection","classification tasks","classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.504.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.504","similar_paper_uids":["main.504","main.593","main.38","main.325","main.537"],"title":"The Cascade Transformer: an Application for Efficient Answer Sentence Selection","tldr":"Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While pre...","track":"Question Answering"},"forum":"main.504","id":"main.504","presentation_id":"38929221"},{"card_image_alt_text":"A representative figure from paper main.497","card_image_path":"static/images/papers/main.497.png","content":{"abstract":"Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. We find that these intermediate annotations can provide two-fold benefits. First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref. Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.","authors":["Dheeru Dua","Sameer Singh","Matt Gardner"],"demo_url":"","keywords":["Reading Comprehension","data collection","data process","Intermediate Annotations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.497.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.497","similar_paper_uids":["main.497","main.495","main.467","main.507","srw.28"],"title":"Benefits of Intermediate Annotations in Reading Comprehension","tldr":"Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compoun...","track":"Question Answering"},"forum":"main.497","id":"main.497","presentation_id":"38929228"},{"card_image_alt_text":"A representative figure from paper main.654","card_image_path":"static/images/papers/main.654.png","content":{"abstract":"Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization. To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA. In this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains. To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training. More importantly, the proposed MMT is independent of backbone language models. Extensive experiments demonstrate the superiority of MMT over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.","authors":["Ming Yan","Hao Zhang","Di Jin","Joey Tianyi Zhou"],"demo_url":"","keywords":["Multi-source Transfer","Low Answering","Multiple-choice answering","machine comprehension"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.654.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.654","similar_paper_uids":["main.654","main.336","main.28","main.507","main.636"],"title":"Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering","tldr":"Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. Unfortun...","track":"Question Answering"},"forum":"main.654","id":"main.654","presentation_id":"38929127"},{"card_image_alt_text":"A representative figure from paper main.651","card_image_path":"static/images/papers/main.651.png","content":{"abstract":"Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.","authors":["Vaibhav Kumar","Alan W Black"],"demo_url":"","keywords":["Clarification Generation","Question answering","classifying questions","downstream question-answering"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.651.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.651","similar_paper_uids":["main.651","main.330","main.21","tacl.1845","main.600"],"title":"ClarQ: A large-scale and diverse dataset for Clarification Question Generation","tldr":"Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification que...","track":"Question Answering"},"forum":"main.651","id":"main.651","presentation_id":"38928968"},{"card_image_alt_text":"A representative figure from paper main.652","card_image_path":"static/images/papers/main.652.png","content":{"abstract":"The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data. The good results carry over into the more challenging IR scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.","authors":["Jon Ander Campos","Arantxa Otegi","Aitor Soroa","Jan Deriu","Mark Cieliebak","Eneko Agirre"],"demo_url":"","keywords":["DoQA FAQs","conversational interfaces","information scenario","IR scenario"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.652.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.652","similar_paper_uids":["main.652","main.413","main.20","main.19","main.600"],"title":"DoQA - Accessing Domain-Specific FAQs via Conversational QA","tldr":"The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are colle...","track":"Question Answering"},"forum":"main.652","id":"main.652","presentation_id":"38929118"},{"card_image_alt_text":"A representative figure from paper main.653","card_image_path":"static/images/papers/main.653.png","content":{"abstract":"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.","authors":["Patrick Lewis","Barlas Oguz","Ruty Rinott","Sebastian Riedel","Holger Schwenk"],"demo_url":"","keywords":["Evaluating Answering","Cross-lingual Answering","Question models","MLQA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.653.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.653","similar_paper_uids":["main.653","main.413","main.412","main.652","main.600"],"title":"MLQA: Evaluating Cross-lingual Extractive Question Answering","tldr":"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making buil...","track":"Question Answering"},"forum":"main.653","id":"main.653","presentation_id":"38928716"},{"card_image_alt_text":"A representative figure from paper main.89","card_image_path":"static/images/papers/main.89.png","content":{"abstract":"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 --> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","authors":["Mor Geva","Ankit Gupta","Jonathan Berant"],"demo_url":"","keywords":["numerical reasoning","automatic generation","RC tasks","automatic augmentation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.89.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.89","similar_paper_uids":["main.89","main.247","main.197","main.357","main.195"],"title":"Injecting Numerical Reasoning Skills into Language Models","tldr":"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently...","track":"Question Answering"},"forum":"main.89","id":"main.89","presentation_id":"38929021"},{"card_image_alt_text":"A representative figure from paper main.88","card_image_path":"static/images/papers/main.88.png","content":{"abstract":"The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4. We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.","authors":["Yifan Gao","Chien-Sheng Wu","Shafiq Joty","Caiming Xiong","Richard Socher","Irwin King","Michael Lyu","Steven C.H. Hoi"],"demo_url":"","keywords":["Conversational Reading","decision making","Explicit Tracker","Coarse-to-Fine Reasoning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.88.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.88","similar_paper_uids":["main.88","main.135","main.586","main.100","main.105"],"title":"Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading","tldr":"The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-rela...","track":"Question Answering"},"forum":"main.88","id":"main.88","presentation_id":"38928988"},{"card_image_alt_text":"A representative figure from paper tacl.1929","card_image_path":"static/images/papers/tacl.1929.png","content":{"abstract":"Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA, a question answering dataset covering 11 typologically diverse languages with 141K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology --- the set of linguistic features that each language expresses --- such that we expect models performing well on this set to generalize across a large number of the languages in the world. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don't know the answer yet, and the data is collected directly in each language without the use of translation. We provide initial quality measurements with a baseline model, suggesting a significant room for future work on this data.","authors":["Jonathan H Clark","Jennimaria Palomaki","Vitaly Nikolaev","Eunsol Choi","Dan Garrette","Michael Collins","Tom Kwiatkowski"],"demo_url":"","keywords":["Information-Seeking Answering","multilingual modeling","information-seeking task","translation"],"paper_type":"TACL","pdf_url":"https://arxiv.org/abs/2003.05002","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://arxiv.org/abs/2003.05002","similar_paper_uids":["tacl.1929","main.662","main.652","main.653","main.500"],"title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages","tldr":"Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA, a question answering dataset covering 11 typologically diverse languages with 141K question-answer pairs. The languages of TyDi QA...","track":"Question Answering"},"forum":"tacl.1929","id":"tacl.1929","presentation_id":"38929512"},{"card_image_alt_text":"A representative figure from paper tacl.1882","card_image_path":"static/images/papers/tacl.1882.png","content":{"abstract":"Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C^3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations. We present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5%) and human readers (96.0%), especially on problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C^3 to present great challenges to existing systems as answering 86.8% of questions requires both knowledge within and beyond the accompanying document, and we hope that C^3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text. C^3 is available at https://dataset.org/c3/.","authors":["Kai Sun","Dian Yu","Dong Yu","Claire Cardie"],"demo_url":"","keywords":["Chinese Comprehension","Machine tasks","real-world problems","data augmentation"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00305","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00305","similar_paper_uids":["tacl.1882","main.507","main.508","demo.69","main.135"],"title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension","tldr":"Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C^3), containing 13,369 docu...","track":"Question Answering"},"forum":"tacl.1882","id":"tacl.1882","presentation_id":"38929504"},{"card_image_alt_text":"A representative figure from paper tacl.1845","card_image_path":"static/images/papers/tacl.1845.png","content":{"abstract":"Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.","authors":["Tomer Wolfson","Mor Geva","Ankit Gupta","Yoav Goldberg","Matt Gardner","Daniel Deutch","Jonathan Berant"],"demo_url":"","keywords":["Question Benchmark","Understanding questions","open-domain answering","annotation"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00309","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00309","similar_paper_uids":["tacl.1845","main.69","main.498","main.662","main.21"],"title":"Break It Down: A Question Understanding Benchmark","tldr":"Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes...","track":"Question Answering"},"forum":"tacl.1845","id":"tacl.1845","presentation_id":"38929499"}]
