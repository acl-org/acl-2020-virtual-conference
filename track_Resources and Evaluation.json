[{"card_image_alt_text":"A representative figure from paper main.417","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.417.png","content":{"abstract":"We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.","authors":["Marta Ba\u00f1\u00f3n","Pinzhen Chen","Barry Haddow","Kenneth Heafield","Hieu Hoang","Miquel Espl\u00e0-Gomis","Mikel L. Forcada","Amir Kamran","Faheem Kirefu","Philipp Koehn","Sergio Ortiz Rojas","Leopoldo Pla Sempere","Gema Ram\u00edrez-S\u00e1nchez","Elsa Sarr\u00edas","Marek Strelec","Brian Thompson","William Waites","Dion Wiggins","Jaume Zaragoza"],"demo_url":"","keywords":["sentence alignment","sentence filtering","machine systems","ParaCrawl"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.417.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.417","similar_paper_uids":["main.417","main.756","main.709","srw.137","srw.106"],"title":"ParaCrawl: Web-Scale Acquisition of Parallel Corpora","tldr":"We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filter...","track":"Resources and Evaluation"},"forum":"main.417","id":"main.417","presentation_id":"38928733"},{"card_image_alt_text":"A representative figure from paper main.416","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.416.png","content":{"abstract":"Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as boxes which are not straightforward to process automatically. Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs. However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams. Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.","authors":["Jiangming Liu","Shay B. Cohen","Mirella Lapata"],"demo_url":"","keywords":["Discourse Parsing","Dscorer","Fast Metric",""],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.416.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.416","similar_paper_uids":["main.416","main.569","main.162","main.397","main.268"],"title":"Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing","tldr":"Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typicall...","track":"Resources and Evaluation"},"forum":"main.416","id":"main.416","presentation_id":"38928931"},{"card_image_alt_text":"A representative figure from paper main.92","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.92.png","content":{"abstract":"We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.","authors":["Shen-yun Miao","Chao-Chun Liang","Keh-Yih Su"],"demo_url":"","keywords":["AI progress","English Solvers","MWP solvers","ASDiv"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.92.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.92","similar_paper_uids":["main.92","main.362","main.610","main.27","tacl.1727"],"title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers","tldr":"We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI...","track":"Resources and Evaluation"},"forum":"main.92","id":"main.92","presentation_id":"38928951"},{"card_image_alt_text":"A representative figure from paper main.93","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.93.png","content":{"abstract":"Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation.","authors":["Yanzhi Yi","Hangyu Deng","Jinglu Hu"],"demo_url":"","keywords":["Image Evaluation","Evaluating captions","system-level tasks","BERTScore"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.93.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.93","similar_paper_uids":["main.93","main.664","main.674","main.4","main.704"],"title":"Improving Image Captioning Evaluation by Considering Inter References Variance","tldr":"Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption...","track":"Resources and Evaluation"},"forum":"main.93","id":"main.93","presentation_id":"38929015"},{"card_image_alt_text":"A representative figure from paper main.415","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.415.png","content":{"abstract":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","authors":["Elizabeth Salesky","Eleanor Chodroff","Tiago Pimentel","Matthew Wiesner","Ryan Cotterell","Alan W Black","Jason Eisner"],"demo_url":"","keywords":["Large-Scale Typology","phonetic typology","cross-linguistic variation","domain knowledge"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.415.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.415","similar_paper_uids":["main.415","main.336","main.421","main.627","srw.137"],"title":"A Corpus for Large-Scale Phonetic Typology","tldr":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phone...","track":"Resources and Evaluation"},"forum":"main.415","id":"main.415","presentation_id":"38928945"},{"card_image_alt_text":"A representative figure from paper main.363","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.363.png","content":{"abstract":"In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as \"positive\", \"neutral\", \"negative\" in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory. Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.","authors":["Enrique Amigo","Julio Gonzalo","Stefano Mizzaro","Jorge Carrillo-de-Albornoz"],"demo_url":"","keywords":["Ordinal Classification","Ordinal tasks","sentiment analysis","NLP tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.363.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.363","similar_paper_uids":["main.363","main.704","main.483","main.248","main.510"],"title":"An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results","tldr":"In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as \"positive\", \"neutral\", \"negative\" in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks eit...","track":"Resources and Evaluation"},"forum":"main.363","id":"main.363","presentation_id":"38928965"},{"card_image_alt_text":"A representative figure from paper main.94","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.94.png","content":{"abstract":"Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows. The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.","authors":["Ryokan Ri","Yoshimasa Tsuruoka"],"demo_url":"","keywords":["Cross-lingual Embeddings","mapping-based embeddings","bilingual induction","mapping-based embeddings"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.94.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.94","similar_paper_uids":["main.94","main.318","demo.116","main.766","main.156"],"title":"Revisiting the Context Window for Cross-lingual Word Embeddings","tldr":"Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of ...","track":"Resources and Evaluation"},"forum":"main.94","id":"main.94","presentation_id":"38928903"},{"card_image_alt_text":"A representative figure from paper main.448","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.448.png","content":{"abstract":"Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.","authors":["Nitika Mathur","Timothy Baldwin","Trevor Cohn"],"demo_url":"","keywords":["judging metrics","assessment","pairwise ranking","thresholding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.448.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.448","similar_paper_uids":["main.448","main.173","main.333","main.113","main.455"],"title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics","tldr":"Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show th...","track":"Resources and Evaluation"},"forum":"main.448","id":"main.448","presentation_id":"38929447"},{"card_image_alt_text":"A representative figure from paper main.110","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.110.png","content":{"abstract":"Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses. Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing.","authors":["Serra Sinem Tekiro\u011flu","Yi-Ling Chung","Marco Guerini"],"demo_url":"","keywords":["natural generation","generation data","data filtering","expert validation/post-editing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.110.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.110","similar_paper_uids":["main.110","main.185","main.19","main.173","main.52"],"title":"Generating Counter Narratives against Online Hate Speech: Data and Strategies","tldr":"Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual respon...","track":"Resources and Evaluation"},"forum":"main.110","id":"main.110","presentation_id":"38928785"},{"card_image_alt_text":"A representative figure from paper main.111","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.111.png","content":{"abstract":"In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.","authors":["Piotr Rybak","Robert Mroczkowski","Janusz Tracz","Ireneusz Gawlik"],"demo_url":"","keywords":["Polish Understanding","general tasks","named recognition","textual entailment"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.111.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.111","similar_paper_uids":["main.111","demo.91","main.542","main.192","main.421"],"title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding","tldr":"In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair compari...","track":"Resources and Evaluation"},"forum":"main.111","id":"main.111","presentation_id":"38929260"},{"card_image_alt_text":"A representative figure from paper main.107","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.107.png","content":{"abstract":"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches.","authors":["Djam\u00e9 Seddah","Farah Essaidi","Amal Fethi","Matthieu Futeral","Benjamin Muller","Pedro Javier Ortiz Su\u00e1rez","Beno\u00eet Sagot","Abhishek Srivastava"],"demo_url":"","keywords":["POS tagging","dependency parsing","intensive techniques","NLP approaches"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.107.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.107","similar_paper_uids":["main.107","main.329","main.443","main.716","main.108"],"title":"Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell","tldr":"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependen...","track":"Resources and Evaluation"},"forum":"main.107","id":"main.107","presentation_id":"38929457"},{"card_image_alt_text":"A representative figure from paper main.113","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.113.png","content":{"abstract":"Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.","authors":["Marina Fomicheva","Lucia Specia","Francisco Guzm\u00e1n"],"demo_url":"","keywords":["Multi-Hypothesis Evaluation","Machine MT","MT uncertainty","linguistic reference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.113.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.113","similar_paper_uids":["main.113","main.151","srw.58","main.93","main.253"],"title":"Multi-Hypothesis Machine Translation Evaluation","tldr":"Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the match...","track":"Resources and Evaluation"},"forum":"main.113","id":"main.113","presentation_id":"38929458"},{"card_image_alt_text":"A representative figure from paper main.112","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.112.png","content":{"abstract":"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.","authors":["Sven Buechel","Susanna R\u00fccker","Udo Hahn"],"demo_url":"","keywords":["sentiment analysis","downstream applications","lexicon creation","bilingual model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.112.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.112","similar_paper_uids":["main.112","main.706","demo.69","demo.115","main.290"],"title":"Learning and Evaluating Emotion Lexicons for 91 Languages","tldr":"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world ...","track":"Resources and Evaluation"},"forum":"main.112","id":"main.112","presentation_id":"38929408"},{"card_image_alt_text":"A representative figure from paper main.328","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.328.png","content":{"abstract":"This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.","authors":["Yutong Shao","Ndapa Nakashole"],"demo_url":"","keywords":["Natural Instructions","conversational agents","ChartDialogs","plotting library"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.328.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.328","similar_paper_uids":["main.328","demo.79","main.98","main.126","main.57"],"title":"ChartDialogs: Plotting from Natural Language Instructions","tldr":"This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a...","track":"Resources and Evaluation"},"forum":"main.328","id":"main.328","presentation_id":"38928845"},{"card_image_alt_text":"A representative figure from paper main.116","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.116.png","content":{"abstract":"This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.","authors":["Annemarie Friedrich","Heike Adel","Federico Tomazic","Johannes Hingerl","Renou Benteau","Anika Marusczyk","Lukas Lange"],"demo_url":"","keywords":["Information Extraction","information task","materials science","slot tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.116.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.116","similar_paper_uids":["main.116","main.173","main.453","main.467","main.705"],"title":"The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain","tldr":"This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involve...","track":"Resources and Evaluation"},"forum":"main.116","id":"main.116","presentation_id":"38928792"},{"card_image_alt_text":"A representative figure from paper main.506","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.506.png","content":{"abstract":"Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues. While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community. We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference). Since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method. This is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses. Our survey reveals that these issues are omnipresent in the NLP research community. As a step forward, we provide best practices and guidelines tailored to NLP research, as well as an easy-to-use package for Bayesian assessment of hypotheses, complementing existing tools.","authors":["Erfan Sadeqi Azer","Daniel Khashabi","Ashish Sabharwal","Dan Roth"],"demo_url":"","keywords":["Natural Processing","NLP","NLP research","Bayesian hypotheses"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.506.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.506","similar_paper_uids":["main.506","main.485","main.700","main.261","main.764"],"title":"Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses","tldr":"Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues. While alternative proposals have been well-debate...","track":"Resources and Evaluation"},"forum":"main.506","id":"main.506","presentation_id":"38928886"},{"card_image_alt_text":"A representative figure from paper main.507","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.507.png","content":{"abstract":"We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them to textual span annotations. The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English. We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability. Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension. 47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer. OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.","authors":["Yevgeni Berzak","Jonathan Malmaud","Roger Levy"],"demo_url":"","keywords":["Reading Comprehension","Structured Comprehension","evaluation comprehension","SAT-like materials"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.507.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.507","similar_paper_uids":["main.507","tacl.1882","main.410","main.247","main.701"],"title":"STARC: Structured Annotations for Reading Comprehension","tldr":"We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them t...","track":"Resources and Evaluation"},"forum":"main.507","id":"main.507","presentation_id":"38928901"},{"card_image_alt_text":"A representative figure from paper main.117","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.117.png","content":{"abstract":"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated speci\ufb01cally for a competition or a task. Second, it has a real-world size \u2013 600 training, 310 dev, and 490 evaluation question/answer pairs \u2013 thus re\ufb02ecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote\u2014a technical document that addresses a speci\ufb01c technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.","authors":["Vittorio Castelli","Rishav Chakravarti","Saswati Dana","Anthony Ferritto","Radu Florian","Martin Franz","Dinesh Garg","Dinesh Khandelwal","Scott McCarley","Michael McCawley","Mohamed Nasr","Lin Pan","Cezar Pendus","John Pitrelli","Saurabh Pujar","Salim Roukos","Andrzej Sakrajda","Avi Sil","Rosario Uceda-Sosa","Todd Ward","Rong Zhang"],"demo_url":"","keywords":["real-world issues","domain adaptation","representations language","TECHQA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.117.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.117","similar_paper_uids":["main.117","main.19","main.652","main.410","main.662"],"title":"The TechQA Dataset","tldr":"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on...","track":"Resources and Evaluation"},"forum":"main.117","id":"main.117","presentation_id":"38928787"},{"card_image_alt_text":"A representative figure from paper main.329","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.329.png","content":{"abstract":"Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.","authors":["Simran Khanuja","Sandipan Dandapat","Anirudh Srinivasan","Sunayana Sitaram","Monojit Choudhury"],"demo_url":"","keywords":["Code-Switched NLP","cross-lingual tasks","NLP tasks","Language Identification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.329.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.329","similar_paper_uids":["main.329","main.716","main.421","main.348","main.747"],"title":"GLUECoS: An Evaluation Benchmark for Code-Switched NLP","tldr":"Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tas...","track":"Resources and Evaluation"},"forum":"main.329","id":"main.329","presentation_id":"38928983"},{"card_image_alt_text":"A representative figure from paper main.115","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.115.png","content":{"abstract":"Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP---one that is grounded in human-like reasoning and understanding.","authors":["G\u00f6zde G\u00fcl \u015eahin","Yova Kementchedjhieva","Phillip Rust","Iryna Gurevych"],"demo_url":"","keywords":["Learning","memorizing patterns","Solving them","NLP"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.115.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.115","similar_paper_uids":["main.115","main.100","main.586","main.408","main.420"],"title":"PuzzLing Machines: A Challenge on Learning From Small Data","tldr":"Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon k...","track":"Resources and Evaluation"},"forum":"main.115","id":"main.115","presentation_id":"38929178"},{"card_image_alt_text":"A representative figure from paper main.114","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.114.png","content":{"abstract":"We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.","authors":["Shu Okabe","Fr\u00e9d\u00e9ric Blain","Lucia Specia"],"demo_url":"","keywords":["Multimodal Estimation","Machine Translation","Quality Estimation","Quality QE"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.114.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.114","similar_paper_uids":["main.114","main.558","main.400","main.306","main.214"],"title":"Multimodal Quality Estimation for Machine Translation","tldr":"We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level pr...","track":"Resources and Evaluation"},"forum":"main.114","id":"main.114","presentation_id":"38929452"},{"card_image_alt_text":"A representative figure from paper main.333","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.333.png","content":{"abstract":"Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.","authors":["Bo Pang","Erik Nijkamp","Wenjuan Han","Linqi Zhou","Yixian Liu","Kewei Tu"],"demo_url":"","keywords":["Holistic Generation","Open-domain generation","Natural Processing","human evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.333.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.333","similar_paper_uids":["main.333","main.4","main.173","main.708","main.568"],"title":"Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation","tldr":"Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated subst...","track":"Resources and Evaluation"},"forum":"main.333","id":"main.333","presentation_id":"38929412"},{"card_image_alt_text":"A representative figure from paper main.441","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.441.png","content":{"abstract":"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.","authors":["Yixin Nie","Adina Williams","Emily Dinan","Mohit Bansal","Jason Weston","Douwe Kiela"],"demo_url":"","keywords":["Adversarial NLI","Natural Understanding","never-ending scenario","NLU"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.441.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.441","similar_paper_uids":["main.441","main.197","main.115","main.477","main.770"],"title":"Adversarial NLI: A New Benchmark for Natural Language Understanding","tldr":"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI be...","track":"Resources and Evaluation"},"forum":"main.441","id":"main.441","presentation_id":"38928732"},{"card_image_alt_text":"A representative figure from paper main.327","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.327.png","content":{"abstract":"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.","authors":["Kosuke Takahashi","Katsuhito Sudoh","Satoshi Nakamura"],"demo_url":"","keywords":["Automatic Evaluation","machine translation","Cross-lingual Model","regression model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.327.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.327","similar_paper_uids":["main.327","main.153","main.4","srw.58","main.554"],"title":"Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model","tldr":"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired so...","track":"Resources and Evaluation"},"forum":"main.327","id":"main.327","presentation_id":"38928994"},{"card_image_alt_text":"A representative figure from paper main.508","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.508.png","content":{"abstract":"In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC). For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions. Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.","authors":["Hongming Zhang","Xinran Zhao","Yangqiu Song"],"demo_url":"","keywords":["Deep Knowledge","Answering Challenge","WinoWhy","commonsense reasoning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.508.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.508","similar_paper_uids":["main.508","tacl.1882","main.115","main.387","main.769"],"title":"WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge","tldr":"In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC). For each of the questions, we invite annotators to first provide reasons for making correct decisio...","track":"Resources and Evaluation"},"forum":"main.508","id":"main.508","presentation_id":"38928876"},{"card_image_alt_text":"A representative figure from paper main.118","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.118.png","content":{"abstract":"We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.","authors":["Silviu Oprea","Walid Magdy"],"demo_url":"","keywords":["textual detection","NLP research","iSarcasm","labelling methods"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.118.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.118","similar_paper_uids":["main.118","main.711","main.349","main.401","main.75"],"title":"iSarcasm: A Dataset of Intended Sarcasm","tldr":"We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpr...","track":"Resources and Evaluation"},"forum":"main.118","id":"main.118","presentation_id":"38929208"},{"card_image_alt_text":"A representative figure from paper main.440","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.440.png","content":{"abstract":"Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.","authors":["Angela Lin","Sudha Rao","Asli Celikyilmaz","Elnaz Nouri","Chris Brockett","Debadeepta Dey","Bill Dolan"],"demo_url":"","keywords":["Sequential Tasks","high-level tasks","cooking domain","high-level task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.440.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.440","similar_paper_uids":["main.440","main.729","main.644","main.229","demo.41"],"title":"A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks","tldr":"Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe ...","track":"Resources and Evaluation"},"forum":"main.440","id":"main.440","presentation_id":"38929210"},{"card_image_alt_text":"A representative figure from paper main.332","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.332.png","content":{"abstract":"The recent proliferation of ''fake news'' has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","authors":["Shaden Shaar","Nikolay Babulkov","Giovanni Da San Martino","Preslav Nakov"],"demo_url":"","keywords":["Detecting Claims","manual initiatives","manual fact-checking","retrieval"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.332.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.332","similar_paper_uids":["main.332","main.308","main.761","main.17","main.655"],"title":"That is a Known Lie: Detecting Previously Fact-Checked Claims","tldr":"The recent proliferation of ''fake news'' has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which incre...","track":"Resources and Evaluation"},"forum":"main.332","id":"main.332","presentation_id":"38929333"},{"card_image_alt_text":"A representative figure from paper main.442","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.442.png","content":{"abstract":"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.","authors":["Marco Tulio Ribeiro","Tongshuang Wu","Carlos Guestrin","Sameer Singh"],"demo_url":"","keywords":["measuring accuracy","generalization","behavioral testing","software engineering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.442.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.442","similar_paper_uids":["main.442","srw.55","main.543","tacl.1743","main.506"],"title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList","tldr":"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific be...","track":"Resources and Evaluation"},"forum":"main.442","id":"main.442","presentation_id":"38929272"},{"card_image_alt_text":"A representative figure from paper main.330","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.330.png","content":{"abstract":"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF.","authors":["Canwen Xu","Jiaxin Pei","Hongtao Wu","Yiyu Liu","Chenliang Li"],"demo_url":"","keywords":["Classification","Question Answering","Summarization","Natural Processing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.330.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.330","similar_paper_uids":["main.330","main.651","main.222","main.388","main.600"],"title":"MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization","tldr":"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MA...","track":"Resources and Evaluation"},"forum":"main.330","id":"main.330","presentation_id":"38928859"},{"card_image_alt_text":"A representative figure from paper main.331","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.331.png","content":{"abstract":"News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.","authors":["Fangzhao Wu","Ying Qiao","Jiun-Hung Chen","Chuhan Wu","Tao Qi","Jianxun Lian","Danyang Liu","Xing Xie","Jianfeng Gao","Winnie Wu","Ming Zhou"],"demo_url":"","keywords":["News Recommendation","personalized service","news understanding","MIND"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.331.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.331","similar_paper_uids":["main.331","main.77","main.392","demo.58","demo.100"],"title":"MIND: A Large-scale Dataset for News Recommendation","tldr":"News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of ...","track":"Resources and Evaluation"},"forum":"main.331","id":"main.331","presentation_id":"38928993"},{"card_image_alt_text":"A representative figure from paper main.443","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.443.png","content":{"abstract":"There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We also present the SoftNER model that combines contextual information with domain specific knowledge using an attention network. The code token recognizer combined with an entity segmentation model we proposed, consistently improves the performance of the named entity tagger. Our proposed SoftNER tagger outperforms the BiLSTM-CRF model with an absolute increase of +9.73 F-1 score on StackOverflow data. We have published our code and data at: https://github.com/jeniyat/StackOverflowNER","authors":["Jeniya Tabassum","Mounica Maddela","Wei Xu","Alan Ritter"],"demo_url":"","keywords":["Named Recognition","computer domain","StackOverflow","NLP techniques"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.443.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.443","similar_paper_uids":["main.443","main.538","main.449","demo.115","main.290"],"title":"Code and Named Entity Recognition in StackOverflow","tldr":"There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related ques...","track":"Resources and Evaluation"},"forum":"main.443","id":"main.443","presentation_id":"38929334"},{"card_image_alt_text":"A representative figure from paper main.447","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.447.png","content":{"abstract":"We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.","authors":["Kyle Lo","Lucy Wang","Mark Neumann","Rodney Kinney","Daniel Weld"],"demo_url":"","keywords":["text mining","S2ORC","bibliographic references","inline citations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.447.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.447","similar_paper_uids":["main.447","demo.48","demo.93","main.464","demo.46"],"title":"S2ORC: The Semantic Scholar Open Research Corpus","tldr":"We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open a...","track":"Resources and Evaluation"},"forum":"main.447","id":"main.447","presentation_id":"38929131"},{"card_image_alt_text":"A representative figure from paper main.446","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.446.png","content":{"abstract":"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpus-level metrics.","authors":["Katherine Stasaski","Grace Hui Yang","Marti A. Hearst"],"demo_url":"","keywords":["Automated dialogue","diversity problem","Diversity-Informed Collection","emotion classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.446.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.446","similar_paper_uids":["main.446","main.52","main.638","main.634","main.568"],"title":"More Diverse Dialogue Datasets via Diversity-Informed Data Collection","tldr":"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity pr...","track":"Resources and Evaluation"},"forum":"main.446","id":"main.446","presentation_id":"38929100"},{"card_image_alt_text":"A representative figure from paper main.444","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.444.png","content":{"abstract":"We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/.","authors":["Dian Yu","Kai Sun","Claire Cardie","Dong Yu"],"demo_url":"","keywords":["Dialogue-Based Extraction","cross-sentence RE","dialogue-based tasks","conversational settings"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.444.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.444","similar_paper_uids":["main.444","main.98","demo.79","main.60","main.126"],"title":"Dialogue-Based Relation Extraction","tldr":"We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross...","track":"Resources and Evaluation"},"forum":"main.444","id":"main.444","presentation_id":"38928692"},{"card_image_alt_text":"A representative figure from paper main.108","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.108.png","content":{"abstract":"This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper.","authors":["Janek Bevendorff","Khalid Al Khatib","Martin Potthast","Benno Stein"],"demo_url":"","keywords":["Preprocessing Lists","Dialog Analysis","Crawling","semantically components"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.108.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.108","similar_paper_uids":["main.108","main.767","main.449","main.290","main.447"],"title":"Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis","tldr":"This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent com...","track":"Resources and Evaluation"},"forum":"main.108","id":"main.108","presentation_id":"38928790"},{"card_image_alt_text":"A representative figure from paper main.109","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.109.png","content":{"abstract":"The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs. We propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on Universal Dependencies. We show that our framework provides a detailed picture of cross-language divergences, generalizes previous approaches, and lends itself to full automation. We further present a novel dataset, a manually word-aligned subset of the Parallel UD corpus in five languages, and use it to perform a detailed corpus study. We demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser.","authors":["Dmitry Nikolaev","Ofir Arviv","Taelin Karidi","Neta Kenneth","Veronika Mitnik","Lilja Maria Saeboe","Omri Abend"],"demo_url":"","keywords":["Fine-Grained Divergences","cross-lingual transfer","full automation","cross-lingual parser"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.109.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.109","similar_paper_uids":["main.109","main.554","main.658","main.421","srw.137"],"title":"Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences","tldr":"The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences...","track":"Resources and Evaluation"},"forum":"main.109","id":"main.109","presentation_id":"38928779"},{"card_image_alt_text":"A representative figure from paper main.445","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.445.png","content":{"abstract":"Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary. To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods. Data can be found at https://github.com/morningmoni/FAR.","authors":["Yuning Mao","Liyuan Liu","Qi Zhu","Xiang Ren","Jiawei Han"],"demo_url":"","keywords":["Facet-Aware Evaluation","Extractive Summarization","fine-grained evaluation","comparative analysis"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.445.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.445","similar_paper_uids":["main.445","main.552","main.124","main.453","main.100"],"title":"Facet-Aware Evaluation for Extractive Summarization","tldr":"Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we tre...","track":"Resources and Evaluation"},"forum":"main.445","id":"main.445","presentation_id":"38928688"},{"card_image_alt_text":"A representative figure from paper main.424","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.424.png","content":{"abstract":"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","authors":["Fernando Alva-Manchego","Louis Martin","Antoine Bordes","Carolina Scarton","Beno\u00eet Sagot","Lucia Specia"],"demo_url":"","keywords":["Tuning Models","rewriting transformations","automatic simplification","splitting"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.424.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.424","similar_paper_uids":["main.424","main.709","main.707","main.173","main.448"],"title":"ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations","tldr":"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete...","track":"Resources and Evaluation"},"forum":"main.424","id":"main.424","presentation_id":"38929012"},{"card_image_alt_text":"A representative figure from paper main.425","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.425.png","content":{"abstract":"Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges. We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB). BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community. We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB. BabelPic is available for download at http://babelpic.org.","authors":["Agostina Calabrese","Michele Bevilacqua","Roberto Navigli"],"demo_url":"","keywords":["multimodal research","BabelPic","language system","pre-trained systems"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.425.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.425","similar_paper_uids":["main.425","demo.69","main.444","main.100","main.469"],"title":"Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts","tldr":"Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded ...","track":"Resources and Evaluation"},"forum":"main.425","id":"main.425","presentation_id":"38929120"},{"card_image_alt_text":"A representative figure from paper main.157","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.157.png","content":{"abstract":"We present a new challenging stance detection dataset, called Will-They-Won\u2019t-They (WT\u2013WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain.","authors":["Costanza Conforti","Jakob Berndt","Mohammad Taher Pilehvar","Chryssi Giannitsarou","Flavio Toxvaerd","Nigel Collier"],"demo_url":"","keywords":["Stance Detection","stance systems","Will-They-Won\u2019t-They WT","Will-They-Won\u2019t-They"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.157.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.157","similar_paper_uids":["main.157","main.291","main.509","main.565","main.441"],"title":"Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter","tldr":"We present a new challenging stance detection dataset, called Will-They-Won\u2019t-They (WT\u2013WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore...","track":"Resources and Evaluation"},"forum":"main.157","id":"main.157","presentation_id":"38929075"},{"card_image_alt_text":"A representative figure from paper main.156","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.156.png","content":{"abstract":"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for several mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.","authors":["Pedro Javier Ortiz Su\u00e1rez","Laurent Romary","Beno\u00eet Sagot"],"demo_url":"","keywords":["Contextualized Embeddings","cleaning","part-of-speech tasks","parsing tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.156.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.156","similar_paper_uids":["main.156","main.421","demo.116","main.260","main.329"],"title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages","tldr":"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for several mid-resource languages. We then compare the performance of O...","track":"Resources and Evaluation"},"forum":"main.156","id":"main.156","presentation_id":"38929074"},{"card_image_alt_text":"A representative figure from paper cl.1547","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/cl.1547.png","content":{"abstract":"Despite an ever growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation which requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: the information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging due to lack of resources, lower quality of tools and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing datasets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.","authors":["G\u00f6zde G\u00fcl Sahin","Clara Vania","Ilia Kuznetsov","Iryna Gurevych"],"demo_url":"","keywords":["Word Representations","NLP","classification tasks","probing tasks"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00376?mobileUi=0","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/coli_a_00376?mobileUi=0","similar_paper_uids":["cl.1547","main.420","demo.115","main.140","main.383"],"title":"LINSPECTOR: Multilingual Probing Tasks for Word Representations","tldr":"Despite an ever growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to ...","track":"Resources and Evaluation"},"forum":"cl.1547","id":"cl.1547","presentation_id":"38929480"},{"card_image_alt_text":"A representative figure from paper tacl.1756","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1756.png","content":{"abstract":"Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense-Tagged Sentences (PSTS). Built upon the premise that a word's paraphrases instantiate its fine-grained meanings -- i.e. 'bug' has different meanings corresponding to its paraphrases 'fly' and 'microbe' -- the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the target word takes on the meaning of the paraphrase. We describe an automatic method based on bilingual pivoting used to enumerate sentences for PSTS, and present two models for ranking PSTS sentences based on their quality. Finally, we demonstrate the utility of PSTS by using it to build a dataset for the task of hypernym prediction in context. Training a model on this automatically-generated dataset produces accuracy that is competitive with a model trained on smaller datasets crafted with some manual effort.","authors":["Anne Cocos","Chris Callison-Burch"],"demo_url":"","keywords":["natural tasks","ranking sentences","hypernym prediction","sense-aware models"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00295","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00295","similar_paper_uids":["tacl.1756","main.463","main.382","main.144","main.545"],"title":"Paraphrase-Sense-Tagged Sentences","tldr":"Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a par...","track":"Resources and Evaluation"},"forum":"tacl.1756","id":"tacl.1756","presentation_id":"38929488"}]
