[{"card_image_alt_text":"A representative figure from paper main.359","card_image_path":"static/images/papers/main.359.png","content":{"abstract":"Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.","authors":["Xabier Soto","Dimitar Shterionov","Alberto Poncelas","Andy Way"],"demo_url":"","keywords":["Neural Translation","Machine MT","new systems","MT systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.359.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.359","similar_paper_uids":["main.359","main.692","main.252","main.217","main.151"],"title":"Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation","tldr":"Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when...","track":"Machine Translation"},"forum":"main.359","id":"main.359","presentation_id":"38929436"},{"card_image_alt_text":"A representative figure from paper main.170","card_image_path":"static/images/papers/main.170.png","content":{"abstract":"Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.","authors":["Ivan Provilkov","Dmitrii Emelianenko","Elena Voita"],"demo_url":"","keywords":["open problem","machine translation","subword segmentation","training"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.170.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.170","similar_paper_uids":["main.170","main.275","main.29","main.755","srw.2"],"title":"BPE-Dropout: Simple and Effective Subword Regularization","tldr":"Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones int...","track":"Machine Translation"},"forum":"main.170","id":"main.170","presentation_id":"38928817"},{"card_image_alt_text":"A representative figure from paper main.171","card_image_path":"static/images/papers/main.171.png","content":{"abstract":"Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with the goal of transferring the AR model's generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model's performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.","authors":["Jiawei Zhou","Phillip Keung"],"demo_url":"","keywords":["Non-autoregressive Translation","WMT14 tasks","monolingual augmentation","knowledge distillation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.171.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.171","similar_paper_uids":["main.171","main.15","main.251","main.753","main.235"],"title":"Improving Non-autoregressive Neural Machine Translation with Monolingual Data","tldr":"Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with the goal of tra...","track":"Machine Translation"},"forum":"main.171","id":"main.171","presentation_id":"38929283"},{"card_image_alt_text":"A representative figure from paper main.165","card_image_path":"static/images/papers/main.165.png","content":{"abstract":"Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.","authors":["Haoming Jiang","Chen Liang","Chong Wang","Tuo Zhao"],"demo_url":"","keywords":["knowledge transfer","domain sharing","NMT tasks","Multi-Domain Translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.165.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.165","similar_paper_uids":["main.165","main.565","main.370","main.595","main.150"],"title":"Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing","tldr":"Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propo...","track":"Machine Translation"},"forum":"main.165","id":"main.165","presentation_id":"38928696"},{"card_image_alt_text":"A representative figure from paper main.40","card_image_path":"static/images/papers/main.40.png","content":{"abstract":"Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.","authors":["Xiangpeng Wei","Heng Yu","Yue Hu","Yue Zhang","Rongxiang Weng","Weihua Luo"],"demo_url":"","keywords":["Neural Translation","training models","IWSLT tasks","WMT14 task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.40.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.40","similar_paper_uids":["main.40","main.41","main.34","main.38","srw.54"],"title":"Multiscale Collaborative Deep Models for Neural Machine Translation","tldr":"Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models...","track":"Machine Translation"},"forum":"main.40","id":"main.40","presentation_id":"38928953"},{"card_image_alt_text":"A representative figure from paper main.149","card_image_path":"static/images/papers/main.149.png","content":{"abstract":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","authors":["Emanuele Bugliarello","Sabrina J. Mielke","Antonios Anastasopoulos","Ryan Cotterell","Naoaki Okazaki"],"demo_url":"","keywords":["Measuring Difficulty","generation","asymmetric difficulty","machine difficulty"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.149.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.149","similar_paper_uids":["main.149","main.320","demo.66","main.105","main.18"],"title":"It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information","tldr":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are ...","track":"Machine Translation"},"forum":"main.149","id":"main.149","presentation_id":"38929401"},{"card_image_alt_text":"A representative figure from paper main.148","card_image_path":"static/images/papers/main.148.png","content":{"abstract":"Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.","authors":["Biao Zhang","Philip Williams","Ivan Titov","Rico Sennrich"],"demo_url":"","keywords":["Massively Translation","Zero-Shot Translation","neural translation","NMT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.148.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.148","similar_paper_uids":["main.148","main.252","main.150","main.324","main.12"],"title":"Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation","tldr":"Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingu...","track":"Machine Translation"},"forum":"main.148","id":"main.148","presentation_id":"38929037"},{"card_image_alt_text":"A representative figure from paper main.389","card_image_path":"static/images/papers/main.389.png","content":{"abstract":"This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.","authors":["Marion Weller-Di Marco","Alexander Fraser"],"demo_url":"","keywords":["Word Formation","English Translation","NMT","word approach"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.389.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.389","similar_paper_uids":["main.389","main.594","srw.54","main.596","main.650"],"title":"Modeling Word Formation in English\u2013German Neural Machine Translation","tldr":"This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation i...","track":"Machine Translation"},"forum":"main.389","id":"main.389","presentation_id":"38929104"},{"card_image_alt_text":"A representative figure from paper main.41","card_image_path":"static/images/papers/main.41.png","content":{"abstract":"A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).","authors":["Xuebo Liu","Houtim Lai","Derek F. Wong","Lidia S. Chao"],"demo_url":"","keywords":["Neural Translation","norm-based difficulty","WMT'14 tasks","Norm-Based Learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.41.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.41","similar_paper_uids":["main.41","main.34","main.40","main.620","main.693"],"title":"Norm-Based Curriculum Learning for Neural Machine Translation","tldr":"A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an...","track":"Machine Translation"},"forum":"main.41","id":"main.41","presentation_id":"38928801"},{"card_image_alt_text":"A representative figure from paper main.360","card_image_path":"static/images/papers/main.360.png","content":{"abstract":"Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter's initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.","authors":["Christopher Brix","Parnia Bahar","Hermann Ney"],"demo_url":"","keywords":["inference","time-critical computations","transformer architecture","WMT tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.360.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.360","similar_paper_uids":["main.360","main.593","main.325","main.250","main.76"],"title":"Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture","tldr":"Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis ...","track":"Machine Translation"},"forum":"main.360","id":"main.360","presentation_id":"38929399"},{"card_image_alt_text":"A representative figure from paper main.42","card_image_path":"static/images/papers/main.42.png","content":{"abstract":"Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.","authors":["Renjie Zheng","Mingbo Ma","Baigong Zheng","Kaibo Liu","Liang Huang"],"demo_url":"","keywords":["Simultaneous Translation","Chinese-to-English translation","Opportunistic Decoding","Timely Correction"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.42.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.42","similar_paper_uids":["main.42","main.254","main.36","srw.2","main.325"],"title":"Opportunistic Decoding with Timely Correction for Simultaneous Translation","tldr":"Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e...","track":"Machine Translation"},"forum":"main.42","id":"main.42","presentation_id":"38929355"},{"card_image_alt_text":"A representative figure from paper main.689","card_image_path":"static/images/papers/main.689.png","content":{"abstract":"Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.","authors":["Wei Wang","Ye Tian","Jiquan Ngiam","Yinfei Yang","Isaac Caswell","Zarana Parekh"],"demo_url":"","keywords":["Neural Translation","data selection","machine translation","multi-domain curriculum"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.689.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.689","similar_paper_uids":["main.689","main.692","main.595","main.165","main.86"],"title":"Learning a Multi-Domain Curriculum for Neural Machine Translation","tldr":"Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically con...","track":"Machine Translation"},"forum":"main.689","id":"main.689","presentation_id":"38929116"},{"card_image_alt_text":"A representative figure from paper main.273","card_image_path":"static/images/papers/main.273.png","content":{"abstract":"Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.","authors":["Yongjing Yin","Fandong Meng","Jinsong Su","Chulun Zhou","Zhengyuan Yang","Jie Zhou","Jiebo Luo"],"demo_url":"","keywords":["Neural Translation","multi-modal learning","NMT","Graph-based Encoder"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.273.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.273","similar_paper_uids":["main.273","main.570","main.401","main.402","main.556"],"title":"A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation","tldr":"Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of d...","track":"Machine Translation"},"forum":"main.273","id":"main.273","presentation_id":"38929288"},{"card_image_alt_text":"A representative figure from paper main.529","card_image_path":"static/images/papers/main.529.png","content":{"abstract":"In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.","authors":["Yong Cheng","Lu Jiang","Wolfgang Macherey","Jacob Eisenstein"],"demo_url":"","keywords":["Robust Augmentation","Neural Translation","Neural NMT","Neural"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.529.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.529","similar_paper_uids":["main.529","main.41","main.263","srw.2","main.40"],"title":"AdvAug: Robust Adversarial Augmentation for Neural Machine Translation","tldr":"In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vi...","track":"Machine Translation"},"forum":"main.529","id":"main.529","presentation_id":"38929269"},{"card_image_alt_text":"A representative figure from paper main.688","card_image_path":"static/images/papers/main.688.png","content":{"abstract":"Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.","authors":["Alham Fikri Aji","Nikolay Bogoychev","Kenneth Heafield","Rico Sennrich"],"demo_url":"","keywords":["Neural Translation","Transfer Transfer","low-resource translation","information transfer"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.688.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.688","similar_paper_uids":["main.688","main.260","main.239","main.536","main.169"],"title":"In Neural Machine Translation, What Does Transfer Learning Transfer?","tldr":"Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to ...","track":"Machine Translation"},"forum":"main.688","id":"main.688","presentation_id":"38929410"},{"card_image_alt_text":"A representative figure from paper main.37","card_image_path":"static/images/papers/main.37.png","content":{"abstract":"The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (\"phrases\") and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.","authors":["Hongfei Xu","Josef van Genabith","Deyi Xiong","Qiuhui Liu","Jingyi Zhang"],"demo_url":"","keywords":["Neural Translation","WMT tasks","Learning Representations","Transformer model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.37.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.37","similar_paper_uids":["main.37","main.672","main.147","main.38","cl.1552"],"title":"Learning Source Phrase Representations for Neural Machine Translation","tldr":"The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively th...","track":"Machine Translation"},"forum":"main.37","id":"main.37","presentation_id":"38928684"},{"card_image_alt_text":"A representative figure from paper main.274","card_image_path":"static/images/papers/main.274.png","content":{"abstract":"Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest. One of the crucial parts in methods for the BLI task is the matching procedure. Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings. Thus We propose a relaxed matching procedure to find a more precise matching between two languages. We also find that aligning source and target language embedding space bidirectionally will bring significant improvement. We follow the previous iterative framework to conduct experiments. Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.","authors":["Xu Zhao","Zihao Wang","Yong Zhang","Hao Wu"],"demo_url":"","keywords":["Unsupervised BLI","unsupervised Induction(BLI","unsupervised","BLI task"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.274.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.274","similar_paper_uids":["main.274","main.547","main.758","main.578","main.600"],"title":"A Relaxed Matching Procedure for Unsupervised BLI","tldr":"Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest. One of the crucial parts in methods for the BLI task is the matching procedure. Previous works impose a too strong constraint on ...","track":"Machine Translation"},"forum":"main.274","id":"main.274","presentation_id":"38929428"},{"card_image_alt_text":"A representative figure from paper main.275","card_image_path":"static/images/papers/main.275.png","content":{"abstract":"This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).","authors":["Xuanli He","Gholamreza Haffari","Mohammad Norouzi"],"demo_url":"","keywords":["Subword Segmentation","Neural Translation","learning","inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.275.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.275","similar_paper_uids":["main.275","main.170","main.29","main.245","main.615"],"title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation","tldr":"This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning...","track":"Machine Translation"},"forum":"main.275","id":"main.275","presentation_id":"38929282"},{"card_image_alt_text":"A representative figure from paper main.36","card_image_path":"static/images/papers/main.36.png","content":{"abstract":"The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.","authors":["Junliang Guo","Linli Xu","Enhong Chen"],"demo_url":"","keywords":["Non-Autoregressive Translation","natural tasks","non-autoregressive translation~(NAT","non-autoregressive"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.36.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.36","similar_paper_uids":["main.36","main.687","main.38","tacl.1849","main.609"],"title":"Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation","tldr":"The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly...","track":"Machine Translation"},"forum":"main.36","id":"main.36","presentation_id":"38928715"},{"card_image_alt_text":"A representative figure from paper main.34","card_image_path":"static/images/papers/main.34.png","content":{"abstract":"Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.","authors":["Kehai Chen","Rui Wang","Masao Utiyama","Eiichiro Sumita"],"demo_url":"","keywords":["Content Translation","Neural translation","translation","WMT17 tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.34.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.34","similar_paper_uids":["main.34","main.41","main.40","srw.54","main.532"],"title":"Content Word Aware Neural Machine Translation","tldr":"Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words)...","track":"Machine Translation"},"forum":"main.34","id":"main.34","presentation_id":"38928867"},{"card_image_alt_text":"A representative figure from paper main.277","card_image_path":"static/images/papers/main.277.png","content":{"abstract":"Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.","authors":["Qiu Ran","Yankai Lin","Peng Li","Jie Zhou"],"demo_url":"","keywords":["Non-Autoregressive Translation","Non-Autoregressive ","inference process","multi-modality problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.277.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.277","similar_paper_uids":["main.277","main.29","main.251","main.641","main.15"],"title":"Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation","tldr":"Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from t...","track":"Machine Translation"},"forum":"main.277","id":"main.277","presentation_id":"38929246"},{"card_image_alt_text":"A representative figure from paper main.276","card_image_path":"static/images/papers/main.276.png","content":{"abstract":"We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.","authors":["Pratik Jawanpuria","Mayank Meghwanshi","Bamdev Mishra"],"demo_url":"","keywords":["unsupervised embeddings","alignment problem","domain problem","bilingual task"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.276.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.276","similar_paper_uids":["main.276","main.324","main.766","main.318","srw.137"],"title":"Geometry-aware domain adaptation for unsupervised alignment of word embeddings","tldr":"We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the ma...","track":"Machine Translation"},"forum":"main.276","id":"main.276","presentation_id":"38929319"},{"card_image_alt_text":"A representative figure from paper main.35","card_image_path":"static/images/papers/main.35.png","content":{"abstract":"Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.","authors":["Jierui Li","Lemao Liu","Huayang Li","Guanlin Li","Guoping Huang","Shuming Shi"],"demo_url":"","keywords":["Neural Translation","translation tasks","Explanation Methods","black-box models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.35.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.35","similar_paper_uids":["main.35","srw.55","main.491","main.387","main.448"],"title":"Evaluating Explanation Methods for Neural Machine Translation","tldr":"Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, h...","track":"Machine Translation"},"forum":"main.35","id":"main.35","presentation_id":"38929375"},{"card_image_alt_text":"A representative figure from paper main.38","card_image_path":"static/images/papers/main.38.png","content":{"abstract":"The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.","authors":["Hongfei Xu","Qiuhui Liu","Josef van Genabith","Deyi Xiong","Jingyi Zhang"],"demo_url":"","keywords":["Lipschitz Initialization","Deep Transformers","Transformer model","layer normalization"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.38.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.38","similar_paper_uids":["main.38","main.250","main.687","main.686","main.504"],"title":"Lipschitz Constrained Parameter Initialization for Deep Transformers","tldr":"The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer norm...","track":"Machine Translation"},"forum":"main.38","id":"main.38","presentation_id":"38928681"},{"card_image_alt_text":"A representative figure from paper main.694","card_image_path":"static/images/papers/main.694.png","content":{"abstract":"Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture. In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows. We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.","authors":["Hendra Setiawan","Matthias Sperber","Udhyakumar Nallasamy","Matthias Paulik"],"demo_url":"","keywords":["Variational Translation","Variational VNMT","Variational","generation translations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.694.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.694","similar_paper_uids":["main.694","main.753","main.437","main.235","main.316"],"title":"Variational Neural Machine Translation with Normalizing Flows","tldr":"Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introd...","track":"Machine Translation"},"forum":"main.694","id":"main.694","presentation_id":"38928909"},{"card_image_alt_text":"A representative figure from paper main.253","card_image_path":"static/images/papers/main.253.png","content":{"abstract":"Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.","authors":["Sergey Edunov","Myle Ott","Marc'Aurelio Ranzato","Michael Auli"],"demo_url":"","keywords":["Machine SystemsTrained","translation","translationese","fluency"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.253.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.253","similar_paper_uids":["main.253","main.691","main.532","main.320","main.113"],"title":"On The Evaluation of Machine Translation SystemsTrained With Back-Translation","tldr":"Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the ...","track":"Machine Translation"},"forum":"main.253","id":"main.253","presentation_id":"38928797"},{"card_image_alt_text":"A representative figure from paper main.252","card_image_path":"static/images/papers/main.252.png","content":{"abstract":"Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.","authors":["Aditya Siddhant","Ankur Bapna","Yuan Cao","Orhan Firat","Mia Chen","Sneha Kudugunta","Naveen Arivazhagan","Yonghui Wu"],"demo_url":"","keywords":["Multilingual Translation","Multilingual ","low-resource translation","low-resource NMT"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.252.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.252","similar_paper_uids":["main.252","main.148","srw.54","srw.137","main.150"],"title":"Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation","tldr":"Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The...","track":"Machine Translation"},"forum":"main.252","id":"main.252","presentation_id":"38929252"},{"card_image_alt_text":"A representative figure from paper main.326","card_image_path":"static/images/papers/main.326.png","content":{"abstract":"The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justi\ufb01cation for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","authors":["Chaojun Wang","Rico Sennrich"],"demo_url":"","keywords":["Domain Translation","neural translation","NMT","beam problem"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.326.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.326","similar_paper_uids":["main.326","main.264","main.468","main.262","main.690"],"title":"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","tldr":"The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exp...","track":"Machine Translation"},"forum":"main.326","id":"main.326","presentation_id":"38928743"},{"card_image_alt_text":"A representative figure from paper main.39","card_image_path":"static/images/papers/main.39.png","content":{"abstract":"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","authors":["Yann Dubois","Gautier Dagan","Dieuwke Hupkes","Elia Bruni"],"demo_url":"","keywords":["Extrapolation","natural processing","generalization","Lookup task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.39.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.39","similar_paper_uids":["main.39","main.432","main.419","main.387","main.312"],"title":"Location Attention for Extrapolation to Longer Sequences","tldr":"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions ...","track":"Machine Translation"},"forum":"main.39","id":"main.39","presentation_id":"38929417"},{"card_image_alt_text":"A representative figure from paper main.318","card_image_path":"static/images/papers/main.318.png","content":{"abstract":"Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages. However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words. To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way. We first build a graph for each language with its vertices representing different words. Then we extract word cliques from the graphs and map the cliques of two languages. Based on that, we induce the initial word translation solution with the central words of the aligned cliques. This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.","authors":["Shuo Ren","Shujie Liu","Ming Zhou","Shuai Ma"],"demo_url":"","keywords":["Unsupervised Induction","inducing translations","bilingual induction","Graph-based Method"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.318.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.318","similar_paper_uids":["main.318","main.94","main.766","main.156","main.618"],"title":"A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction","tldr":"Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions...","track":"Machine Translation"},"forum":"main.318","id":"main.318","presentation_id":"38928855"},{"card_image_alt_text":"A representative figure from paper main.324","card_image_path":"static/images/papers/main.324.png","content":{"abstract":"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","authors":["Haipeng Sun","Rui Wang","Kehai Chen","Masao Utiyama","Eiichiro Sumita","Tiejun Zhao"],"demo_url":"","keywords":["Multilingual Translation","Unsupervised translation","Unsupervised UNMT","multilingual UNMT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.324.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.324","similar_paper_uids":["main.324","srw.137","main.148","main.150","main.421"],"title":"Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation","tldr":"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at th...","track":"Machine Translation"},"forum":"main.324","id":"main.324","presentation_id":"38928849"},{"card_image_alt_text":"A representative figure from paper main.278","card_image_path":"static/images/papers/main.278.png","content":{"abstract":"Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.","authors":["Shuo Wang","Zhaopeng Tu","Shuming Shi","Yang Liu"],"demo_url":"","keywords":["Inference Translation","neural translation","NMT","inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.278.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.278","similar_paper_uids":["main.278","main.188","main.242","main.532","main.575"],"title":"On the Inference Calibration of Neural Machine Translation","tldr":"Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While ...","track":"Machine Translation"},"forum":"main.278","id":"main.278","presentation_id":"38929377"},{"card_image_alt_text":"A representative figure from paper main.251","card_image_path":"static/images/papers/main.251.png","content":{"abstract":"We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.","authors":["Lifu Tu","Richard Yuanzhe Pang","Sam Wiseman","Kevin Gimpel"],"demo_url":"","keywords":["Energy-Based Networks","Non-Autoregressive Translation","Non-Autoregressive ","ENGINE"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.251.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.251","similar_paper_uids":["main.251","main.171","main.277","main.36","main.593"],"title":"ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation","tldr":"We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) tra...","track":"Machine Translation"},"forum":"main.251","id":"main.251","presentation_id":"38929336"},{"card_image_alt_text":"A representative figure from paper main.325","card_image_path":"static/images/papers/main.325.png","content":{"abstract":"This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.","authors":["Raymond Hendy Susanto","Shamil Chollampatt","Liling Tan"],"demo_url":"","keywords":["Lexically Translation","neural translation","Levenshtein Transformer","beam decoding"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.325.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.325","similar_paper_uids":["main.325","main.593","main.504","main.744","main.537"],"title":"Lexically Constrained Neural Machine Translation with Levenshtein Transformer","tldr":"This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search ...","track":"Machine Translation"},"forum":"main.325","id":"main.325","presentation_id":"38928773"},{"card_image_alt_text":"A representative figure from paper main.319","card_image_path":"static/images/papers/main.319.png","content":{"abstract":"Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems\u2014fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.","authors":["Wei Zou","Shujian Huang","Jun Xie","Xinyu Dai","Jiajun Chen"],"demo_url":"","keywords":["Reinforced Examples","Neural Translation","Neural ","industrial maintenance"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.319.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.319","similar_paper_uids":["main.319","srw.105","main.540","main.326","main.590"],"title":"A Reinforced Generation of Adversarial Examples for Neural Machine Translation","tldr":"Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems\u2014fathoming how and when neural-based systems fail in such cases is critical for indus...","track":"Machine Translation"},"forum":"main.319","id":"main.319","presentation_id":"38929000"},{"card_image_alt_text":"A representative figure from paper main.321","card_image_path":"static/images/papers/main.321.png","content":{"abstract":"Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts (In this work, document-level contexts denote the surrounding sentences of the current source sentence.) are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.","authors":["Shuming Ma","Dongdong Zhang","Ming Zhou"],"demo_url":"","keywords":["Document-Level Translation","Unified Encoder","encoders","pre-training models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.321.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.321","similar_paper_uids":["main.321","main.207","main.141","main.322","main.670"],"title":"A Simple and Effective Unified Encoder for Document-Level Machine Translation","tldr":"Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts (In this work, document-level contexts denote the surrounding sentences of th...","track":"Machine Translation"},"forum":"main.321","id":"main.321","presentation_id":"38929020"},{"card_image_alt_text":"A representative figure from paper main.686","card_image_path":"static/images/papers/main.686.png","content":{"abstract":"Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.","authors":["Hanrui Wang","Zhanghao Wu","Zhijian Liu","Han Cai","Ligeng Zhu","Chuang Gan","Song Han"],"demo_url":"","keywords":["Natural Processing","Natural tasks","low-latency inference","machine tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.686.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.686","similar_paper_uids":["main.686","main.204","main.411","main.38","main.504"],"title":"HAT: Hardware-Aware Transformers for Efficient Natural Language Processing","tldr":"Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to desi...","track":"Machine Translation"},"forum":"main.686","id":"main.686","presentation_id":"38928706"},{"card_image_alt_text":"A representative figure from paper main.692","card_image_path":"static/images/papers/main.692.png","content":{"abstract":"The notion of ``in-domain data'' in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.","authors":["Roee Aharoni","Yoav Goldberg"],"demo_url":"","keywords":["NLP","data-driven domains","neural translation","Unsupervised Clusters"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.692.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.692","similar_paper_uids":["main.692","main.689","main.595","main.165","main.740"],"title":"Unsupervised Domain Clusters in Pretrained Language Models","tldr":"The notion of ``in-domain data'' in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challen...","track":"Machine Translation"},"forum":"main.692","id":"main.692","presentation_id":"38929165"},{"card_image_alt_text":"A representative figure from paper main.533","card_image_path":"static/images/papers/main.533.png","content":{"abstract":"Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.","authors":["Shun-Po Chuang","Tzu-Wei Sung","Alexander H. Liu","Hung-yi Lee"],"demo_url":"","keywords":["Speech translation","Word Embedding","ST","multitask learning"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.533.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.533","similar_paper_uids":["main.533","main.36","main.316","main.531","srw.42"],"title":"Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation","tldr":"Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text ...","track":"Machine Translation"},"forum":"main.533","id":"main.533","presentation_id":"38929303"},{"card_image_alt_text":"A representative figure from paper main.532","card_image_path":"static/images/papers/main.532.png","content":{"abstract":"In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.","authors":["Benjamin Marie","Raphael Rubino","Atsushi Fujita"],"demo_url":"","keywords":["Tagged Revisited","neural systems","NMT systems","back-translations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.532.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.532","similar_paper_uids":["main.532","main.253","main.252","main.691","srw.54"],"title":"Tagged Back-translation Revisited: Why Does It Really Work?","tldr":"In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translati...","track":"Machine Translation"},"forum":"main.532","id":"main.532","presentation_id":"38929455"},{"card_image_alt_text":"A representative figure from paper main.254","card_image_path":"static/images/papers/main.254.png","content":{"abstract":"Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.","authors":["Baigong Zheng","Kaibo Liu","Renjie Zheng","Mingbo Ma","Hairong Liu","Liang Huang"],"demo_url":"","keywords":["simultaneous translation","full-sentence translation","Simultaneous Policies","Adaptive policies"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.254.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.254","similar_paper_uids":["main.254","main.42","main.155","demo.33","main.400"],"title":"Simultaneous Translation Policies: From Fixed to Adaptive","tldr":"Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive p...","track":"Machine Translation"},"forum":"main.254","id":"main.254","presentation_id":"38928934"},{"card_image_alt_text":"A representative figure from paper main.693","card_image_path":"static/images/papers/main.693.png","content":{"abstract":"We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective. We first sample pseudo-documents from sentence samples. We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT). This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.","authors":["Danielle Saunders","Felix Stahlberg","Bill Byrne"],"demo_url":"","keywords":["Neural training","NMT training","document-level training","NMT objective"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.693.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.693","similar_paper_uids":["main.693","main.207","main.322","main.321","main.40"],"title":"Using Context in Neural Machine Translation Training Objectives","tldr":"We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspon...","track":"Machine Translation"},"forum":"main.693","id":"main.693","presentation_id":"38928974"},{"card_image_alt_text":"A representative figure from paper main.687","card_image_path":"static/images/papers/main.687.png","content":{"abstract":"Recent work has questioned the importance of the Transformer's multi-headed attention for achieving high translation quality. We push further in this direction by developing a ``hard-coded'' attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.","authors":["Weiqiu You","Simeng Sun","Mohit Iyyer"],"demo_url":"","keywords":["Neural Translation","Hard-Coded Attention","variant","encoder"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.687.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.687","similar_paper_uids":["main.687","main.385","main.387","srw.115","tacl.1815"],"title":"Hard-Coded Gaussian Attention for Neural Machine Translation","tldr":"Recent work has questioned the importance of the Transformer's multi-headed attention for achieving high translation quality. We push further in this direction by developing a ``hard-coded'' attention variant without any learned parameters. Surprisin...","track":"Machine Translation"},"forum":"main.687","id":"main.687","presentation_id":"38929389"},{"card_image_alt_text":"A representative figure from paper main.320","card_image_path":"static/images/papers/main.320.png","content":{"abstract":"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.","authors":["Shuo Ren","Yu Wu","Shujie Liu","Ming Zhou","Shuai Ma"],"demo_url":"","keywords":["Unsupervised Translation","translation","Retrieve-and-Rewrite Method","translation models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.320.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.320","similar_paper_uids":["main.320","main.149","main.124","main.253","main.522"],"title":"A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation","tldr":"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage i...","track":"Machine Translation"},"forum":"main.320","id":"main.320","presentation_id":"38928942"},{"card_image_alt_text":"A representative figure from paper main.322","card_image_path":"static/images/papers/main.322.png","content":{"abstract":"In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.","authors":["Bei Li","Hui Liu","Ziyang Wang","Yufan Jiang","Tong Xiao","Jingbo Zhu","Tongran Liu","Changliang Li"],"demo_url":"","keywords":["Context-Aware Translation","document-level translation","document-level NMT","document-level"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.322.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.322","similar_paper_uids":["main.322","main.321","main.693","srw.116","main.273"],"title":"Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation","tldr":"In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translatio...","track":"Machine Translation"},"forum":"main.322","id":"main.322","presentation_id":"38929005"},{"card_image_alt_text":"A representative figure from paper main.691","card_image_path":"static/images/papers/main.691.png","content":{"abstract":"Machine translation has an undesirable propensity to produce ``translationese\" artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.","authors":["Parker Riley","Isaac Caswell","Markus Freitag","David Grangier"],"demo_url":"","keywords":["Translationese","Machine translation","zero-shot translation","Multilingual NMT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.691.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.691","similar_paper_uids":["main.691","main.253","main.532","main.773","main.690"],"title":"Translationese as a Language in \"Multilingual\" NMT","tldr":"Machine translation has an undesirable propensity to produce ``translationese\" artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as sepa...","track":"Machine Translation"},"forum":"main.691","id":"main.691","presentation_id":"38928765"},{"card_image_alt_text":"A representative figure from paper main.530","card_image_path":"static/images/papers/main.530.png","content":{"abstract":"The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.","authors":["KayYen Wong","Sameen Maruf","Gholamreza Haffari"],"demo_url":"","keywords":["Translation Pronouns","translation phenomena","anaphora translation","Contextual Translation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.530.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.530","similar_paper_uids":["main.530","main.396","main.322","main.693","tacl.1915"],"title":"Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns","tldr":"The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as ...","track":"Machine Translation"},"forum":"main.530","id":"main.530","presentation_id":"38928844"},{"card_image_alt_text":"A representative figure from paper main.531","card_image_path":"static/images/papers/main.531.png","content":{"abstract":"Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.","authors":["Jian Yang","Shuming Ma","Dongdong Zhang","Zhoujun Li","Ming Zhou"],"demo_url":"","keywords":["Neural Translation","translation","translation procedure","Soft Prediction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.531.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.531","similar_paper_uids":["main.531","main.757","srw.54","main.641"],"title":"Improving Neural Machine Translation with Soft Template Prediction","tldr":"Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in oth...","track":"Machine Translation"},"forum":"main.531","id":"main.531","presentation_id":"38929072"},{"card_image_alt_text":"A representative figure from paper main.690","card_image_path":"static/images/papers/main.690.png","content":{"abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a `balanced' dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is `catastrophic forgetting', which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","authors":["Danielle Saunders","Bill Byrne"],"demo_url":"","keywords":["Reducing Bias","Neural Translation","Domain Problem","NLP tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.690.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.690","similar_paper_uids":["main.690","main.484","main.619","main.262","main.326"],"title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","tldr":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammat...","track":"Machine Translation"},"forum":"main.690","id":"main.690","presentation_id":"38928755"},{"card_image_alt_text":"A representative figure from paper main.323","card_image_path":"static/images/papers/main.323.png","content":{"abstract":"The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.","authors":["Hongfei Xu","Josef van Genabith","Deyi Xiong","Qiuhui Liu"],"demo_url":"","keywords":["Dynamically Size","Monitoring Change","accelerating convergence","training"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.323.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.323","similar_paper_uids":["main.323","main.504","main.593","main.686","main.325"],"title":"Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change","tldr":"The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate,...","track":"Machine Translation"},"forum":"main.323","id":"main.323","presentation_id":"38929013"},{"card_image_alt_text":"A representative figure from paper main.152","card_image_path":"static/images/papers/main.152.png","content":{"abstract":"We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.","authors":["Pinzhen Chen","Nikolay Bogoychev","Kenneth Heafield","Faheem Kirefu"],"demo_url":"","keywords":["Parallel Mining","decoding","Constrained Decoding","neural translation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.152.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.152","similar_paper_uids":["main.152","main.756","srw.54","main.147","srw.137"],"title":"Parallel Sentence Mining by Constrained Decoding","tldr":"We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus...","track":"Machine Translation"},"forum":"main.152","id":"main.152","presentation_id":"38929223"},{"card_image_alt_text":"A representative figure from paper main.620","card_image_path":"static/images/papers/main.620.png","content":{"abstract":"Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.","authors":["Yikai Zhou","Baosong Yang","Derek F. Wong","Yu Wan","Lidia S. Chao"],"demo_url":"","keywords":["Neural Translation","assessment difficulty","translation tasks","Uncertainty-Aware Learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.620.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.620","similar_paper_uids":["main.620","main.41","main.542","main.34","main.36"],"title":"Uncertainty-Aware Curriculum Learning for Neural Machine Translation","tldr":"Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose unc...","track":"Machine Translation"},"forum":"main.620","id":"main.620","presentation_id":"38928807"},{"card_image_alt_text":"A representative figure from paper main.146","card_image_path":"static/images/papers/main.146.png","content":{"abstract":"Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.","authors":["Thomas Zenkel","Joern Wuebker","John DeNero"],"demo_url":"","keywords":["Word alignment","unsupervised task","natural processing","neural translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.146.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.146","similar_paper_uids":["main.146","main.578","main.747","srw.2","main.40"],"title":"End-to-End Neural Word Alignment Outperforms GIZA++","tldr":"Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still p...","track":"Machine Translation"},"forum":"main.146","id":"main.146","presentation_id":"38929138"},{"card_image_alt_text":"A representative figure from paper main.754","card_image_path":"static/images/papers/main.754.png","content":{"abstract":"When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.","authors":["Xinyi Wang","Yulia Tsvetkov","Graham Neubig"],"demo_url":"","keywords":["Multilingual Translation","Balancing Training","multilingual models","heuristic baselines"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.754.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.754","similar_paper_uids":["main.754","main.720","main.769","main.750","main.18"],"title":"Balancing Training for Multilingual Neural Machine Translation","tldr":"When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less reso...","track":"Machine Translation"},"forum":"main.754","id":"main.754","presentation_id":"38928991"},{"card_image_alt_text":"A representative figure from paper main.755","card_image_path":"static/images/papers/main.755.png","content":{"abstract":"Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.","authors":["Xing Niu","Prashant Mathur","Georgiana Dinu","Yaser Al-Onaizan"],"demo_url":"","keywords":["Neural Translation","Neural models","subword methods","relative degradation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.755.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.755","similar_paper_uids":["main.755","main.540","main.245","main.317","main.138"],"title":"Evaluating Robustness to Input Perturbations for Neural Machine Translation","tldr":"Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metric...","track":"Machine Translation"},"forum":"main.755","id":"main.755","presentation_id":"38929225"},{"card_image_alt_text":"A representative figure from paper main.147","card_image_path":"static/images/papers/main.147.png","content":{"abstract":"Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.","authors":["Emanuele Bugliarello","Naoaki Okazaki"],"demo_url":"","keywords":["Machine Translation","neural models","attention mechanism","Transformer model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.147.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.147","similar_paper_uids":["main.147","main.252","main.687","main.37","main.336"],"title":"Enhancing Machine Translation with Dependency-Aware Self-Attention","tldr":"Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge i...","track":"Machine Translation"},"forum":"main.147","id":"main.147","presentation_id":"38929162"},{"card_image_alt_text":"A representative figure from paper main.153","card_image_path":"static/images/papers/main.153.png","content":{"abstract":"Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, \\eg machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT'14 English\u21d2German, WAT'17 Japanese\u21d2English, and WMT'17 Chinese\u21d4English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.","authors":["Liang Ding","Longyue Wang","Dacheng Tao"],"demo_url":"","keywords":["natural tasks","WMT'17 tasks","Cross-Lingual Representation","Position encoding"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.153.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.153","similar_paper_uids":["main.153","main.554","main.269","demo.116","main.121"],"title":"Self-Attention with Cross-Lingual Position Representation","tldr":"Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenar...","track":"Machine Translation"},"forum":"main.153","id":"main.153","presentation_id":"38928754"},{"card_image_alt_text":"A representative figure from paper main.145","card_image_path":"static/images/papers/main.145.png","content":{"abstract":"We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.","authors":["Yingqiang Gao","Nikola I. Nikolov","Yuhuang Hu","Richard H.R. Hahnloser"],"demo_url":"","keywords":["Character-Level Translation","bilingual translation","self-attention models","transformer model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.145.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.145","similar_paper_uids":["main.145","srw.9","main.270","main.687","main.38"],"title":"Character-Level Translation with Self-attention","tldr":"We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convol...","track":"Machine Translation"},"forum":"main.145","id":"main.145","presentation_id":"38929029"},{"card_image_alt_text":"A representative figure from paper main.151","card_image_path":"static/images/papers/main.151.png","content":{"abstract":"Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish \"translationese\", i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.","authors":["Wei Zhao","Goran Glava\u0161","Maxime Peyrard","Yang Gao","Robert West","Steffen Eger"],"demo_url":"","keywords":["Evaluation encoders","zero-shot transfer","supervised tasks","web-scale systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.151.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.151","similar_paper_uids":["main.151","main.554","main.421","main.113","main.658"],"title":"On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation","tldr":"Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine...","track":"Machine Translation"},"forum":"main.151","id":"main.151","presentation_id":"38929444"},{"card_image_alt_text":"A representative figure from paper main.757","card_image_path":"static/images/papers/main.757.png","content":{"abstract":"Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.","authors":["Xintong Li","Lemao Liu","Rui Wang","Guoping Huang","Max Meng"],"demo_url":"","keywords":["Machine Translation","learning gates","Regularized Gates","Transformer"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.757.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.757","similar_paper_uids":["main.757","tacl.1843","main.504","main.125","main.753"],"title":"Regularized Context Gates on Transformer for Machine Translation","tldr":"Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer archite...","track":"Machine Translation"},"forum":"main.757","id":"main.757","presentation_id":"38928846"},{"card_image_alt_text":"A representative figure from paper main.756","card_image_path":"static/images/papers/main.756.png","content":{"abstract":"Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new state-of-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.","authors":["Boliang Zhang","Ajay Nagesh","Kevin Knight"],"demo_url":"","keywords":["machine models","WMT task","Parallel Filtering","Pre-trained Models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.756.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.756","similar_paper_uids":["main.756","main.417","demo.54","srw.137","main.294"],"title":"Parallel Corpus Filtering via Pre-trained Language Models","tldr":"Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than tra...","track":"Machine Translation"},"forum":"main.756","id":"main.756","presentation_id":"38928720"},{"card_image_alt_text":"A representative figure from paper main.150","card_image_path":"static/images/papers/main.150.png","content":{"abstract":"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.","authors":["Changfeng Zhu","Heng Yu","Shanbo Cheng","Weihua Luo"],"demo_url":"","keywords":["Multilingual Translation","low-resource scenarios","Language-aware Interlingua","NMT"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.150.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.150","similar_paper_uids":["main.150","main.252","main.148","main.304","main.324"],"title":"Language-aware Interlingua for Multilingual Neural Machine Translation","tldr":"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity ...","track":"Machine Translation"},"forum":"main.150","id":"main.150","presentation_id":"38929351"},{"card_image_alt_text":"A representative figure from paper main.144","card_image_path":"static/images/papers/main.144.png","content":{"abstract":"This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with ``copy'' information while translations based on embedding similarities tend to extend the translation ``context''. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.","authors":["Jitao XU","Josep Crego","Jean Senellart"],"demo_url":"","keywords":["Boosting Translation","Neural Translation","data methods","human translator"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.144.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.144","similar_paper_uids":["main.144","tacl.1756","main.463","main.692","main.253"],"title":"Boosting Neural Machine Translation with Similar Translations","tldr":"This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model...","track":"Machine Translation"},"forum":"main.144","id":"main.144","presentation_id":"38929113"},{"card_image_alt_text":"A representative figure from paper main.154","card_image_path":"static/images/papers/main.154.png","content":{"abstract":"The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages ``sound'' older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.","authors":["Dirk Hovy","Federico Bianchi","Tommaso Fornaciari"],"demo_url":"","keywords":["machine translation","commercial systems","machine systems","translation models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.154.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.154","similar_paper_uids":["main.154","main.468","main.690","main.702","main.332"],"title":"``You Sound Just Like Your Father'' Commercial Machine Translation Systems Include Stylistic Biases","tldr":"The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make de...","track":"Machine Translation"},"forum":"main.154","id":"main.154","presentation_id":"38929070"},{"card_image_alt_text":"A representative figure from paper main.753","card_image_path":"static/images/papers/main.753.png","content":{"abstract":"This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro\u2194En and De\u2194En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).","authors":["Arya D. McCarthy","Xian Li","Jiatao Gu","Ning Dong"],"demo_url":"","keywords":["Variational Translation","posterior collapse","auxiliary task","uncertainty"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.753.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.753","similar_paper_uids":["main.753","main.694","main.235","main.437","main.316"],"title":"Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation","tldr":"This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well a...","track":"Machine Translation"},"forum":"main.753","id":"main.753","presentation_id":"38929049"},{"card_image_alt_text":"A representative figure from paper main.143","card_image_path":"static/images/papers/main.143.png","content":{"abstract":"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.","authors":["Xiangyu Duan","Baijun Ji","Hao Jia","Min Tan","Min Zhang","Boxing Chen","Weihua Luo","Yue Zhang"],"demo_url":"","keywords":["Bilingual Translation","machine MT","MT","dictionary-based translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.143.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.143","similar_paper_uids":["main.143","main.201","srw.137","main.324","main.148"],"title":"Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences","tldr":"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the...","track":"Machine Translation"},"forum":"main.143","id":"main.143","presentation_id":"38929017"},{"card_image_alt_text":"A representative figure from paper main.619","card_image_path":"static/images/papers/main.619.png","content":{"abstract":"Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).","authors":["Luisa Bentivogli","Beatrice Savoldi","Matteo Negri","Mattia A. Di Gangi","Roldano Cattoni","Marco Turchi"],"demo_url":"","keywords":["Speech Technology","Translating","machines","machine translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.619.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.619","similar_paper_uids":["main.619","main.690","main.484","main.597","main.262"],"title":"Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus","tldr":"Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically ref...","track":"Machine Translation"},"forum":"main.619","id":"main.619","presentation_id":"38929196"},{"card_image_alt_text":"A representative figure from paper main.618","card_image_path":"static/images/papers/main.618.png","content":{"abstract":"Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure. It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings. In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs. Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process. We show the benefits of ClassyMap for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs.","authors":["Mladen Karan","Ivan Vuli\u0107","Anna Korhonen","Goran Glava\u0161"],"demo_url":"","keywords":["Weakly Induction","self-learning","induction CLWEs","bilingual induction"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.618.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.618","similar_paper_uids":["main.618","main.675","main.766","main.201","main.318"],"title":"Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction","tldr":"Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure. It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings. In this work, we pre...","track":"Machine Translation"},"forum":"main.618","id":"main.618","presentation_id":"38929124"},{"card_image_alt_text":"A representative figure from paper cl.1482","card_image_path":"static/images/papers/cl.1482.png","content":{"abstract":"Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.","authors":["Yonatan Belinkov","Nadir Durrani","Fahim Dalvi","Hassan Sajjad","James Glass"],"demo_url":"","keywords":["Linguistic Models","natural processing","artificial intelligence","translating languages"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00367","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/coli_a_00367","similar_paper_uids":["cl.1482","main.682","main.493","cl.1552","main.422"],"title":"On the Linguistic Representational Power of Neural Machine Translation Models","tldr":"Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) mod...","track":"Machine Translation"},"forum":"cl.1482","id":"cl.1482","presentation_id":"38929477"},{"card_image_alt_text":"A representative figure from paper cl.1552","card_image_path":"static/images/papers/cl.1552.png","content":{"abstract":"Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning.We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability of encoding linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.","authors":["Ra\u00fal V\u00e1zquez","Alessandro Raganato","Mathias Creutz","J\u00f6rg Tiedemann"],"demo_url":"","keywords":["Multilingual Translation","Neural translation","transfer learning","translation"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00377","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/coli_a_00377","similar_paper_uids":["cl.1552","main.311","main.422","main.687","main.385"],"title":"A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation","tldr":"Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence repr...","track":"Machine Translation"},"forum":"cl.1552","id":"cl.1552","presentation_id":"38929482"},{"card_image_alt_text":"A representative figure from paper cl.1550","card_image_path":"static/images/papers/cl.1550.png","content":{"abstract":"Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.","authors":["Tasnim Mohiuddin","Shafiq Joty"],"demo_url":"","keywords":["Unsupervised Translation","machine translation","transfer learning","word task"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00374","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/COLI_a_00374","similar_paper_uids":["cl.1550","srw.105","main.590","main.540","main.688"],"title":"Unsupervised Word Translation with Adversarial Autoencoder","tldr":"Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embedding...","track":"Machine Translation"},"forum":"cl.1550","id":"cl.1550","presentation_id":"38929481"},{"card_image_alt_text":"A representative figure from paper tacl.2001","card_image_path":"static/images/papers/tacl.2001.png","content":{"abstract":"We show that Bayes' rule provides an effective mechanism for creating document translation models that can be learned from only parallel sentences and monolingual documents---a compelling benefit as parallel documents are not always available. In our formulation, the posterior probability of a candidate translation is the product of the unconditional (prior) probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the source language. Our proposed model uses a powerful autoregressive language model as the prior on target language documents, but it assumes that each sentence is translated independently from the target to the source language. Crucially, at test time, when a source document is observed, the document language model prior induces dependencies between the translations of the source sentences in the posterior. The model's independence assumption not only enables efficient use of available data, but it additionally admits a practical left-to-right beam-search algorithm for carrying out inference. Experiments show that our model benefits from using cross-sentence context in the language model, and it outperforms existing document translation approaches.","authors":["Lei Yu","Laurent Sartran","Wojciech Stokowiec","Wang Ling","Lingpeng Kong","Phil Blunsom","Chris Dyer"],"demo_url":"","keywords":["Document-level Translation","inference","Bayes Rule","document models"],"paper_type":"TACL","pdf_url":"","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"","similar_paper_uids":["tacl.2001","main.580","main.207","main.235","main.321"],"title":"Better Document-level Machine Translation with Bayes' Rule","tldr":"We show that Bayes' rule provides an effective mechanism for creating document translation models that can be learned from only parallel sentences and monolingual documents---a compelling benefit as parallel documents are not always available. In our...","track":"Machine Translation"},"forum":"tacl.2001","id":"tacl.2001","presentation_id":"38929514"}]
