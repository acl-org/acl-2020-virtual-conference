[{"card_image_alt_text":"A representative figure from paper main.402","card_image_path":"static/images/papers/main.402.png","content":{"abstract":"The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.","authors":["Tulika Saha","Aditya Patra","Sriparna Saha","Pushpak Bhattacharyya"],"demo_url":"","keywords":["Emotion-aided Classification","Dialogue Classification","automatic DAs","ER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.402.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.402","similar_paper_uids":["main.402","main.401","main.570","main.273","main.388"],"title":"Towards Emotion-aided Multi-modal Dialogue Act Classification","tldr":"The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to id...","track":"Speech and Multimodality"},"forum":"main.402","id":"main.402","presentation_id":"38929394"},{"card_image_alt_text":"A representative figure from paper main.400","card_image_path":"static/images/papers/main.400.png","content":{"abstract":"Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.","authors":["Shaowei Yao","Xiaojun Wan"],"demo_url":"","keywords":["Multimodal MMT","Multimodal","MMT","representation images"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.400.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.400","similar_paper_uids":["main.400","main.731","main.114","main.343","main.215"],"title":"Multimodal Transformer for Multimodal Machine Translation","tldr":"Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative i...","track":"Speech and Multimodality"},"forum":"main.400","id":"main.400","presentation_id":"38929440"},{"card_image_alt_text":"A representative figure from paper main.213","card_image_path":"static/images/papers/main.213.png","content":{"abstract":"Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.","authors":["Yu-An Chung","James Glass"],"demo_url":"","keywords":["Training objectives","downstream tasks","generalization task","phonetic classification"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.213.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.213","similar_paper_uids":["main.213","main.351","main.473","main.438","main.1"],"title":"Improved Speech Representations with Multi-Target Autoregressive Predictive Coding","tldr":"Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregre...","track":"Speech and Multimodality"},"forum":"main.213","id":"main.213","presentation_id":"38928760"},{"card_image_alt_text":"A representative figure from paper main.401","card_image_path":"static/images/papers/main.401.png","content":{"abstract":"In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention). The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities. Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.","authors":["Dushyant Singh Chauhan","Dhanush S R","Asif Ekbal","Pushpak Bhattacharyya"],"demo_url":"","keywords":["Sentiment Analysis","Sentiment ","multi-tasking","sarcasm detection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.401.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.401","similar_paper_uids":["main.401","main.402","main.273","main.570","main.343"],"title":"Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis","tldr":"In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, ...","track":"Speech and Multimodality"},"forum":"main.401","id":"main.401","presentation_id":"38929427"},{"card_image_alt_text":"A representative figure from paper main.217","card_image_path":"static/images/papers/main.217.png","content":{"abstract":"End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work -- by up to 9 BLEU on our low-resource setting.","authors":["Elizabeth Salesky","Alan W Black"],"demo_url":"","keywords":["Speech Translation","speech recognition","ASR","machine translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.217.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.217","similar_paper_uids":["main.217","main.661","main.614","main.336","main.345"],"title":"Phone Features Improve Speech Translation","tldr":"End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error pro...","track":"Speech and Multimodality"},"forum":"main.217","id":"main.217","presentation_id":"38929284"},{"card_image_alt_text":"A representative figure from paper main.216","card_image_path":"static/images/papers/main.216.png","content":{"abstract":"This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.","authors":["Georgios Paraskevopoulos","Srinivas Parthasarathy","Aparna Khare","Shiva Sundaram"],"demo_url":"","keywords":["Multimodal Recognition","ASR","multiresolution ASR","Transformers"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.216.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.216","similar_paper_uids":["main.216","main.273","main.38","main.683","main.505"],"title":"Multimodal and Multiresolution Speech Recognition with Transformers","tldr":"This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for ...","track":"Speech and Multimodality"},"forum":"main.216","id":"main.216","presentation_id":"38928735"},{"card_image_alt_text":"A representative figure from paper main.348","card_image_path":"static/images/papers/main.348.png","content":{"abstract":"An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.","authors":["Genta Indra Winata","Samuel Cahyawijaya","Zhaojiang Lin","Zihan Liu","Peng Xu","Pascale Fung"],"demo_url":"","keywords":["Code-Switched Recognition","speech recognition","speech tasks","language tasks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.348.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.348","similar_paper_uids":["main.348","main.329","main.716","main.336","main.747"],"title":"Meta-Transfer Learning for Code-Switched Speech Recognition","tldr":"An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expens...","track":"Speech and Multimodality"},"forum":"main.348","id":"main.348","presentation_id":"38928739"},{"card_image_alt_text":"A representative figure from paper main.214","card_image_path":"static/images/papers/main.214.png","content":{"abstract":"Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.","authors":["Wasifur Rahman","Md Kamrul Hasan","Sangwu Lee","AmirAli Bagher Zadeh","Chengfeng Mao","Louis-Philippe Morency","Ehsan Hoque"],"demo_url":"","keywords":["NLP","lexical applications","modeling communication","multimodal analysis"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.214.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.214","similar_paper_uids":["main.214","srw.79","main.197","main.314","main.357"],"title":"Integrating Multimodal Information in Large Pretrained Transformers","tldr":"Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to ac...","track":"Speech and Multimodality"},"forum":"main.214","id":"main.214","presentation_id":"38929139"},{"card_image_alt_text":"A representative figure from paper main.215","card_image_path":"static/images/papers/main.215.png","content":{"abstract":"We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.","authors":["Jakob D. Havtorn","Jan Latko","Joakim Edin","Lars Maal\u00f8e","Lasse Borgholt","Lorenzo Belgrano","Nicolai Jacobsen","Regitze Sdun","\u017deljko Agi\u0107"],"demo_url":"","keywords":["real-time speech","labeling speech","emergency services","real-time labeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.215.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.215","similar_paper_uids":["main.215","main.400","main.155","demo.33","main.189"],"title":"MultiQT: Multimodal learning for real-time question tracking in speech","tldr":"We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a no...","track":"Speech and Multimodality"},"forum":"main.215","id":"main.215","presentation_id":"38929009"},{"card_image_alt_text":"A representative figure from paper main.349","card_image_path":"static/images/papers/main.349.png","content":{"abstract":"Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. Thus traditional text-based methods are insufficient to detect multimodal sarcasm. To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context. Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net). The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context. Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.","authors":["Nan Xu","Zhixiong Zeng","Wenji Mao"],"demo_url":"","keywords":["Reasoning","sarcasm","multimodal detection","Sarcasm"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.349.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.349","similar_paper_uids":["main.349","main.118","main.711","main.306","main.114"],"title":"Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association","tldr":"Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no ...","track":"Speech and Multimodality"},"forum":"main.349","id":"main.349","presentation_id":"38928703"},{"card_image_alt_text":"A representative figure from paper main.350","card_image_path":"static/images/papers/main.350.png","content":{"abstract":"In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.","authors":["Yi Ren","Jinglin Liu","Xu Tan","Chen Zhang","Tao Qin","Zhou Zhao","Tie-Yan Liu"],"demo_url":"","keywords":["simultaneous translation","simultaneous recognition","ASR","NMT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.350.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.350","similar_paper_uids":["main.350","main.344","main.36","main.217","main.147"],"title":"SimulSpeech: End-to-End Simultaneous Speech to Text Translation","tldr":"In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a ...","track":"Speech and Multimodality"},"forum":"main.350","id":"main.350","presentation_id":"38929241"},{"card_image_alt_text":"A representative figure from paper main.344","card_image_path":"static/images/papers/main.344.png","content":{"abstract":"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.","authors":["Chengyi Wang","Yu Wu","Shujie Liu","Ming Zhou","Zhenglu Yang"],"demo_url":"","keywords":["Curriculum Pre-training","End-to-End Translation","speech recognition","transcription learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.344.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.344","similar_paper_uids":["main.344","main.347","main.370","main.247","main.89"],"title":"Curriculum Pre-training for End-to-End Speech Translation","tldr":"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech...","track":"Speech and Multimodality"},"forum":"main.344","id":"main.344","presentation_id":"38928753"},{"card_image_alt_text":"A representative figure from paper main.345","card_image_path":"static/images/papers/main.345.png","content":{"abstract":"In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.","authors":["Archiki Prasad","Preethi Jyothi"],"demo_url":"","keywords":["Probing","Accent Information","End-to-End Systems","end-to-end system"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.345.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.345","similar_paper_uids":["main.345","main.661","main.217","main.395","main.1"],"title":"How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems","tldr":"In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convo...","track":"Speech and Multimodality"},"forum":"main.345","id":"main.345","presentation_id":"38929438"},{"card_image_alt_text":"A representative figure from paper main.351","card_image_path":"static/images/papers/main.351.png","content":{"abstract":"Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.","authors":["Karan Singla","Zhuohao Chen","David Atkins","Shrikanth Narayanan"],"demo_url":"","keywords":["predicting codes","Spoken tasks","voice detection","speaker diarization"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.351.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.351","similar_paper_uids":["main.351","main.206","main.344","main.215","main.1"],"title":"Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations","tldr":"Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance leve...","track":"Speech and Multimodality"},"forum":"main.351","id":"main.351","presentation_id":"38929299"},{"card_image_alt_text":"A representative figure from paper main.347","card_image_path":"static/images/papers/main.347.png","content":{"abstract":"Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.","authors":["Chao-Wei Huang","Yun-Nung Chen"],"demo_url":"","keywords":["NLP tasks","spoken tasks","intent detection","Spoken Representations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.347.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.347","similar_paper_uids":["main.347","demo.67","main.374","main.522","main.391"],"title":"Learning Spoken Language Representations with Neural Lattice Language Modeling","tldr":"Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea...","track":"Speech and Multimodality"},"forum":"main.347","id":"main.347","presentation_id":"38929295"},{"card_image_alt_text":"A representative figure from paper main.346","card_image_path":"static/images/papers/main.346.png","content":{"abstract":"Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. However, we show that self-training --- a semi-supervised technique for incorporating unlabeled data --- sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations. We also show that ensembling self-trained parsers provides further gains for disfluency detection.","authors":["Paria Jamshid Lou","Mark Johnson"],"demo_url":"","keywords":["Disfluency Detection","joint parsing","Self-Attentive Model","Self-attentive parsers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.346.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.346","similar_paper_uids":["main.346","tacl.1801","main.197","main.347","main.89"],"title":"Improving Disfluency Detection by Self-Training a Self-Attentive Model","tldr":"Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are ...","track":"Speech and Multimodality"},"forum":"main.346","id":"main.346","presentation_id":"38929215"},{"card_image_alt_text":"A representative figure from paper main.343","card_image_path":"static/images/papers/main.343.png","content":{"abstract":"Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The full dataset and codes are available for use at https://github.com/thuiar/MMSA.","authors":["Wenmeng Yu","Hua Xu","Fanyang Meng","Yilin Zhu","Yixiao Ma","Jiele Wu","Jiyun Zou","Kaicheng Yang"],"demo_url":"","keywords":["multimodal analysis","unimodal analysis","CH-SIMS","multimodal annotations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.343.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.343","similar_paper_uids":["main.343","main.570","main.374","main.290","main.273"],"title":"CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality","tldr":"Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model...","track":"Speech and Multimodality"},"forum":"main.343","id":"main.343","presentation_id":"38928933"},{"card_image_alt_text":"A representative figure from paper tacl.1834","card_image_path":"static/images/papers/tacl.1834.png","content":{"abstract":"Humans rarely perform better than chance at lie detection. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built classifiers to automatically distinguish trusted from mistrusted speech, achieving an F1 of 66.1%. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, while several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the strategies that judges reported using in deception detection were not helpful for the task. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.","authors":["Xi (Leslie) Chen","Sarah Ita Levitan","Michelle Levine","Marko Mandic","and Julia Hirschberg"],"demo_url":"","keywords":["Deception","lie detection","human deception","deception detection"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00311","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00311","similar_paper_uids":["tacl.1834","main.353","main.405","main.2","main.419"],"title":"Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies","tldr":"Humans rarely perform better than chance at lie detection. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interview...","track":"Speech and Multimodality"},"forum":"tacl.1834","id":"tacl.1834","presentation_id":"38929497"}]
