[{"card_image_alt_text":"A representative figure from paper main.629","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.629.png","content":{"abstract":"Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT. The resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 datasets among previous state-of-the-art graph-based parsers.","authors":["Daniel Fern\u00e1ndez-Gonz\u00e1lez","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":"","keywords":["dependency parsing","harder problem","NLP problem","semantic parsing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.629.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.629","similar_paper_uids":["main.629","main.605","main.67","tacl.1876","main.588"],"title":"Transition-based Semantic Dependency Parsing with Pointer Networks","tldr":"Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the cap...","track":"Semantics: Sentence Level"},"forum":"main.629","id":"main.629","presentation_id":"38928865"},{"card_image_alt_text":"A representative figure from paper main.628","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.628.png","content":{"abstract":"We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Sch\u00fctze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson\u2019s r over single-source systems.","authors":["Nina Poerner","Ulli Waltinger","Hinrich Sch\u00fctze"],"demo_url":"","keywords":["Unsupervised Similarity","unsupervised STS","dimensionality reduction","pre-trained encoders"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.628.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.628","similar_paper_uids":["main.628","main.654","tacl.1849","main.28","main.247"],"title":"Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity","tldr":"We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature...","track":"Semantics: Sentence Level"},"forum":"main.628","id":"main.628","presentation_id":"38929064"},{"card_image_alt_text":"A representative figure from paper main.398","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.398.png","content":{"abstract":"Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.","authors":["Jonathan Herzig","Pawel Krzysztof Nowak","Thomas M\u00fcller","Francesco Piccinno","Julian Eisenschlos"],"demo_url":"","keywords":["Weakly Parsing","semantic task","question tables","SQA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.398.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.398","similar_paper_uids":["main.398","main.745","main.608","main.247","main.708"],"title":"TaPas: Weakly Supervised Table Parsing via Pre-training","tldr":"Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. ...","track":"Semantics: Sentence Level"},"forum":"main.398","id":"main.398","presentation_id":"38929121"},{"card_image_alt_text":"A representative figure from paper main.607","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.607.png","content":{"abstract":"Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations. We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework. Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence. Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph. Our model is arc-factored and therefore parsing and learning are both tractable. Experiments show our model achieves significant and consistent improvement over the supervised baseline.","authors":["Zixia Jia","Youmi Ma","Jiong Cai","Kewei Tu"],"demo_url":"","keywords":["Semantic parsing","semi-supervised parsers","parsing","learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.607.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.607","similar_paper_uids":["main.607","main.605","main.588","tacl.1876","main.397"],"title":"Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders","tldr":"Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations. We propose an approach to semi-supervised learning of semantic dependency pa...","track":"Semantics: Sentence Level"},"forum":"main.607","id":"main.607","presentation_id":"38929248"},{"card_image_alt_text":"A representative figure from paper main.606","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.606.png","content":{"abstract":"This paper is concerned with semantic parsing for English as a second language (ESL). Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings. We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model. Experiments demonstrate that in comparison to human annotations, our method can obtain a very promising SemBanking quality. By means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models. The evaluation pro\ufb01les the performance of neural NLP techniques for handling ESL data and suggests some research directions.","authors":["Yuanyuan Zhao","Weiwei Sun","Junjie Cao","Xiaojun Wan"],"demo_url":"","keywords":["semantic parsing","second acquisition","Semantic Parsing","ESL"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.606.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.606","similar_paper_uids":["main.606","main.605","main.116","main.669","main.40"],"title":"Semantic Parsing for English as a Second Language","tldr":"This paper is concerned with semantic parsing for English as a second language (ESL). Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the t...","track":"Semantics: Sentence Level"},"forum":"main.606","id":"main.606","presentation_id":"38929300"},{"card_image_alt_text":"A representative figure from paper main.605","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.605.png","content":{"abstract":"We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.","authors":["Yufei Chen","Weiwei Sun"],"demo_url":"","keywords":["logical parsing","Parsing","variable-in-situ graphs","graph-based representation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.605.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.605","similar_paper_uids":["main.605","main.67","main.539","main.607","main.588"],"title":"Parsing into Variable-in-situ Logico-Semantic Graphs","tldr":"We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification...","track":"Semantics: Sentence Level"},"forum":"main.605","id":"main.605","presentation_id":"38929396"},{"card_image_alt_text":"A representative figure from paper main.676","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.676.png","content":{"abstract":"We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.","authors":["Jacob Andreas"],"demo_url":"","keywords":["Good-Enough Augmentation","diagnostic tasks","semantic task","data protocol"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.676.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.676","similar_paper_uids":["main.676","main.770","main.138","main.733","main.754"],"title":"Good-Enough Compositional Data Augmentation","tldr":"We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and r...","track":"Semantics: Sentence Level"},"forum":"main.676","id":"main.676","presentation_id":"38928687"},{"card_image_alt_text":"A representative figure from paper main.677","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.677.png","content":{"abstract":"When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.","authors":["Bailin Wang","Richard Shin","Xiaodong Liu","Oleksandr Polozov","Matthew Richardson"],"demo_url":"","keywords":["generalization challenge","schema encoding","schema linking","RAT-SQL"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.677.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.677","similar_paper_uids":["main.677","demo.130","main.105","main.449","main.538"],"title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers","tldr":"When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database r...","track":"Semantics: Sentence Level"},"forum":"main.677","id":"main.677","presentation_id":"38929348"},{"card_image_alt_text":"A representative figure from paper main.738","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.738.png","content":{"abstract":"We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.","authors":["Belinda Z. Li","Gabriel Stanovsky","Luke Zettlemoyer"],"demo_url":"","keywords":["Coreference Resolution","active resolution","Active Learning","Discrete Annotation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.738.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.738","similar_paper_uids":["main.738","main.622","main.418","main.520","main.193"],"title":"Active Learning for Coreference Resolution using Discrete Annotation","tldr":"We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention...","track":"Semantics: Sentence Level"},"forum":"main.738","id":"main.738","presentation_id":"38928722"},{"card_image_alt_text":"A representative figure from paper main.739","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.739.png","content":{"abstract":"This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time. We present new annotations on top of corpora annotating possession existence and experimental results. Regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds. Regarding co-possessions, we use a binary label. Cohen's kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks.","authors":["Dhivya Chinnappa","Srikala Murugan","Eduardo Blanco"],"demo_url":"","keywords":["Possession Existence","Duration","Co-Possession","duration relations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.739.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.739","similar_paper_uids":["main.739","main.583","main.93","main.497","main.626"],"title":"Beyond Possession Existence: Duration and Co-Possession","tldr":"This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time. We present new annotations on top of corpora annotating possession e...","track":"Semantics: Sentence Level"},"forum":"main.739","id":"main.739","presentation_id":"38928887"},{"card_image_alt_text":"A representative figure from paper main.539","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.539.png","content":{"abstract":"Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking. It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables. This is achieved by a graph module network built upon the Transformer-based architecture. With a textual statement and a table as the input, LogicalFactChecker automatically derives a program (a.k.a. logical form) of the statement in a semantic parsing manner. A heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities. Such a graph reveals the related contexts of each word in the statement, the table and the program. The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture. After that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules. Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results.","authors":["Wanjun Zhong","Duyu Tang","Zhangyin Feng","Nan Duan","Ming Zhou","Ming Gong","Linjun Shou","Daxin Jiang","Jiahai Wang","Jian Yin"],"demo_url":"","keywords":["Fact Checking","LogicalFactChecker","Graph Network","semantic reasoning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.539.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.539","similar_paper_uids":["main.539","main.549","main.605","main.224","main.67"],"title":"LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network","tldr":"Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. In this work, we propose LogicalFactChec...","track":"Semantics: Sentence Level"},"forum":"main.539","id":"main.539","presentation_id":"38928864"},{"card_image_alt_text":"A representative figure from paper main.538","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.538.png","content":{"abstract":"Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.","authors":["Frank F. Xu","Zhengbao Jiang","Pengcheng Yin","Bogdan Vasilescu","Graham Neubig"],"demo_url":"","keywords":["Open-domain generation","NL-to-code generation","data augmentation","retrieval-based re-sampling"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.538.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.538","similar_paper_uids":["main.538","main.449","main.443","main.18","main.135"],"title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation","tldr":"Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we ex...","track":"Semantics: Sentence Level"},"forum":"main.538","id":"main.538","presentation_id":"38928800"},{"card_image_alt_text":"A representative figure from paper main.119","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.119.png","content":{"abstract":"We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).","authors":["Deng Cai","Wai Lam"],"demo_url":"","keywords":["AMR parsing","AMR Parsing","Graph-Sequence Inference","end-to-end model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.119.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.119","similar_paper_uids":["main.119","main.397","tacl.1805","main.167","main.67"],"title":"AMR Parsing via Graph-Sequence Iterative Inference","tldr":"We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that...","track":"Semantics: Sentence Level"},"forum":"main.119","id":"main.119","presentation_id":"38928981"},{"card_image_alt_text":"A representative figure from paper main.536","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.536.png","content":{"abstract":"We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.","authors":["Alexis Conneau","Shijie Wu","Haoran Li","Luke Zettlemoyer","Veselin Stoyanov"],"demo_url":"","keywords":["multilingual modeling","cross-lingual transfer","transfer","Cross-lingual Models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.536.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.536","similar_paper_uids":["main.536","main.421","main.493","main.260","main.329"],"title":"Emerging Cross-lingual Structure in Pretrained Language Models","tldr":"We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cros...","track":"Semantics: Sentence Level"},"forum":"main.536","id":"main.536","presentation_id":"38928831"},{"card_image_alt_text":"A representative figure from paper main.537","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.537.png","content":{"abstract":"Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.","authors":["Weijie Liu","Peng Zhou","Zhiruo Wang","Zhe Zhao","Haotang Deng","QI JU"],"demo_url":"","keywords":["inference","FastBERT","Self-distilling BERT","Pre-trained models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.537.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.537","similar_paper_uids":["main.537","main.593","main.204","main.195","main.197"],"title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time","tldr":"Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their...","track":"Semantics: Sentence Level"},"forum":"main.537","id":"main.537","presentation_id":"38928748"},{"card_image_alt_text":"A representative figure from paper main.608","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.608.png","content":{"abstract":"One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.","authors":["Ruisheng Cao","Su Zhu","Chenyu Yang","Chen Liu","Rao Ma","Yanbin Zhao","Lu Chen","Kai Yu"],"demo_url":"","keywords":["Two-stage Parsing","semantic parsing","annotation","self-supervised tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.608.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.608","similar_paper_uids":["main.608","tacl.1801","main.398","main.545","main.197"],"title":"Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing","tldr":"One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabel...","track":"Semantics: Sentence Level"},"forum":"main.608","id":"main.608","presentation_id":"38929390"},{"card_image_alt_text":"A representative figure from paper main.740","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.740.png","content":{"abstract":"Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.","authors":["Suchin Gururangan","Ana Marasovi\u0107","Swabha Swayamdipta","Kyle Lo","Iz Beltagy","Doug Downey","Noah A. Smith"],"demo_url":"","keywords":["NLP","classification tasks","pretraining","domain-adaptive pretraining"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.740.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.740","similar_paper_uids":["main.740","main.467","main.703","main.244","main.370"],"title":"Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks","tldr":"Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a t...","track":"Semantics: Sentence Level"},"forum":"main.740","id":"main.740","presentation_id":"38929123"},{"card_image_alt_text":"A representative figure from paper main.741","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.741.png","content":{"abstract":"Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data. In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings. We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.","authors":["Vitalii Zhelezniak","Aleksandar Savkov","Nils Hammerla"],"demo_url":"","keywords":["Estimating Information","unsupervised tasks","Dense Embeddings","statistical view"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.741.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.741","similar_paper_uids":["main.741","main.630","main.612","main.628","main.422"],"title":"Estimating Mutual Information Between Dense Word Embeddings","tldr":"Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approac...","track":"Semantics: Sentence Level"},"forum":"main.741","id":"main.741","presentation_id":"38929249"},{"card_image_alt_text":"A representative figure from paper main.743","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.743.png","content":{"abstract":"The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances. In this paper, we experiment with neural networks to predict the focus of negation. Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network. Experimental results show that doing so obtains the best results to date. Additionally, we perform a detailed error analysis providing insights into the main error categories, and analyze errors depending on whether the model takes into account scope and context information.","authors":["Md Mosharaf Hossain","Kathleen Hamilton","Alexis Palmer","Eduardo Blanco"],"demo_url":"","keywords":["Negation","error analysis","Model Analysis","neural networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.743.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.743","similar_paper_uids":["main.743","main.429","tacl.1852","main.680","main.142"],"title":"Predicting the Focus of Negation: Model and Error Analysis","tldr":"The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances. In this paper, we experiment with neural networks to predict the focus of negation. Our main novelt...","track":"Semantics: Sentence Level"},"forum":"main.743","id":"main.743","presentation_id":"38929211"},{"card_image_alt_text":"A representative figure from paper main.540","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.540.png","content":{"abstract":"Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.","authors":["Yuan Zang","Fanchao Qi","Chenghao Yang","Zhiyuan Liu","Meng Zhang","Qun Liu","Maosong Sun"],"demo_url":"","keywords":["Textual attacking","Word-level attacking","combinatorial problem","Word-level Attacking"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.540.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.540","similar_paper_uids":["main.540","srw.105","main.245","main.319","main.249"],"title":"Word-level Textual Adversarial Attacking as Combinatorial Optimization","tldr":"Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level att...","track":"Semantics: Sentence Level"},"forum":"main.540","id":"main.540","presentation_id":"38928950"},{"card_image_alt_text":"A representative figure from paper main.742","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.742.png","content":{"abstract":"We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training. Recently, several datasets, including Spider, were proposed to support development of XSP systems. We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use. We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead. We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set. Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets.","authors":["Alane Suhr","Ming-Wei Chang","Peter Shaw","Kenton Lee"],"demo_url":"","keywords":["Exploring Challenges","Cross-Database Parsing","cross-database XSP","cross-database"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.742.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.742","similar_paper_uids":["main.742","main.677","tacl.1901","main.718","main.595"],"title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing","tldr":"We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training. Recently, several datasets, including Spider, were proposed t...","track":"Semantics: Sentence Level"},"forum":"main.742","id":"main.742","presentation_id":"38929266"},{"card_image_alt_text":"A representative figure from paper main.626","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.626.png","content":{"abstract":"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.","authors":["Paul Roit","Ayal Klein","Daniela Stepanov","Jonathan Mamou","Julian Michael","Gabriel Stanovsky","Luke Zettlemoyer","Ido Dagan"],"demo_url":"","keywords":["High-Quality Annotation","Question-answer Labeling","complex annotation","training"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.626.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.626","similar_paper_uids":["main.626","main.413","demo.48","main.500","main.450"],"title":"Controlled Crowdsourcing for High-Quality QA-SRL Annotation","tldr":"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to...","track":"Semantics: Sentence Level"},"forum":"main.626","id":"main.626","presentation_id":"38929025"},{"card_image_alt_text":"A representative figure from paper main.746","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.746.png","content":{"abstract":"We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.","authors":["Elias Stengel-Eskin","Aaron Steven White","Sheng Zhang","Benjamin Van Durme"],"demo_url":"","keywords":["parsing","Universal Parsing","transductive model","Universal representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.746.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.746","similar_paper_uids":["main.746","main.640","main.607","main.224","main.67"],"title":"Universal Decompositional Semantic Parsing","tldr":"We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attri...","track":"Semantics: Sentence Level"},"forum":"main.746","id":"main.746","presentation_id":"38929022"},{"card_image_alt_text":"A representative figure from paper main.747","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.747.png","content":{"abstract":"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.","authors":["Alexis Conneau","Kartikay Khandelwal","Naman Goyal","Vishrav Chaudhary","Guillaume Wenzek","Francisco Guzm\u00e1n","Edouard Grave","Myle Ott","Luke Zettlemoyer","Veselin Stoyanov"],"demo_url":"","keywords":["cross-lingual tasks","XNLI","MLQA","NER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.747.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.747","similar_paper_uids":["main.747","main.421","main.329","main.493","srw.137"],"title":"Unsupervised Cross-lingual Representation Learning at Scale","tldr":"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more t...","track":"Semantics: Sentence Level"},"forum":"main.747","id":"main.747","presentation_id":"38928776"},{"card_image_alt_text":"A representative figure from paper main.627","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.627.png","content":{"abstract":"Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.","authors":["Hao Fei","Meishan Zhang","Donghong Ji"],"demo_url":"","keywords":["Cross-Lingual Labeling","semantic labeling","natural understanding","model transferring"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.627.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.627","similar_paper_uids":["main.627","main.581","main.336","tacl.1906","main.193"],"title":"Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus","tldr":"Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages s...","track":"Semantics: Sentence Level"},"forum":"main.627","id":"main.627","presentation_id":"38929023"},{"card_image_alt_text":"A representative figure from paper main.745","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.745.png","content":{"abstract":"Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.","authors":["Pengcheng Yin","Graham Neubig","Wen-tau Yih","Sebastian Riedel"],"demo_url":"","keywords":["Joint Data","text-based tasks","semantic parsing","TaBERT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.745.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.745","similar_paper_uids":["main.745","main.398","main.677","main.742","main.195"],"title":"TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data","tldr":"Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing...","track":"Semantics: Sentence Level"},"forum":"main.745","id":"main.745","presentation_id":"38929345"},{"card_image_alt_text":"A representative figure from paper main.744","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.744.png","content":{"abstract":"Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.","authors":["Tao Li","Parth Anand Jawale","Martha Palmer","Vivek Srikumar"],"demo_url":"","keywords":["Semantic Labeling","Structured Tuning","expressive representations","knowledge-rich mechanisms"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.744.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.744","similar_paper_uids":["main.744","main.325","main.593","main.438","main.6"],"title":"Structured Tuning for Semantic Role Labeling","tldr":"Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constr...","track":"Semantics: Sentence Level"},"forum":"main.744","id":"main.744","presentation_id":"38929153"},{"card_image_alt_text":"A representative figure from paper main.630","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.630.png","content":{"abstract":"Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.","authors":["Nicole Peinelt","Dong Nguyen","Maria Liakata"],"demo_url":"","keywords":["Semantic Detection","natural understanding","pairwise detection","tBERT"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.630.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.630","similar_paper_uids":["main.630","main.705","main.76","main.247","main.73"],"title":"tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection","tldr":"Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no...","track":"Semantics: Sentence Level"},"forum":"main.630","id":"main.630","presentation_id":"38928734"},{"card_image_alt_text":"A representative figure from paper main.397","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.397.png","content":{"abstract":"Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).","authors":["Qiji Zhou","Yue Zhang","Donghong Ji","Hao Tang"],"demo_url":"","keywords":["parsing AMR","AMR Parsing","Abstract Representations","AMRs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.397.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.397","similar_paper_uids":["main.397","main.119","main.167","main.67","tacl.1805"],"title":"AMR Parsing with Latent Structural Information","tldr":"Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate t...","track":"Semantics: Sentence Level"},"forum":"main.397","id":"main.397","presentation_id":"38929103"},{"card_image_alt_text":"A representative figure from paper tacl.1805","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1805.png","content":{"abstract":"Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. The current state-of-the-art methods use graph-to-sequence models; however, they still cannot significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address the above-mentioned task. The model directly encodes the AMR graphs and learns the node representations. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, attention mechanisms are employed for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively. Our model outperforms the state-of-the-art neural approach by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.","authors":["Tianming Wang","Xiaojun Wan","Hanqi Jin"],"demo_url":"","keywords":["AMR-To-Text Generation","Abstract generation","generating texts","Graph Transformer"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00297","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00297","similar_paper_uids":["tacl.1805","main.67","main.167","main.119","main.640"],"title":"AMR-To-Text Generation with Graph Transformer","tldr":"Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. The current state-of-the-art methods use graph-to-sequen...","track":"Semantics: Sentence Level"},"forum":"tacl.1805","id":"tacl.1805","presentation_id":"38929494"},{"card_image_alt_text":"A representative figure from paper tacl.1912","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1912.png","content":{"abstract":"Recent years have seen a growing interest within the natural language processing (NLP) community in evaluating the ability of semantic models to capture human meaning representation in the brain. Existing research has mainly focused on applying semantic models to decode brain activity patterns associated with the meaning of individual words, and, more recently, this approach has been extended to sentences and larger text fragments. Our work is the first to investigate metaphor processing in the brain in this context. We evaluate a range of semantic models (word embeddings, compositional, and visual models) in their ability to decode brain activity associated with reading of both literal and metaphoric sentences. Our results suggest that compositional models and word embeddings are able to capture differences in the processing of literal and metaphoric sentences, providing support for the idea that the literal meaning is not fully accessible during familiar metaphor comprehension.","authors":["Vesna G. Djokic","Jean Maillard","Luana Bulat","Ekaterina Shutova"],"demo_url":"","keywords":["Decoding Activity","Literal Comprehension","human representation","metaphor processing"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00307","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00307","similar_paper_uids":["tacl.1912","main.259","main.507","srw.28","main.664"],"title":"Decoding Brain Activity Associated with Literal and Metaphoric Sentence Comprehension using Distributional Semantic Models","tldr":"Recent years have seen a growing interest within the natural language processing (NLP) community in evaluating the ability of semantic models to capture human meaning representation in the brain. Existing research has mainly focused on applying seman...","track":"Semantics: Sentence Level"},"forum":"tacl.1912","id":"tacl.1912","presentation_id":"38929510"}]
