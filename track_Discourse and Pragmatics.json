[{"card_image_alt_text":"A representative figure from paper main.482","card_image_path":"static/images/papers/main.482.png","content":{"abstract":"Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.","authors":["Linfeng Song","Kun Xu","Yue Zhang","Jianshu Chen","Dong Yu"],"demo_url":"","keywords":["Joint Resolution","Zero recovery","resolution","zero resolution"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.482.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.482","similar_paper_uids":["main.482","main.300","main.231","main.671","main.272"],"title":"ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT","tldr":"Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separ...","track":"Discourse and Pragmatics"},"forum":"main.482","id":"main.482","presentation_id":"38928741"},{"card_image_alt_text":"A representative figure from paper main.13","card_image_path":"static/images/papers/main.13.png","content":{"abstract":"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","authors":["Shyh-Shiun Hung","Hen-Hsen Huang","Hsin-Hsi Chen"],"demo_url":"","keywords":["Chinese parsing","rhetorical recognition","Shift-Reduce Parser","Robust Oracle"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.13.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.13","similar_paper_uids":["main.13","tacl.1811","main.376","main.301","main.451"],"title":"A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle","tldr":"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, b...","track":"Discourse and Pragmatics"},"forum":"main.13","id":"main.13","presentation_id":"38929376"},{"card_image_alt_text":"A representative figure from paper main.481","card_image_path":"static/images/papers/main.481.png","content":{"abstract":"We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.","authors":["Shubham Toshniwal","Allyson Ettinger","Kevin Gimpel","Karen Livescu"],"demo_url":"","keywords":["People Tracking","PeTra","Sparsely Model","memory-augmented network"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.481.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.481","similar_paper_uids":["main.481","main.233","main.61","main.643","main.53"],"title":"PeTra: A Sparsely Supervised Memory Model for People Tracking","tldr":"We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simple...","track":"Discourse and Pragmatics"},"forum":"main.481","id":"main.481","presentation_id":"38929426"},{"card_image_alt_text":"A representative figure from paper main.132","card_image_path":"static/images/papers/main.132.png","content":{"abstract":"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of \u201cquasi-bridging\u201d training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro \u0308siger, 2018)).","authors":["Yufang Hou"],"demo_url":"","keywords":["Anaphora Resolution","Question Answering","bridging resolution","pairwise model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.132.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.132","similar_paper_uids":["main.132","main.650","main.357","main.622","srw.79"],"title":"Bridging Anaphora Resolution as Question Answering","tldr":"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora re...","track":"Discourse and Pragmatics"},"forum":"main.132","id":"main.132","presentation_id":"38928717"},{"card_image_alt_text":"A representative figure from paper main.133","card_image_path":"static/images/papers/main.133.png","content":{"abstract":"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code.","authors":["Mohsen Mesgar","Sebastian B\u00fccker","Iryna Gurevych"],"demo_url":"","keywords":["Dialogue Assessment","auxiliary task","multi-task scenario","informative representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.133.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.133","similar_paper_uids":["main.133","main.218","main.637","main.638","srw.22"],"title":"Dialogue Coherence Assessment Without Explicit Dialogue Act Labels","tldr":"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two dra...","track":"Discourse and Pragmatics"},"forum":"main.133","id":"main.133","presentation_id":"38929137"},{"card_image_alt_text":"A representative figure from paper main.480","card_image_path":"static/images/papers/main.480.png","content":{"abstract":"Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0. Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0. We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.","authors":["Najoung Kim","Song Feng","Chulaka Gunasekara","Luis Lastras"],"demo_url":"","keywords":["Implicit Classification","Evaluation","understanding relations","PDTB 2.0"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.480.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.480","similar_paper_uids":["main.480","main.14","main.695","srw.22","main.667"],"title":"Implicit Discourse Relation Classification: We Need to Talk about Evaluation","tldr":"Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair compa...","track":"Discourse and Pragmatics"},"forum":"main.480","id":"main.480","presentation_id":"38929324"},{"card_image_alt_text":"A representative figure from paper main.478","card_image_path":"static/images/papers/main.478.png","content":{"abstract":"Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. Next, we propose several document-level neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research.","authors":["Prafulla Kumar Choubey","Aaron Lee","Ruihong Huang","Lu Wang"],"demo_url":"","keywords":["Profiling Structure","computational structures","event resolution","theory structure"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.478.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.478","similar_paper_uids":["main.478","demo.100","main.77","demo.58","main.331"],"title":"Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event","tldr":"Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news arti...","track":"Discourse and Pragmatics"},"forum":"main.478","id":"main.478","presentation_id":"38928770"},{"card_image_alt_text":"A representative figure from paper main.479","card_image_path":"static/images/papers/main.479.png","content":{"abstract":"Pragmatic inferences often subtly depend on the presence or absence of linguistic features. For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing \"Chris ate some of the cookies\" than after hearing the same utterance without a partitive, \"Chris ate some cookies\". In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences. We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78). We then probe the model's behavior using manually constructed minimal sentence pairs and corpus data. We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.","authors":["Sebastian Schuster","Yuxing Chen","Judith Degen"],"demo_url":"","keywords":["scalar inferences","Pragmatic inferences","partitive construction","neural encoders"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.479.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.479","similar_paper_uids":["main.479","main.768","main.543","tacl.1780","srw.135"],"title":"Harnessing the linguistic signal to predict scalar inferences","tldr":"Pragmatic inferences often subtly depend on the presence or absence of linguistic features. For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that C...","track":"Discourse and Pragmatics"},"forum":"main.479","id":"main.479","presentation_id":"38928842"},{"card_image_alt_text":"A representative figure from paper main.14","card_image_path":"static/images/papers/main.14.png","content":{"abstract":"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","authors":["Ruifang He","Jian Wang","Fengyu Guo","Yugui Han"],"demo_url":"","keywords":["Implicit Recognition","discourse understanding","TransS-Driven Architecture","multi-level encoder"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.14.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.14","similar_paper_uids":["main.14","main.480","main.230","main.526","main.241"],"title":"TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition","tldr":"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have ...","track":"Discourse and Pragmatics"},"forum":"main.14","id":"main.14","presentation_id":"38928869"},{"card_image_alt_text":"A representative figure from paper main.609","card_image_path":"static/images/papers/main.609.png","content":{"abstract":"Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.","authors":["Qiankun Fu","Yue Zhang","Jiangming Liu","Meishan Zhang"],"demo_url":"","keywords":["Discourse parsing","semantic task","tree construction","incremental problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.609.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.609","similar_paper_uids":["main.609","tacl.1805","main.36","main.224","main.67"],"title":"DRTS Parsing with Structure-Aware Encoding and Decoding","tldr":"Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an ...","track":"Discourse and Pragmatics"},"forum":"main.609","id":"main.609","presentation_id":"38929443"},{"card_image_alt_text":"A representative figure from paper main.569","card_image_path":"static/images/papers/main.569.png","content":{"abstract":"Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse. In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing. On the basis, we propose a top-down neural architecture toward text-level DRS parsing. In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly. In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack. Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing.","authors":["Longyin Zhang","Yuqing Xing","Fang Kong","Peifeng Li","Guodong Zhou"],"demo_url":"","keywords":["Text-level Structure","natural understanding","down-stream applications","text-level parsing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.569.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.569","similar_paper_uids":["main.569","main.416","srw.9","main.301","main.609"],"title":"A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure","tldr":"Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous st...","track":"Discourse and Pragmatics"},"forum":"main.569","id":"main.569","presentation_id":"38928746"},{"card_image_alt_text":"A representative figure from paper tacl.1811","card_image_path":"static/images/papers/tacl.1811.png","content":{"abstract":"In this paper, we introduce an unsupervised discourse constituency parsing algorithm. We use Viterbi EM with a margin-based criterion to train a span-based discourse parser in an unsupervised manner. We also propose initialization methods for Viterbi training of discourse constituents based on our prior knowledge of text structures. Experimental results demonstrate that our unsupervised parser achieves comparable or even superior performance to fully supervised parsers. We also investigate discourse constituents that are learned by our method.","authors":["Noriki Nishida","Hideki Nakayama"],"demo_url":"","keywords":["Viterbi constituents","Unsupervised Parsing","Viterbi EM","unsupervised algorithm"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00312","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00312","similar_paper_uids":["tacl.1811","main.13","main.300","main.460","main.439"],"title":"Unsupervised Discourse Constituency Parsing Using Viterbi EM","tldr":"In this paper, we introduce an unsupervised discourse constituency parsing algorithm. We use Viterbi EM with a margin-based criterion to train a span-based discourse parser in an unsupervised manner. We also propose initialization methods for Viterbi...","track":"Discourse and Pragmatics"},"forum":"tacl.1811","id":"tacl.1811","presentation_id":"38929495"}]
