[{"card_image_alt_text":"A representative figure from paper main.229","card_image_path":"static/images/papers/main.229.png","content":{"abstract":"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk's generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.","authors":["Wang Zhu","Hexiang Hu","Jiacheng Chen","Zhiwei Deng","Vihan Jain","Eugene Ie","Fei Sha"],"demo_url":"","keywords":["Vision-and-Language Navigation","vision-and-language VLN","VLN","navigation tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.229.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.229","similar_paper_uids":["main.229","main.769","main.194","main.18","main.88"],"title":"BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps","tldr":"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We sho...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.229","id":"main.229","presentation_id":"38928711"},{"card_image_alt_text":"A representative figure from paper main.729","card_image_path":"static/images/papers/main.729.png","content":{"abstract":"We present a new problem: grounding natural language instructions to mobile user interface actions, and contribute three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.","authors":["Yang Li","Jiacong He","Xin Zhou","Yuan Zhang","Jason Baldridge"],"demo_url":"","keywords":["full evaluation","scale training","predicting sequences","PixelHelp"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.729.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.729","similar_paper_uids":["main.729","main.644","demo.41","main.440","main.232"],"title":"Mapping Natural Language Instructions to Mobile UI Action Sequences","tldr":"We present a new problem: grounding natural language instructions to mobile user interface actions, and contribute three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions perform...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.729","id":"main.729","presentation_id":"38929135"},{"card_image_alt_text":"A representative figure from paper main.728","card_image_path":"static/images/papers/main.728.png","content":{"abstract":"Visual Dialogue involves \"understanding'' the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don't, achieving state-of-the-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63%.","authors":["Shubham Agarwal","Trung Bui","Joon-Young Lee","Ioannis Konstas","Verena Rieser"],"demo_url":"","keywords":["Visual Dialogue","Visual Dialog","co-attention models","crowdsourcing procedure"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.728.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.728","similar_paper_uids":["main.728","main.19","main.463","srw.53","main.54"],"title":"History for Visual Dialog: Do we really need it?","tldr":"Visual Dialogue involves \"understanding'' the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this pape...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.728","id":"main.728","presentation_id":"38928892"},{"card_image_alt_text":"A representative figure from paper main.643","card_image_path":"static/images/papers/main.643.png","content":{"abstract":"We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. Our novel approach uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in the scene context. The input module of the MN-GMN generates a set of visual features plus a set of region-grounded captions (RGCs) for the image. The RGCs capture object attributes and their relationships. Two GNs are constructed from the input module using visual features and RGCs. Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. To combine the information from both GNs, each node writes the updated representations to an external spatial memory. The final states of the memory cells are fed into an answer module to predict an answer. Experiments show that MN-GMN rivals the state-of-the-art models on the Visual7W, VQA-v2.0, and CLEVR datasets.","authors":["Mahmoud Khademi"],"demo_url":"","keywords":["Multimodal Networks","Visual Answering","neural architecture","Multimodal Networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.643.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.643","similar_paper_uids":["main.643","main.306","main.642","main.683","main.664"],"title":"Multimodal Neural Graph Memory Networks for Visual Question Answering","tldr":"We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. Our novel approach uses graph structure with different region features as node attributes and applies a recently proposed...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.643","id":"main.643","presentation_id":"38929347"},{"card_image_alt_text":"A representative figure from paper main.642","card_image_path":"static/images/papers/main.642.png","content":{"abstract":"Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-the-art approaches.","authors":["Qingbao Huang","Jielong Wei","Yi Cai","Changmeng Zheng","Junying Chen","Ho-fung Leung","Qing Li"],"demo_url":"","keywords":["Visual Answering","image representations","question representations","Aligned Network"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.642.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.642","similar_paper_uids":["main.642","main.583","main.643","main.640","main.664"],"title":"Aligned Dual Channel Graph Convolutional Network for Visual Question Answering","tldr":"Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.642","id":"main.642","presentation_id":"38928979"},{"card_image_alt_text":"A representative figure from paper main.683","card_image_path":"static/images/papers/main.683.png","content":{"abstract":"This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR). We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space. In addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image. Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results. The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task.","authors":["Chen Zheng","Quan Guo","Parisa Kordjamshidi"],"demo_url":"","keywords":["Cross-Modality Relevance","Language Vision","visual answering","VQA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.683.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.683","similar_paper_uids":["main.683","main.306","main.643","main.664","main.642"],"title":"Cross-Modality Relevance for Reasoning on Language and Vision","tldr":"This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR). We design a novel cross-modality rel...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.683","id":"main.683","presentation_id":"38929203"},{"card_image_alt_text":"A representative figure from paper main.682","card_image_path":"static/images/papers/main.682.png","content":{"abstract":"Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu. We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning. By using diagnostic classifiers, we show that current models' learned representations are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).","authors":["Alessandro Suglia","Ioannis Konstas","Andrea Vanzo","Emanuele Bastianelli","Desmond Elliott","Stella Frank","Oliver Lemon"],"demo_url":"","keywords":["Grounded Learning","Goal-oriented evaluation","Object evaluation","Zero-shot evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.682.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.682","similar_paper_uids":["main.682","cl.1482","main.643","main.62","main.219"],"title":"CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning","tldr":"Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or g...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.682","id":"main.682","presentation_id":"38929114"},{"card_image_alt_text":"A representative figure from paper main.731","card_image_path":"static/images/papers/main.731.png","content":{"abstract":"Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.","authors":["Po-Yao Huang","Junjie Hu","Xiaojun Chang","Alexander Hauptmann"],"demo_url":"","keywords":["Unsupervised Translation","Unsupervised MT","MT","alignment"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.731.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.731","similar_paper_uids":["main.731","main.664","main.306","main.400","main.683"],"title":"Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting","tldr":"Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.731","id":"main.731","presentation_id":"38928946"},{"card_image_alt_text":"A representative figure from paper main.730","card_image_path":"static/images/papers/main.730.png","content":{"abstract":"We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.","authors":["Jie Lei","Licheng Yu","Tamara Berg","Mohit Bansal"],"demo_url":"","keywords":["Spatio-Temporal Grounding","Video Answering","Spatio-Temporal Answering","Spatio-Temporal Evidence"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.730.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.730","similar_paper_uids":["main.730","main.435","main.135","main.600","main.643"],"title":"TVQA+: Spatio-Temporal Grounding for Video Question Answering","tldr":"We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about vide...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.730","id":"main.730","presentation_id":"38929082"},{"card_image_alt_text":"A representative figure from paper main.644","card_image_path":"static/images/papers/main.644.png","content":{"abstract":"We propose a novel large-scale referring expression recognition dataset, Refer360\u00b0, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360\u00b0 scenes. Refer360\u00b0 differs from existing related datasets in three ways. First, we propose a more realistic scenario where instructors and the followers have partial, yet dynamic, views of the scene \u2013 followers continuously modify their field-of-view (FoV) while interpreting instructions that specify a final target location. Second, instructions to find the target location consist of multiple steps for followers who will start at random FoVs. As a result, intermediate instructions are strongly grounded in object references, and followers must identify intermediate FoVs to find the final target location correctly. Third, the target locations are neither restricted to predefined objects nor chosen by annotators; instead, they are distributed randomly across scenes. This \u201cpoint anywhere\u201d approach leads to more linguistically complex instructions, as shown in our analyses. Our examination of the dataset shows that Refer360\u00b0 manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language, vision, and navigation.","authors":["Volkan Cirik","Taylor Berg-Kirkpatrick","Louis-Philippe Morency"],"demo_url":"","keywords":["linguistically phenomena","language task","computational language","vision"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.644.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.644","similar_paper_uids":["main.644","main.729","main.440","main.229","tacl.1843"],"title":"Refer360\u00b0: A Referring Expression Recognition Dataset in 360\u00b0 Images","tldr":"We propose a novel large-scale referring expression recognition dataset, Refer360\u00b0, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360\u00b0 scenes. Refer360\u00b0 differs from existing related datasets...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.644","id":"main.644","presentation_id":"38929256"},{"card_image_alt_text":"A representative figure from paper main.685","card_image_path":"static/images/papers/main.685.png","content":{"abstract":"We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.","authors":["Angeliki Lazaridou","Anna Potapenko","Olivier Tieleman"],"demo_url":"","keywords":["Multi-agent Communication","natural learning","visual task","Functional Learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.685.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.685","similar_paper_uids":["main.685","main.328","main.625","main.729","main.190"],"title":"Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning","tldr":"We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language mode...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.685","id":"main.685","presentation_id":"38929310"},{"card_image_alt_text":"A representative figure from paper main.727","card_image_path":"static/images/papers/main.727.png","content":{"abstract":"Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.","authors":["Robik Shrestha","Kushal Kafle","Christopher Kanan"],"demo_url":"","keywords":["VQA","visual grounding","VQA-CPv2","negative methods"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.727.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.727","similar_paper_uids":["main.727","main.731","main.306","main.770","main.586"],"title":"A negative case analysis of visual grounding methods for VQA","tldr":"Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.727","id":"main.727","presentation_id":"38929240"},{"card_image_alt_text":"A representative figure from paper main.684","card_image_path":"static/images/papers/main.684.png","content":{"abstract":"We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms. We present a dataset of explanations paired with demonstrations for web-based tasks. Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment. In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior. This is an interesting example of pragmatic language acquisition without any linguistic annotation.","authors":["Shashank Srivastava","Oleksandr Polozov","Nebojsa Jojic","Christopher Meek"],"demo_url":"","keywords":["learning tasks","semantic parsing","mapping explanations","web-based tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.684.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.684","similar_paper_uids":["main.684","main.382","main.771","main.566","main.46"],"title":"Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context","tldr":"We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.684","id":"main.684","presentation_id":"38929361"},{"card_image_alt_text":"A representative figure from paper main.436","card_image_path":"static/images/papers/main.436.png","content":{"abstract":"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.","authors":["Jesse Mu","Percy Liang","Noah Goodman"],"demo_url":"","keywords":["Few-Shot Classification","human learning","supervision","machine models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.436.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.436","similar_paper_uids":["main.436","main.18","main.128","main.731","main.12"],"title":"Shaping Visual Representations with Language for Few-Shot Classification","tldr":"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.436","id":"main.436","presentation_id":"38929250"},{"card_image_alt_text":"A representative figure from paper main.230","card_image_path":"static/images/papers/main.230.png","content":{"abstract":"We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","authors":["Manling Li","Alireza Zareian","Qi Zeng","Spencer Whitehead","Di Lu","Heng Ji","Shih-Fu Chang"],"demo_url":"","keywords":["Multimedia Extraction","text labeling","visual extraction","argument labeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.230.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.230","similar_paper_uids":["main.230","demo.94","main.14","main.136","main.683"],"title":"Cross-media Structured Common Space for Multimedia Event Extraction","tldr":"We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events a...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.230","id":"main.230","presentation_id":"38928686"},{"card_image_alt_text":"A representative figure from paper main.231","card_image_path":"static/images/papers/main.231.png","content":{"abstract":"We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.","authors":["Daniel Fried","Jean-Baptiste Alayrac","Phil Blunsom","Chris Dyer","Stephen Clark","Aida Nematzadeh"],"demo_url":"","keywords":["action video","unsupervised settings","generative structure","Observation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.231.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.231","similar_paper_uids":["main.231","main.482","srw.28","main.707","main.29"],"title":"Learning to Segment Actions from Observation and Narration","tldr":"We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.231","id":"main.231","presentation_id":"38929315"},{"card_image_alt_text":"A representative figure from paper main.435","card_image_path":"static/images/papers/main.435.png","content":{"abstract":"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.","authors":["Hyounghun Kim","Zineng Tang","Mohit Bansal"],"demo_url":"","keywords":["Dense-Caption Matching","Temporal VideoQA","answering questions","frame problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.435.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.435","similar_paper_uids":["main.435","main.730","main.664","main.83","main.570"],"title":"Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA","tldr":"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information f...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.435","id":"main.435","presentation_id":"38929110"},{"card_image_alt_text":"A representative figure from paper main.233","card_image_path":"static/images/papers/main.233.png","content":{"abstract":"Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.","authors":["Jie Lei","Liwei Wang","Yelong Shen","Dong Yu","Tamara Berg","Mohit Bansal"],"demo_url":"","keywords":["Coherent Captioning","Generating descriptions","captioning tasks","coherent generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.233.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.233","similar_paper_uids":["main.233","main.481","main.643","main.683","main.61"],"title":"MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning","tldr":"Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.233","id":"main.233","presentation_id":"38929078"},{"card_image_alt_text":"A representative figure from paper main.583","card_image_path":"static/images/papers/main.583.png","content":{"abstract":"We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image--caption coherence relations, we annotate 10,000 instances from publicly-available image--caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.","authors":["Malihe Alikhani","Piyush Sharma","Shengjie Li","Radu Soricut","Matthew Stone"],"demo_url":"","keywords":["Caption Generation","image captioning","coherence prediction","Cross-modal Modeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.583.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.583","similar_paper_uids":["main.583","main.642","main.664","srw.22","srw.53"],"title":"Cross-modal Coherence Modeling for Caption Generation","tldr":"We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image--caption coherence relations, we annotate 10,...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.583","id":"main.583","presentation_id":"38929201"},{"card_image_alt_text":"A representative figure from paper main.232","card_image_path":"static/images/papers/main.232.png","content":{"abstract":"The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B's past actions as well as B's perspective leads to a significant improvement in performance on this challenging language understanding problem.","authors":["Prashant Jayannavar","Anjali Narayan-Chen","Julia Hockenmaier"],"demo_url":"","keywords":["Minecraft Task","predicting sequences","language problem","Builder"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.232.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.232","similar_paper_uids":["main.232","main.729","main.463","main.427","main.340"],"title":"Learning to execute instructions in a Minecraft dialogue","tldr":"The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (bl...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.232","id":"main.232","presentation_id":"38929058"},{"card_image_alt_text":"A representative figure from paper main.586","card_image_path":"static/images/papers/main.586.png","content":{"abstract":"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn\u2019t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.","authors":["Arjun Akula","Spandana Gella","Yaser Al-Onaizan","Song-Chun Zhu","Siva Reddy"],"demo_url":"","keywords":["Robustness Expressions","Grounding Expressions","Visual recognition","natural understanding"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.586.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.586","similar_paper_uids":["main.586","main.135","main.115","main.522","main.769"],"title":"Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions","tldr":"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.586","id":"main.586","presentation_id":"38929224"},{"card_image_alt_text":"A representative figure from paper main.584","card_image_path":"static/images/papers/main.584.png","content":{"abstract":"In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts. We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects. Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion. We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects. Our models also achieve promising results when tested on new object categories not seen during training.","authors":["Simeon Sch\u00fcz","Sina Zarrie\u00df"],"demo_url":"","keywords":["Knowledge Grounding","human cognition","visual terms","bottom-up baseline"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.584.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.584","similar_paper_uids":["main.584","main.727","main.643","main.306","main.436"],"title":"Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms","tldr":"In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts. We go beyond previous studies on colour terms using isolated colour swatches and...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.584","id":"main.584","presentation_id":"38929097"},{"card_image_alt_text":"A representative figure from paper main.585","card_image_path":"static/images/papers/main.585.png","content":{"abstract":"Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"demo_url":"","keywords":["Natural Localization","NLVL","ranking task","regression task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.585.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.585","similar_paper_uids":["main.585","main.413","main.500","main.622","tacl.1853"],"title":"Span-based Localizing Network for Natural Language Video Localization","tldr":"Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimo...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.585","id":"main.585","presentation_id":"38929045"},{"card_image_alt_text":"A representative figure from paper main.234","card_image_path":"static/images/papers/main.234.png","content":{"abstract":"Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning.","authors":["Noriyuki Kojima","Hadar Averbuch-Elor","Alexander Rush","Yoav Artzi"],"demo_url":"","keywords":["Visually Acquisition","bootstrap models","blackbox models","visual components"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.234.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.234","similar_paper_uids":["main.234","main.409","main.495","main.463","main.46"],"title":"What is Learned in Visually Grounded Neural Syntax Acquisition","tldr":"Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually ...","track":"Language Grounding to Vision, Robotics and Beyond"},"forum":"main.234","id":"main.234","presentation_id":"38929126"}]
