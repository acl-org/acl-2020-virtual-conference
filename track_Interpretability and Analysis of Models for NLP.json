[{"card_image_alt_text":"A representative figure from paper main.429","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.429.png","content":{"abstract":"Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.","authors":["Yiyun Zhao","Steven Bethard"],"demo_url":"","keywords":["downstream task","NLP problems","knowledge-related tasks","downstream tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.429.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.429","similar_paper_uids":["main.429","main.311","main.383","main.743","main.212"],"title":"How does BERT's attention change when you fine-tune? An analysis methodology and a case study in negation scope","tldr":"Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. W...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.429","id":"main.429","presentation_id":"38928830"},{"card_image_alt_text":"A representative figure from paper main.407","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.407.png","content":{"abstract":"Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.","authors":["Rahma Chaabouni","Eugene Kharitonov","Diane Bouchacourt","Emmanuel Dupoux","Marco Baroni"],"demo_url":"","keywords":["generalization","language transmission","representation learning","Compositionality"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.407.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.407","similar_paper_uids":["main.407","srw.28","main.229","main.177","main.644"],"title":"Compositionality and Generalization In Emergent Languages","tldr":"Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-age...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.407","id":"main.407","presentation_id":"38928781"},{"card_image_alt_text":"A representative figure from paper main.312","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.312.png","content":{"abstract":"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token\u2019s significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.","authors":["Xiaobing Sun","Wei Lu"],"demo_url":"","keywords":["Text Classification","natural tasks","NLP tasks","gradient process"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.312.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.312","similar_paper_uids":["main.312","main.432","main.385","main.419","main.687"],"title":"Understanding Attention for Text Classification","tldr":"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local at...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.312","id":"main.312","presentation_id":"38929360"},{"card_image_alt_text":"A representative figure from paper main.311","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.311.png","content":{"abstract":"Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.","authors":["Jae-young Jo","Sung-Hyon Myaeng"],"demo_url":"","keywords":["Transformer-based Models","natural tasks","downstream tasks","probing tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.311.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.311","similar_paper_uids":["main.311","main.429","srw.115","main.197","main.687"],"title":"Roles and Utilization of Attention Heads in Transformer-based Neural Language Models","tldr":"Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-h...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.311","id":"main.311","presentation_id":"38928695"},{"card_image_alt_text":"A representative figure from paper main.489","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.489.png","content":{"abstract":"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.","authors":["Zhiqing Sun","Shikhar Vashishth","Soumya Sanyal","Partha Talukdar","Yiming Yang"],"demo_url":"","keywords":["large-scale graphs","data mining","machine learning","natural processing"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.489.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.489","similar_paper_uids":["main.489","main.664","main.572","main.412","main.449"],"title":"A Re-evaluation of Knowledge Graph Completion Methods","tldr":"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.489","id":"main.489","presentation_id":"38929034"},{"card_image_alt_text":"A representative figure from paper main.310","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.310.png","content":{"abstract":"We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Speci\ufb01cally, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results con\ufb01rm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We \ufb01nd that \ufb01xed contextual encoders with a simple classi\ufb01er trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and speci\ufb01c tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.","authors":["Fan Yin","Quanyu Long","Tao Meng","Kai-Wei Chang"],"demo_url":"","keywords":["downstream applications","linguistic task","Language Encoders","pre-trained encoders"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.310.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.310","similar_paper_uids":["main.310","main.429","main.467","main.195","main.263"],"title":"On the Robustness of Language Encoders against Grammatical Errors","tldr":"We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Speci\ufb01cally, we collect real grammatical errors from non-native speakers and conduct adv...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.310","id":"main.310","presentation_id":"38929308"},{"card_image_alt_text":"A representative figure from paper main.496","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.496.png","content":{"abstract":"Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.","authors":["Kyle Swanson","Lili Yu","Tao Lei"],"demo_url":"","keywords":["Rationalizing Matching","text matching","downstream prediction","constrained problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.496.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.496","similar_paper_uids":["main.496","main.84","main.593","main.747","main.299"],"title":"Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport","tldr":"Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, su...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.496","id":"main.496","presentation_id":"38929403"},{"card_image_alt_text":"A representative figure from paper main.495","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.495.png","content":{"abstract":"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.","authors":["Sanjay Subramanian","Ben Bogin","Nitish Gupta","Tomer Wolfson","Sameer Singh","Jonathan Berant","Matt Gardner"],"demo_url":"","keywords":["vision","abstract process","Compositional Networks","Neural networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.495.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.495","similar_paper_uids":["main.495","main.497","main.409","main.387","main.539"],"title":"Obtaining Faithful Interpretations from Compositional Neural Networks","tldr":"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. H...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.495","id":"main.495","presentation_id":"38929088"},{"card_image_alt_text":"A representative figure from paper main.494","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.494.png","content":{"abstract":"Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.","authors":["Hanjie Chen","Guangtao Zheng","Yangfeng Ji"],"demo_url":"","keywords":["Text Classification","Generating explanations","natural processing","model prediction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.494.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.494","similar_paper_uids":["main.494","main.771","main.491","main.382","main.79"],"title":"Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection","tldr":"Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words o...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.494","id":"main.494","presentation_id":"38929188"},{"card_image_alt_text":"A representative figure from paper main.309","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.309.png","content":{"abstract":"We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model's robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.","authors":["Hiroshi Noji","Hiroya Takamura"],"demo_url":"","keywords":["resolving agreement","Augmentation","Augmentation sentences","Syntactic Models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.309.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.309","similar_paper_uids":["main.309","main.212","main.179","main.769","srw.105"],"title":"An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models","tldr":"We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive exampl...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.309","id":"main.309","presentation_id":"38929448"},{"card_image_alt_text":"A representative figure from paper main.490","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.490.png","content":{"abstract":"A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models' ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.","authors":["Aaron Mueller","Garrett Nicolai","Panayiota Petrou-Zeniou","Natalia Talmina","Tal Linzen"],"demo_url":"","keywords":["Cross-Linguistic Syntax","Syntax","Cross-Linguistic Models","neural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.490.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.490","similar_paper_uids":["main.490","main.493","main.329","main.421","main.304"],"title":"Cross-Linguistic Syntactic Evaluation of Word Prediction Models","tldr":"A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how thes...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.490","id":"main.490","presentation_id":"38929095"},{"card_image_alt_text":"A representative figure from paper main.491","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.491.png","content":{"abstract":"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.","authors":["Peter Hase","Mohit Bansal"],"demo_url":"","keywords":["Evaluating AI","Explainable AI","Algorithmic approaches","machine models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.491.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.491","similar_paper_uids":["main.491","main.771","main.494","main.382","main.35"],"title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?","tldr":"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretab...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.491","id":"main.491","presentation_id":"38929036"},{"card_image_alt_text":"A representative figure from paper main.493","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.493.png","content":{"abstract":"Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.","authors":["Ethan A. Chi","John Hewitt","Christopher D. Manning"],"demo_url":"","keywords":["zero-shot transfer","Multilingual BERT","Multilingual mBERT","Multilingual"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.493.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.493","similar_paper_uids":["main.493","main.421","main.536","main.260","main.747"],"title":"Finding Universal Grammatical Relations in Multilingual BERT","tldr":"Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.493","id":"main.493","presentation_id":"38929418"},{"card_image_alt_text":"A representative figure from paper main.492","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.492.png","content":{"abstract":"Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which `saliency maps' may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.","authors":["Xiaochuang Han","Byron C. Wallace","Yulia Tsvetkov"],"demo_url":"","keywords":["NLP","model prediction","model decisions","natural inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.492.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.492","similar_paper_uids":["main.492","main.534","main.79","main.467","main.96"],"title":"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions","tldr":"Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide e...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.492","id":"main.492","presentation_id":"38929233"},{"card_image_alt_text":"A representative figure from paper main.422","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.422.png","content":{"abstract":"This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.","authors":["John Wu","Yonatan Belinkov","Hassan Sajjad","Nadir Durrani","Fahim Dalvi","James Glass"],"demo_url":"","keywords":["information localization","downstream tasks","Similarity Models","contextual models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.422.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.422","similar_paper_uids":["main.422","cl.1552","cl.1482","demo.84","main.431"],"title":"Similarity Analysis of Contextual Word Representation Models","tldr":"This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from va...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.422","id":"main.422","presentation_id":"38928932"},{"card_image_alt_text":"A representative figure from paper main.387","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.387.png","content":{"abstract":"Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model's predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model's prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model's predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model's predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model's predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model's predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model's predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model's predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention","authors":["Akash Kumar Mohankumar","Preksha Nema","Sharan Narasimhan","Mitesh M. Khapra","Balaji Vasan Srinivasan","Balaraman Ravindran"],"demo_url":"","keywords":["interpretability distributions","attention mechanisms","Human evaluations","Transparent Models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.387.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.387","similar_paper_uids":["main.387","main.432","main.687","main.419","main.385"],"title":"Towards Transparent and Explainable Attention Models","tldr":"Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model's predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.387","id":"main.387","presentation_id":"38929301"},{"card_image_alt_text":"A representative figure from paper main.386","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.386.png","content":{"abstract":"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is \"defined\" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.","authors":["Alon Jacovi","Yoav Goldberg"],"demo_url":"","keywords":["Faithfully Systems","interpretable systems","interpretability research","faithfulness evaluation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.386.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.386","similar_paper_uids":["main.386","main.454","main.771","main.101","main.409"],"title":"Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?","tldr":"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpre...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.386","id":"main.386","presentation_id":"38929099"},{"card_image_alt_text":"A representative figure from paper main.409","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.409.png","content":{"abstract":"In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text 'responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to 'end-to-end' approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.","authors":["Sarthak Jain","Sarah Wiegreffe","Yuval Pinter","Byron C. Wallace"],"demo_url":"","keywords":["NLP","neural classification","training","automatic evaluations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.409.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.409","similar_paper_uids":["main.409","main.408","main.771","main.449","main.387"],"title":"Learning to Faithfully Rationalize by Construction","tldr":"In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text 'responsible for' corresponding model output; when such a snippet comprises tok...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.409","id":"main.409","presentation_id":"38929220"},{"card_image_alt_text":"A representative figure from paper main.421","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.421.png","content":{"abstract":"State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.","authors":["Mikel Artetxe","Sebastian Ruder","Dani Yogatama"],"demo_url":"","keywords":["zero-shot setting","Cross-lingual Representations","unsupervised models","joint training"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.421.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.421","similar_paper_uids":["main.421","main.747","main.329","main.493","main.554"],"title":"On the Cross-lingual Transferability of Monolingual Representations","tldr":"State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint traini...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.421","id":"main.421","presentation_id":"38929158"},{"card_image_alt_text":"A representative figure from paper main.384","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.384.png","content":{"abstract":"Language models keep track of complex information about the preceding context -- including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.","authors":["Ionut-Teodor Sorodoc","Kristina Gulordava","Gemma Boleda"],"demo_url":"","keywords":["Probing","probe tasks","Language Models","LSTM architectures"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.384.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.384","similar_paper_uids":["main.384","main.430","srw.127","main.158","main.375"],"title":"Probing for Referential Information in Language Models","tldr":"Language models keep track of complex information about the preceding context -- including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We anal...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.384","id":"main.384","presentation_id":"38929145"},{"card_image_alt_text":"A representative figure from paper main.385","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.385.png","content":{"abstract":"In the Transformer model, \u201cself-attention\u201d combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.","authors":["Samira Abnar","Willem Zuidema"],"demo_url":"","keywords":["Quantifying Transformers","quantifying information","Attention Transformers","Transformer model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.385.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.385","similar_paper_uids":["main.385","srw.115","main.687","main.432","main.312"],"title":"Quantifying Attention Flow in Transformers","tldr":"In the Transformer model, \u201cself-attention\u201d combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets incr...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.385","id":"main.385","presentation_id":"38928943"},{"card_image_alt_text":"A representative figure from paper main.434","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.434.png","content":{"abstract":"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability---but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.","authors":["Josef Klafka","Allyson Ettinger"],"demo_url":"","keywords":["Fine-grained Embeddings","NLP tasks","probing tasks","encoding information"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.434.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.434","similar_paper_uids":["main.434","main.236","main.612","main.431","main.156"],"title":"Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words","tldr":"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To add...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.434","id":"main.434","presentation_id":"38929325"},{"card_image_alt_text":"A representative figure from paper main.420","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.420.png","content":{"abstract":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually ``know'' about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research---plus English---totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.","authors":["Tiago Pimentel","Josef Valvoda","Rowan Hall Maudslay","Ran Zmigrod","Adina Williams","Ryan Cotterell"],"demo_url":"","keywords":["Information-Theoretic Probing","NLP tasks","linguistic task","probing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.420.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.420","similar_paper_uids":["main.420","cl.1547","main.383","main.408","main.115"],"title":"Information-Theoretic Probing for Linguistic Structure","tldr":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually ``know'' about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.420","id":"main.420","presentation_id":"38928922"},{"card_image_alt_text":"A representative figure from paper main.408","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.408.png","content":{"abstract":"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the `reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER\\, a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of ``rationales'' (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/","authors":["Jay DeYoung","Sarthak Jain","Nazneen Fatema Rajani","Eric Lehman","Caiming Xiong","Richard Socher","Byron C. Wallace"],"demo_url":"","keywords":["NLP","Evaluating Reasoning","ERASER","Rationalized Models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.408.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.408","similar_paper_uids":["main.408","main.409","main.764","main.115","main.387"],"title":"ERASER: A Benchmark to Evaluate Rationalized NLP Models","tldr":"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal t...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.408","id":"main.408","presentation_id":"38928900"},{"card_image_alt_text":"A representative figure from paper main.430","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.430.png","content":{"abstract":"LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject\u2019s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM\u2019s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.","authors":["Kaiji Lu","Piotr Mardziel","Klas Leino","Matt Fredrikson","Anupam Datta"],"demo_url":"","keywords":["natural tasks","LSTM Models","LSTM-based networks","LSTMs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.430.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.430","similar_paper_uids":["main.430","srw.127","main.384","main.490","main.257"],"title":"Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models","tldr":"LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.430","id":"main.430","presentation_id":"38929378"},{"card_image_alt_text":"A representative figure from paper main.381","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.381.png","content":{"abstract":"Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. As a step in this direction we study the case of representations of phonology in neural network models of spoken language. We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. We manipulate two factors that can affect the outcome of analysis. First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models. Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance. We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.","authors":["Grzegorz Chrupa\u0142a","Bertrand Higy","Afra Alishahi"],"demo_url":"","keywords":["NLP systems","learning","reporting analysis","neural language"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.381.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.381","similar_paper_uids":["main.381","main.422","main.269","demo.84","main.365"],"title":"Analyzing analytical methods: The case of phonology in neural models of spoken language","tldr":"Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. As a step in this direction we study the case of representatio...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.381","id":"main.381","presentation_id":"38928815"},{"card_image_alt_text":"A representative figure from paper main.419","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.419.png","content":{"abstract":"Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention. In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification. Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps. Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity. Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.","authors":["Cansu Sen","Thomas Hartvigsen","Biao Yin","Xiangnan Kong","Elke Rundensteiner"],"demo_url":"","keywords":["Text Classification","quantitative mechanisms","text task","large-scale study"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.419.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.419","similar_paper_uids":["main.419","main.432","main.312","main.385","main.387"],"title":"Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?","tldr":"Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. While attention mechanisms are claimed to achieve interpretability, little is known ab...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.419","id":"main.419","presentation_id":"38929024"},{"card_image_alt_text":"A representative figure from paper main.431","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.431.png","content":{"abstract":"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings --- while more diverse and mature than those available for their dynamic counterparts --- are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.","authors":["Rishi Bommasani","Kelly Davis","Claire Cardie"],"demo_url":"","keywords":["Interpreting Representations","downstream applications","static embeddings","Pretrained Representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.431.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.431","similar_paper_uids":["main.431","main.405","main.236","main.488","main.260"],"title":"Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings","tldr":"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. A...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.431","id":"main.431","presentation_id":"38929398"},{"card_image_alt_text":"A representative figure from paper main.433","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.433.png","content":{"abstract":"We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional.","authors":["Nur Geffen Lan","Emmanuel Chemla","Shane Steinert-Threlkeld"],"demo_url":"","keywords":["language emergence","signaling games","neural agents","backpropagation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.433.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.433","similar_paper_uids":["main.433","main.497","srw.114","main.432","main.495"],"title":"On the Spontaneous Emergence of Discrete and Compositional Signals","tldr":"We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.433","id":"main.433","presentation_id":"38929275"},{"card_image_alt_text":"A representative figure from paper main.382","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.382.png","content":{"abstract":"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ''Because there is a dog in the image.'' and ''Because there is no dog in the [same] image.'', exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","authors":["Oana-Maria Camburu","Brendan Shillingford","Pasquale Minervini","Thomas Lukasiewicz","Phil Blunsom"],"demo_url":"","keywords":["Adversarial Explanations","artificial systems","generation explanations","sanity models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.382.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.382","similar_paper_uids":["main.382","main.771","srw.105","main.463","main.494"],"title":"Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations","tldr":"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.382","id":"main.382","presentation_id":"38928826"},{"card_image_alt_text":"A representative figure from paper main.383","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.383.png","content":{"abstract":"By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.","authors":["Zhiyong Wu","Yun Chen","Ben Kao","Qun Liu"],"demo_url":"","keywords":["Analyzing BERT","linguistic tasks","dependency parsing","probing tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.383.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.383","similar_paper_uids":["main.383","main.420","main.263","main.431","main.140"],"title":"Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT","tldr":"By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probin...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.383","id":"main.383","presentation_id":"38929032"},{"card_image_alt_text":"A representative figure from paper main.432","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.432.png","content":{"abstract":"Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention\u2019s reliability as a tool for auditing algorithms in the context of fairness and accountability.","authors":["Danish Pruthi","Mansi Gupta","Bhuwan Dhingra","Graham Neubig","Zachary C. Lipton"],"demo_url":"","keywords":["natural processing","Attention mechanisms","neural architectures","human study"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.432.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.432","similar_paper_uids":["main.432","main.312","main.419","main.387","main.385"],"title":"Learning to Deceive with Attention-Based Explanations","tldr":"Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful bo...","track":"Interpretability and Analysis of Models for NLP"},"forum":"main.432","id":"main.432","presentation_id":"38929279"},{"card_image_alt_text":"A representative figure from paper tacl.1779","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1779.png","content":{"abstract":"Data privacy is an important issue for \"machine learning as a service\" providers. We focus on the problem of membership inference attacks: given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks. ","authors":["Sorami Hisamoto","Matt Post","Kevin Duh"],"demo_url":"","keywords":["Machine System","Data privacy","machine learning","membership attacks"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00299","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00299","similar_paper_uids":["tacl.1779","main.768","main.540","main.479","main.251"],"title":"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?","tldr":"Data privacy is an important issue for \"machine learning as a service\" providers. We focus on the problem of membership inference attacks: given a data sample and black-box access to a model's API, determine whether the sample existed in the model's ...","track":"Interpretability and Analysis of Models for NLP"},"forum":"tacl.1779","id":"tacl.1779","presentation_id":"38929491"},{"card_image_alt_text":"A representative figure from paper tacl.1709","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1709.png","content":{"abstract":"Recurrent neural networks (RNNs) reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover and store any useful linguistic unit from scratch, based on input statistics. The results show that our \"near tabula rasa\" RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned to track \"soft\" word boundaries. Our study opens the door to speculations about the necessity of an explicit word lexicon in language learning and usage.","authors":["Michael Hahn","Marco Baroni"],"demo_url":"","keywords":["natural tasks","morphological tasks","language usage","Tabula"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00283","sessions":[{"end_time":"Tue, 07 Jul 2020 07:00:00 GMT","session_name":"6B","start_time":"Tue, 07 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00283","similar_paper_uids":["tacl.1709","main.43","cl.1482","srw.9","srw.127"],"title":"Tabula nearly Rasa: Probing the linguistic knowledge of character-level neural language models trained on unsegmented text","tldr":"Recurrent neural networks (RNNs) reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current anal...","track":"Interpretability and Analysis of Models for NLP"},"forum":"tacl.1709","id":"tacl.1709","presentation_id":"38929484"},{"card_image_alt_text":"A representative figure from paper tacl.1852","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1852.png","content":{"abstract":"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction \u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation.","authors":["Allyson Ettinger"],"demo_url":"","keywords":["Pre-training","NLP tasks","inference","role-based prediction"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00298","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00298","similar_paper_uids":["tacl.1852","main.429","main.247","main.768","main.370"],"title":"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models","tldr":"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagn...","track":"Interpretability and Analysis of Models for NLP"},"forum":"tacl.1852","id":"tacl.1852","presentation_id":"38929501"},{"card_image_alt_text":"A representative figure from paper tacl.1892","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1892.png","content":{"abstract":"Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.","authors":["R. Thomas McCoy","Robert Frank","Tal Linzen"],"demo_url":"","keywords":["syntactic tasks","English formation","Sequence-to-Sequence Networks","neural models"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00304","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00304","similar_paper_uids":["tacl.1892","main.158","main.303","srw.28","main.375"],"title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks","tldr":"Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which archi...","track":"Interpretability and Analysis of Models for NLP"},"forum":"tacl.1892","id":"tacl.1892","presentation_id":"38929506"}]
