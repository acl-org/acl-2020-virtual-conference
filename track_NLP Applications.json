[{"card_image_alt_text":"A representative figure from paper main.761","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.761.png","content":{"abstract":"The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking \u2013 multiple propositions, temporal reasoning, and ambiguity and lexical variation \u2013 and introduce a resource with these types of claims. Then we present a system designed to be resilient to these \u201cattacks\u201d using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.","authors":["Christopher Hidey","Tuhin Chakrabarty","Tariq Alhindi","Siddharth Varia","Kriste Krstovski","Mona Diab","Smaranda Muresan"],"demo_url":"","keywords":["retrieving evidence","endto- fact-checking","veracity prediction","FEVER"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.761.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.761","similar_paper_uids":["main.761","main.549","main.655","main.97","main.656"],"title":"DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking","tldr":"The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for ev...","track":"NLP Applications"},"forum":"main.761","id":"main.761","presentation_id":"38928924"},{"card_image_alt_text":"A representative figure from paper main.205","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.205.png","content":{"abstract":"In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy's layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.","authors":["Kervy Rivas Rojas","Gina Bustamante","Arturo Oncevay","Marco Antonio Sobrevilla Cabezudo"],"demo_url":"","keywords":["Hierarchical Classification","External Tasks","sequence-to-sequence problem","auxiliary bottom-up-classification"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.205.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.205","similar_paper_uids":["main.205","main.104","main.208","main.426","main.103"],"title":"Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks","tldr":"In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to de...","track":"NLP Applications"},"forum":"main.205","id":"main.205","presentation_id":"38929395"},{"card_image_alt_text":"A representative figure from paper main.204","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.204.png","content":{"abstract":"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.","authors":["Ji Xin","Raphael Tang","Jaejun Lee","Yaoliang Yu","Jimmy Lin"],"demo_url":"","keywords":["Accelerating Inference","NLP applications","inference","real-time applications"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.204.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.204","similar_paper_uids":["main.204","main.195","main.411","main.76","main.537"],"title":"DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference","tldr":"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a...","track":"NLP Applications"},"forum":"main.204","id":"main.204","presentation_id":"38928742"},{"card_image_alt_text":"A representative figure from paper main.760","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.760.png","content":{"abstract":"In traditional approaches to entity linking, linking decisions are based on three sources of information -- the similarity of the mention string to an entity's name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB). In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity. We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology. We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al. 2018), which create a token representation that integrates the surrounding context of the mention and concept name. We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al. 2013). Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker.","authors":["Elliot Schumacher","Andriy Mulyar","Mark Dredze"],"demo_url":"","keywords":["Clinical Linking","entity linking","linking decisions","concept linking"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.760.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.760","similar_paper_uids":["main.760","main.748","main.612","tacl.1906","main.717"],"title":"Clinical Concept Linking with Contextualized Neural Representations","tldr":"In traditional approaches to entity linking, linking decisions are based on three sources of information -- the similarity of the mention string to an entity's name, the similarity of the context of the document to the entity, and broader information...","track":"NLP Applications"},"forum":"main.760","id":"main.760","presentation_id":"38929026"},{"card_image_alt_text":"A representative figure from paper main.79","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.79.png","content":{"abstract":"Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users. This methodology helps with providing meaningful explanations on prediction results. Furthermore, it also initiates an attempt to uncover the so-called \"black-box\" GNN-based models by investigating the effect of individual nodes.","authors":["Ting Zhong","Tianliang Wang","Fan Zhou","Goce Trajcevski","Kunpeng Zhang","Yi Yang"],"demo_url":"","keywords":["Interpreting Geolocation","Identifying geolocation","Identifying networks","user geolocation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.79.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.79","similar_paper_uids":["main.79","main.494","main.492","main.771","main.286"],"title":"Interpreting Twitter User Geolocation","tldr":"Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corres...","track":"NLP Applications"},"forum":"main.79","id":"main.79","presentation_id":"38929254"},{"card_image_alt_text":"A representative figure from paper main.762","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.762.png","content":{"abstract":"In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to. Compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document. We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing. Due to the subjective nature of the task, multiple fonts might be perceived as acceptable for an input text, which makes this problem challenging. To this end, we investigate different end-to-end models to learn label distributions on crowd-sourced data, to capture inter-subjectivity across all annotations.","authors":["Amirreza Shirani","Franck Dernoncourt","Jose Echevarria","Paul Asente","Nedim Lipka","Thamar Solorio"],"demo_url":"","keywords":["Font Selection","crowd-sourcing","end-to-end models","Verbal Context"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.762.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.762","similar_paper_uids":["main.762","main.306","main.643","main.731","main.683"],"title":"Let Me Choose: From Verbal Context to Font Selection","tldr":"In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to. Compared to related work leveraging the surrounding visual context, we choose to focus only on the input...","track":"NLP Applications"},"forum":"main.762","id":"main.762","presentation_id":"38929151"},{"card_image_alt_text":"A representative figure from paper main.206","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.206.png","content":{"abstract":"We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1). We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates. Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task. We discuss areas for improvement and potential applications for text-only speech scoring.","authors":["Hannah Craighead","Andrew Caines","Paula Buttery","Helen Yannakoudakis"],"demo_url":"","keywords":["automated transcriptions","automatically speech","multi-task learning","inductive transfer"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.206.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.206","similar_paper_uids":["main.206","main.351","main.216","main.505","main.1"],"title":"Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions","tldr":"We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. Motivated by recent advances in multi-task learning, we develop neural networks train...","track":"NLP Applications"},"forum":"main.206","id":"main.206","presentation_id":"38929142"},{"card_image_alt_text":"A representative figure from paper main.207","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.207.png","content":{"abstract":"Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.","authors":["Arman Cohan","Sergey Feldman","Iz Beltagy","Doug Downey","Daniel Weld"],"demo_url":"","keywords":["Document-level Learning","Representation learning","natural systems","classification"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.207.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.207","similar_paper_uids":["main.207","main.670","main.141","main.693","main.321"],"title":"SPECTER: Document-level Representation Learning using Citation-informed Transformers","tldr":"Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training ob...","track":"NLP Applications"},"forum":"main.207","id":"main.207","presentation_id":"38928763"},{"card_image_alt_text":"A representative figure from paper main.763","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.763.png","content":{"abstract":"News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation. In NLP, although recent works have studied framing in English news, few have studied how the analysis can be extended to other languages and in a multi-label setting. In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language. We propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language. Our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data. This work opens up an exciting new capability of scaling up frame analysis to many languages, even those without existing translation technologies. Lastly, we apply our method to detect frames on the issue of U.S. gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages.","authors":["Afra Feyza Aky\u00fcrek","Lei Guo","Randa Elanwar","Prakash Ishwar","Margrit Betke","Derry Tanti Wijaya"],"demo_url":"","keywords":["News framing","NLP","multi-label setting","U.S. violence"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.763.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.763","similar_paper_uids":["main.763","main.478","main.331","main.260","srw.18"],"title":"Multi-Label and Multilingual News Framing Analysis","tldr":"News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation. In NLP, although recent works have studied framing in English news, few have studied how the analysis can be e...","track":"NLP Applications"},"forum":"main.763","id":"main.763","presentation_id":"38929186"},{"card_image_alt_text":"A representative figure from paper main.78","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.78.png","content":{"abstract":"Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We empirically evaluate the framework on a real-world dataset. The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. SemiORC also outperforms other baseline methods on operational risk classification.","authors":["Fan Zhou","Shengming Zhang","Yi Yang"],"demo_url":"","keywords":["Interpretable Classification","Operational management","automatic prediction","Operational Classification"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.78.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.78","similar_paper_uids":["main.78","main.194","main.279","main.191","tacl.1801"],"title":"Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder","tldr":"Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled...","track":"NLP Applications"},"forum":"main.78","id":"main.78","presentation_id":"38929255"},{"card_image_alt_text":"A representative figure from paper main.767","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.767.png","content":{"abstract":"Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.","authors":["Sudipto Mukherjee","Subhabrata Mukherjee","Marcello Hasegawa","Ahmed Hassan Awadallah","Ryen White"],"demo_url":"","keywords":["Automatic Items","email applications","task management","automatically items"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.767.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.767","similar_paper_uids":["main.767","main.108","main.248","demo.96","main.217"],"title":"Smart To-Do: Automatic Generation of To-Do Items from Emails","tldr":"Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users wit...","track":"NLP Applications"},"forum":"main.767","id":"main.767","presentation_id":"38929191"},{"card_image_alt_text":"A representative figure from paper main.203","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.203.png","content":{"abstract":"Authorship attribution aims to identify the author of a text based on the stylometric analysis. Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text\u2019s style. In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model. An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated \u2013 a decision that is key to the adversary interested in authorship attribution. We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87. The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner. Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.","authors":["Asad Mahmood","Zubair Shafiq","Padmini Srinivasan"],"demo_url":"","keywords":["Detecting Obfuscation","Authorship attribution","Authorship obfuscation","stylometric analysis"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.203.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.203","similar_paper_uids":["main.203","srw.105","main.540","main.380","main.245"],"title":"A Girl Has A Name: Detecting Authorship Obfuscation","tldr":"Authorship attribution aims to identify the author of a text based on the stylometric analysis. Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text\u2019s style. In this paper, we evaluate the stea...","track":"NLP Applications"},"forum":"main.203","id":"main.203","presentation_id":"38929199"},{"card_image_alt_text":"A representative figure from paper main.766","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.766.png","content":{"abstract":"Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.","authors":["Antonios Anastasopoulos","Graham Neubig"],"demo_url":"","keywords":["cross-lingual embeddings","lexicon tagging","lexicon dictionaries","cross-lingual baselines"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.766.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.766","similar_paper_uids":["main.766","main.421","main.260","main.329","main.618"],"title":"Should All Cross-Lingual Embeddings Speak English?","tldr":"Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub ...","track":"NLP Applications"},"forum":"main.766","id":"main.766","presentation_id":"38928883"},{"card_image_alt_text":"A representative figure from paper main.82","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.82.png","content":{"abstract":"Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this paper. A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model. The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling. In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique. Our method of using `Soft-Masked BERT' is general, and it may be employed in other language detection-correction problems. Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.","authors":["Shaohua Zhang","Haoran Huang","Jicong Liu","Hang Li"],"demo_url":"","keywords":["Spelling Correction","Chinese correction","Chinese CSC","error detection"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.82.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.82","similar_paper_uids":["main.82","main.315","main.705","main.214","main.370"],"title":"Spelling Error Correction with Soft-Masked BERT","tldr":"Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this ...","track":"NLP Applications"},"forum":"main.82","id":"main.82","presentation_id":"38928853"},{"card_image_alt_text":"A representative figure from paper main.80","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.80.png","content":{"abstract":"Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.","authors":["Grandee Lee","Haizhou Li"],"demo_url":"","keywords":["Modeling Languages","bilingual induction","language tasks","Language modeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.80.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.80","similar_paper_uids":["main.80","main.510","main.554","main.329","main.645"],"title":"Modeling Code-Switch Languages Using Bilingual Parallel Corpus","tldr":"Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable train...","track":"NLP Applications"},"forum":"main.80","id":"main.80","presentation_id":"38928986"},{"card_image_alt_text":"A representative figure from paper main.764","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.764.png","content":{"abstract":"Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model. To do so, we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input. Experimenting on~9 different NLP tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts. %we represent experimental settings using an array of features. Going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings. (Code, data and logs are publicly available at https://github.com/xiamengzhou/NLPerf.)","authors":["Mengzhou Xia","Antonios Anastasopoulos","Ruochen Xu","Yiming Yang","Graham Neubig"],"demo_url":"","keywords":["Natural Tasks","natural research","NLP research","NLP tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.764.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.764","similar_paper_uids":["main.764","main.408","main.204","main.769","main.420"],"title":"Predicting Performance for Natural Language Processing Tasks","tldr":"Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. In this work, we ...","track":"NLP Applications"},"forum":"main.764","id":"main.764","presentation_id":"38929262"},{"card_image_alt_text":"A representative figure from paper main.758","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.758.png","content":{"abstract":"The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code--text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.","authors":["Rajarshi Haldar","Lingfei Wu","JinJun Xiong","Julia Hockenmaier"],"demo_url":"","keywords":["Semantic Search","code matching","monolingual matching","cross-lingual task"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.758.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.758","similar_paper_uids":["main.758","main.554","main.329","main.581","main.274"],"title":"A Multi-Perspective Architecture for Semantic Code Search","tldr":"The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual ...","track":"NLP Applications"},"forum":"main.758","id":"main.758","presentation_id":"38929341"},{"card_image_alt_text":"A representative figure from paper main.759","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.759.png","content":{"abstract":"While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision. However, a neural AES typically does not provide useful feature representations for supporting AWE. This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers. We evaluate performance using a feature-based AES requiring TCs. Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.","authors":["Haoran Zhang","Diane Litman"],"demo_url":"","keywords":["Automated Extraction","automated evaluation","essay revision","AWE"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.759.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.759","similar_paper_uids":["main.759","demo.139","main.312","main.697","demo.84"],"title":"Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring","tldr":"While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision. However, a neural AES typically does not provide useful feature representatio...","track":"NLP Applications"},"forum":"main.759","id":"main.759","presentation_id":"38929052"},{"card_image_alt_text":"A representative figure from paper main.765","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.765.png","content":{"abstract":"It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach. In dialogue systems, it would also be useful to drive dialogues by a dialogue plan. In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative. The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative. It keeps track of what in the narrative has been said and what is to be said. A narrative plays a different role than the context (i.e., previous utterances), which is generally used in current dialogue systems. Due to the unavailability of data for this new application, we construct a new large-scale data collection GraphMovie from a movie website where end- users can upload their narratives freely when watching a movie. Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context.","authors":["Yutao Zhu","Ruihua Song","Zhicheng Dou","Jian-Yun Nie","Jin Zhou"],"demo_url":"","keywords":["Narrative-Guided Generation","dialogue systems","ScriptWriter","model ScriptWriter"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.765.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.765","similar_paper_uids":["main.765","main.637","main.568","main.62","main.54"],"title":"ScriptWriter: Narrative-Guided Script Generation","tldr":"It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach. In dialogue systems, it would also be useful to drive dialogues by a dialogue plan. In this paper, we addre...","track":"NLP Applications"},"forum":"main.765","id":"main.765","presentation_id":"38928837"},{"card_image_alt_text":"A representative figure from paper main.81","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.81.png","content":{"abstract":"Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.","authors":["Xingyi Cheng","Weidi Xu","Kunlong Chen","Shaohua Jiang","Feng Wang","Taifeng Wang","Wei Chu","Yuan Qi"],"demo_url":"","keywords":["Chinese Check","spelling errors","spelling language","CSC"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.81.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.81","similar_paper_uids":["main.81","main.67","main.547","main.291","main.339"],"title":"SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check","tldr":"Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowle...","track":"NLP Applications"},"forum":"main.81","id":"main.81","presentation_id":"38928705"},{"card_image_alt_text":"A representative figure from paper main.674","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.674.png","content":{"abstract":"We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer's interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image holds a vital cue for this task. Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task. Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data. To enable BERT to process other visual information, we append image captions to the scene-text. This achieves an accuracy of 89.69%, which is an improvement of 4.7%. This is the best reported result for this task.","authors":["Kanika Kalra","Bhargav Kurma","Silpa Vadakkeeveetil Sreelatha","Manasi Patwardhan","Shirish Karande"],"demo_url":"","keywords":["Advertisements","advertisement understanding","sentence-pair task","BERT model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.674.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.674","similar_paper_uids":["main.674","main.93","main.664","main.583","main.642"],"title":"Understanding Advertisements with BERT","tldr":"We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer's interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image ...","track":"NLP Applications"},"forum":"main.674","id":"main.674","presentation_id":"38929293"},{"card_image_alt_text":"A representative figure from paper main.535","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.535.png","content":{"abstract":"Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.","authors":["Amirhossein Kazemnejad","Mohammadreza Salehi","Mahdieh Soleymani Baghshah"],"demo_url":"","keywords":["Paraphrase Generation","Neural sequence","sequence generation","retrieval-based method"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.535.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.535","similar_paper_uids":["main.535","main.22","main.28","main.545","srw.84"],"title":"Paraphrase Generation by Learning How to Edit from Samples","tldr":"Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems...","track":"NLP Applications"},"forum":"main.535","id":"main.535","presentation_id":"38928811"},{"card_image_alt_text":"A representative figure from paper main.284","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.284.png","content":{"abstract":"Technical support problems are often long and complex. They typically contain user descriptions of the problem, the setup, and steps for attempted resolution. Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces. These elements contain potentially crucial information for problem resolution. However, they cannot be correctly parsed by tools designed for natural language. In this paper, we address the problem of segmentation for technical support questions. We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches. We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach. We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model. Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.","authors":["Kushal Chauhan","Abhirut Gupta"],"demo_url":"","keywords":["Segmentation","Technical Problems","attempted resolution","problem resolution"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.284.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.284","similar_paper_uids":["main.284","main.434","tacl.1849","main.518","main.451"],"title":"Improving Segmentation for Technical Support Problems","tldr":"Technical support problems are often long and complex. They typically contain user descriptions of the problem, the setup, and steps for attempted resolution. Often they also contain various non-natural language text elements like outputs of commands...","track":"NLP Applications"},"forum":"main.284","id":"main.284","presentation_id":"38929176"},{"card_image_alt_text":"A representative figure from paper main.285","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.285.png","content":{"abstract":"The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http://moocdata.cn/data/MOOCCube.","authors":["Jifan Yu","Gan Luo","Tong Xiao","Qingyang Zhong","Yuquan Wang","Wenzheng Feng","Junyi Luo","Chenyu Wang","Lei Hou","Juanzi Li","Zhiyuan Liu","Jie Tang"],"demo_url":"","keywords":["NLP Applications","NLP MOOCs","NLP research","education applications"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.285.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.285","similar_paper_uids":["main.285","demo.93","demo.69","main.184","demo.115"],"title":"MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs","tldr":"The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are...","track":"NLP Applications"},"forum":"main.285","id":"main.285","presentation_id":"38928839"},{"card_image_alt_text":"A representative figure from paper main.534","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.534.png","content":{"abstract":"Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as Document In\ufb02uence Model (DIM) are based on dynamic topic models, which only consider the word frequency change. In this paper, we use both frequency changes and word semantic shifts to measure document in\ufb02uence by developing a neural network framework. Our model has three steps. Firstly, we train the word embeddings for different time periods. Subsequently, we propose an unsupervised method to align vectors for different time periods. Finally, we compute the in\ufb02uence value of documents. Our experimental results show that our model outperforms DIM.","authors":["Jie Tan","Changlin Yang","Ying Li","Siliang Tang","Chen Huang","Yueting Zhuang"],"demo_url":"","keywords":["Measuring Influence","Measuring impact","Neural-DINF","Neural Framework"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.534.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.534","similar_paper_uids":["main.534","main.492","main.31","tacl.2001","main.207"],"title":"Neural-DINF: A Neural Network based Framework for Measuring Document Influence","tldr":"Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as Document In\ufb02uence Model (DIM) are based on dynamic topic models, which only consider the word frequency change. In thi...","track":"NLP Applications"},"forum":"main.534","id":"main.534","presentation_id":"38929332"},{"card_image_alt_text":"A representative figure from paper main.286","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.286.png","content":{"abstract":"The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.","authors":["Jun Chen","Xiaoya Dai","Quan Yuan","Chao Lu","Haifeng Huang"],"demo_url":"","keywords":["Interpretable Diagnosis","automatic diagnosis","clinical use","accurate system"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.286.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.286","similar_paper_uids":["main.286","main.494","main.27","main.719","main.48"],"title":"Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs","tldr":"The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks...","track":"NLP Applications"},"forum":"main.286","id":"main.286","presentation_id":"38928728"},{"card_image_alt_text":"A representative figure from paper main.279","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.279.png","content":{"abstract":"We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A \u201cself-diversity\u201d criterion is proposed for measuring the \u201cworthiness\u201d of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task. To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.","authors":["Zhuoren Jiang","Zhe Gao","Yu Duan","Yangyang Kang","Changlong Sun","Qiong Zhang","Xiaozhong Liu"],"demo_url":"","keywords":["Camouflaged Detection","text problems","Chinese task","annotation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.279.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.279","similar_paper_uids":["main.279","main.78","main.62","main.390","main.99"],"title":"Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning","tldr":"We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A \u201cself-diversity\u201d criterion is proposed for measuring the \u201cworthiness\u201d of ...","track":"NLP Applications"},"forum":"main.279","id":"main.279","presentation_id":"38929247"},{"card_image_alt_text":"A representative figure from paper main.282","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.282.png","content":{"abstract":"The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.","authors":["Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao","Shengping Liu","Weifeng Chong"],"demo_url":"","keywords":["Automatic Coding","International Diseases","manual coding","automatic task"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.282.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.282","similar_paper_uids":["main.282","main.364","main.27","main.449","main.538"],"title":"HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding","tldr":"The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborio...","track":"NLP Applications"},"forum":"main.282","id":"main.282","presentation_id":"38928939"},{"card_image_alt_text":"A representative figure from paper main.283","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.283.png","content":{"abstract":"Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.","authors":["Boli Chen","Xin Huang","Lin Xiao","Liping Jing"],"demo_url":"","keywords":["Multi-Label Classification","MLC","routing","Hyperbolic Networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.283.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.283","similar_paper_uids":["main.283","srw.123","main.31","main.426","main.104"],"title":"Hyperbolic Capsule Networks for Multi-Label Classification","tldr":"Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance...","track":"NLP Applications"},"forum":"main.283","id":"main.283","presentation_id":"38928778"},{"card_image_alt_text":"A representative figure from paper main.281","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.281.png","content":{"abstract":"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method.","authors":["Liting Liu","Jie Liu","Wenzheng Zhang","Ziming Chi","Wenxuan Shi","Yalou Huang"],"demo_url":"","keywords":["Job Generation","recruiting process","conditional problem","Skill-Aware Model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.281.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.281","similar_paper_uids":["main.281","tacl.1886","main.352","main.601"],"title":"Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation","tldr":"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job...","track":"NLP Applications"},"forum":"main.281","id":"main.281","presentation_id":"38929207"},{"card_image_alt_text":"A representative figure from paper main.280","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.280.png","content":{"abstract":"Legal Judgement Prediction (LJP) is the task of automatically predicting a law case\u2019s judgment results given a text describing the case\u2019s facts, which has great prospects in judicial assistance systems and handy services for the public. In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged. To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems. In this paper, we present an end-to-end model, LADAN, to solve the task of LJP. To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions. Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.","authors":["Nuo Xu","Pinghui Wang","Long Chen","Li Pan","Xiaoyan Wang","Junzhou Zhao"],"demo_url":"","keywords":["Legal Prediction","judicial systems","handy services","LJP"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.280.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.280","similar_paper_uids":["main.280","main.199","main.393","main.48","main.466"],"title":"Distinguish Confusing Law Articles for Legal Judgment Prediction","tldr":"Legal Judgement Prediction (LJP) is the task of automatically predicting a law case\u2019s judgment results given a text describing the case\u2019s facts, which has great prospects in judicial assistance systems and handy services for the public. In practice, ...","track":"NLP Applications"},"forum":"main.280","id":"main.280","presentation_id":"38928782"},{"card_image_alt_text":"A representative figure from paper main.393","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.393.png","content":{"abstract":"In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story. We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants' impacts in a complex case.","authors":["Yakun Hu","Zhunchen Luo","Wenhan Chao"],"demo_url":"","keywords":["Comprehension Description","fact description","learning-to-rank framework","behavior analysis"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.393.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.393","similar_paper_uids":["main.393","main.765","main.27","main.280","main.281"],"title":"Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description","tldr":"In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. We treat the fact descriptions as narrative texts and the defendants as roles over the narrative ...","track":"NLP Applications"},"forum":"main.393","id":"main.393","presentation_id":"38928936"},{"card_image_alt_text":"A representative figure from paper main.621","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.621.png","content":{"abstract":"Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model. We set the new state of the art on benchmark datasets in English (96.1% F1 for de-identification and 88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).","authors":["Lukas Lange","Heike Adel","Jannik Str\u00f6tgen"],"demo_url":"","keywords":["Concept Extraction","natural processing","anonymization information","de-identification"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.621.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.621","similar_paper_uids":["main.621","main.748","main.717","main.184","main.760"],"title":"Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain","tldr":"Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, on...","track":"NLP Applications"},"forum":"main.621","id":"main.621","presentation_id":"38928788"},{"card_image_alt_text":"A representative figure from paper main.392","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.392.png","content":{"abstract":"With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user's latent preference factors which cause her clicks on different news. In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.","authors":["Linmei Hu","Siyong Xu","Chen Li","Cheng Yang","Chuan Shi","Nan Duan","Xing Xie","Ming Zhou"],"demo_url":"","keywords":["Graph Recommendation","Unsupervised Disentanglement","personalized recommendation","recommendation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.392.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.392","similar_paper_uids":["main.392","main.331","main.77","demo.58","demo.100"],"title":"Graph Neural News Recommendation with Unsupervised Preference Disentanglement","tldr":"With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for r...","track":"NLP Applications"},"forum":"main.392","id":"main.392","presentation_id":"38928712"},{"card_image_alt_text":"A representative figure from paper main.390","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.390.png","content":{"abstract":"Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.","authors":["Ji-Ung Lee","Christian M. Meyer","Iryna Gurevych"],"demo_url":"","keywords":["educational application","Active Learning","end-user application","active approach"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.390.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.390","similar_paper_uids":["main.390","main.305","main.189","main.59","main.86"],"title":"Empowering Active Learning to Jointly Optimize System and User Demands","tldr":"Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to f...","track":"NLP Applications"},"forum":"main.390","id":"main.390","presentation_id":"38928998"},{"card_image_alt_text":"A representative figure from paper main.623","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.623.png","content":{"abstract":"The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.","authors":["Elena Kochkina","Maria Liakata"],"demo_url":"","keywords":["automatic verification","uncertainty-based rejection","rumour models","natural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.623.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.623","similar_paper_uids":["main.623","main.161","main.262","srw.99","main.392"],"title":"Estimating predictive uncertainty for rumour verification models","tldr":"The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verificati...","track":"NLP Applications"},"forum":"main.623","id":"main.623","presentation_id":"38929128"},{"card_image_alt_text":"A representative figure from paper main.622","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.622.png","content":{"abstract":"In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model's generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.","authors":["Wei Wu","Fei Wang","Arianna Yuan","Fei Wu","Jiwei Li"],"demo_url":"","keywords":["Coreference Resolution","coreference task","span task","in answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.622.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.622","similar_paper_uids":["main.622","tacl.1853","main.738","main.585","main.299"],"title":"CorefQA: Coreference Resolution as Query-based Span Prediction","tldr":"In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surr...","track":"NLP Applications"},"forum":"main.622","id":"main.622","presentation_id":"38928701"},{"card_image_alt_text":"A representative figure from paper main.391","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.391.png","content":{"abstract":"This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.","authors":["Masahiro Kaneko","Masato Mita","Shun Kiyono","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["Grammatical Correction","GEC","Encoder-Decoder Models","Pre-trained Models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.391.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.391","similar_paper_uids":["main.391","main.240","main.290","tacl.1876","main.610"],"title":"Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction","tldr":"This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as on...","track":"NLP Applications"},"forum":"main.391","id":"main.391","presentation_id":"38929265"},{"card_image_alt_text":"A representative figure from paper main.75","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.75.png","content":{"abstract":"Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.","authors":["Yichao Zhou","Jyun-Yu Jiang","Jieyu Zhao","Kai-Wei Chang","Wei Wang"],"demo_url":"","keywords":["Pronunciation-attentive Recognition","human languages","intelligence systems","pun tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.75.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.75","similar_paper_uids":["main.75","demo.96","srw.104","main.118"],"title":"\"The Boating Store Had Its Best Sail Ever\": Pronunciation-attentive Contextualized Pun Recognition","tldr":"Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonet...","track":"NLP Applications"},"forum":"main.75","id":"main.75","presentation_id":"38928896"},{"card_image_alt_text":"A representative figure from paper main.395","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.395.png","content":{"abstract":"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F\u2081: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","authors":["Sebastian Weigelt","Vanessa Steurer","Tobias Hey","Walter F. Tichy"],"demo_url":"","keywords":["intelligent systems","information retrieval","Deep Understanding","end-user programming"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.395.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.395","similar_paper_uids":["main.395","main.729","main.345","srw.98","main.625"],"title":"Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding","tldr":"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built f...","track":"NLP Applications"},"forum":"main.395","id":"main.395","presentation_id":"38928791"},{"card_image_alt_text":"A representative figure from paper main.155","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.155.png","content":{"abstract":"Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions. On the other hand, speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering. Overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard, but not as a complete substitute.","authors":["Nico Herbig","Tim D\u00fcwel","Santanu Pal","Kalliopi Meladaki","Mahsa Monshizadeh","Antonio Kr\u00fcger","Josef van Genabith"],"demo_url":"","keywords":["Post-Editing Translation","machine translation","MT","translators"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.155.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.155","similar_paper_uids":["main.155","demo.33","main.254","main.215","main.400"],"title":"MMPE: A Multi-Modal Interface for Post-Editing Machine Translation","tldr":"Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translatio...","track":"NLP Applications"},"forum":"main.155","id":"main.155","presentation_id":"38929416"},{"card_image_alt_text":"A representative figure from paper main.394","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.394.png","content":{"abstract":"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","authors":["Santhosh Rajamanickam","Pushkar Mishra","Helen Yannakoudakis","Ekaterina Shutova"],"demo_url":"","keywords":["Joint Detection","abuse detection","abusive detection","multi-task framework"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.394.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.394","similar_paper_uids":["main.394","main.401","main.372","main.288","main.336"],"title":"Joint Modelling of Emotion and Abusive Language Detection","tldr":"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has expe...","track":"NLP Applications"},"forum":"main.394","id":"main.394","presentation_id":"38929125"},{"card_image_alt_text":"A representative figure from paper main.76","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.76.png","content":{"abstract":"Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta.","authors":["Joongbo Shin","Yoonhyung Lee","Seunghyun Yoon","Kyomin Jung"],"demo_url":"","keywords":["Unsupervised Learning","supervised tasks","unsupervised tasks","computation representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.76.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.76","similar_paper_uids":["main.76","main.204","main.195","main.247","main.250"],"title":"Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning","tldr":"Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this l...","track":"NLP Applications"},"forum":"main.76","id":"main.76","presentation_id":"38928923"},{"card_image_alt_text":"A representative figure from paper main.396","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.396.png","content":{"abstract":"Moderation is crucial to promoting healthy online discussions. Although several \u2018toxicity\u2019 detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.","authors":["John Pavlopoulos","Jeffrey Sorensen","Lucas Dixon","Nithum Thain","Ion Androutsopoulos"],"demo_url":"","keywords":["Toxicity Detection","healthy discussions","toxicity systems","toxicity classifiers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.396.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.396","similar_paper_uids":["main.396","main.530","srw.122","main.762","main.515"],"title":"Toxicity Detection: Does Context Really Matter?","tldr":"Moderation is crucial to promoting healthy online discussions. Although several \u2018toxicity\u2019 detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently....","track":"NLP Applications"},"forum":"main.396","id":"main.396","presentation_id":"38929415"},{"card_image_alt_text":"A representative figure from paper main.625","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.625.png","content":{"abstract":"Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks. However, fine-tuning still requires task-specific training data. In this paper, we propose N\u00b3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. N\u00b3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively ``fine-tuning'' the network for a new task using only language descriptions as input. To the best of our knowledge, N\u00b3 is the first method to synthesize entire neural networks from natural language. Experimental results show that N\u00b3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N\u00b3 when synthesizing model parameters.","authors":["Tian Jin","Zhun Liu","Shengjia Yan","Alexandre Eichenberger","Louis-Philippe Morency"],"demo_url":"","keywords":["Transfer learning","computer tasks","fine-tuning","Conditional Adaptation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.625.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.625","similar_paper_uids":["main.625","main.12","main.197","main.249","main.314"],"title":"Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions","tldr":"Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks. However, fine-tuning still requires task-specific training data. In this paper, we propose N\u00b3 (Neural Networks from Natural L...","track":"NLP Applications"},"forum":"main.625","id":"main.625","presentation_id":"38928907"},{"card_image_alt_text":"A representative figure from paper main.208","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.208.png","content":{"abstract":"We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.","authors":["Ruiqi Zhong","Mitchell Stern","Dan Klein"],"demo_url":"","keywords":["Pseudocode-to-Code Generation","program generation","inference","Semantic Scaffolds"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.208.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.208","similar_paper_uids":["main.208","main.614","main.707","main.205","main.677"],"title":"Semantic Scaffolds for Pseudocode-to-Code Generation","tldr":"We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints fo...","track":"NLP Applications"},"forum":"main.208","id":"main.208","presentation_id":"38929435"},{"card_image_alt_text":"A representative figure from paper main.624","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.624.png","content":{"abstract":"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.","authors":["Jan-Christoph Klie","Richard Eckart de Castilho","Iryna Gurevych"],"demo_url":"","keywords":["Human-In-The-Loop Linking","Entity linking","disambiguating mentions","annotation process"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.624.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 09:00:00 GMT","session_name":"12A","start_time":"Wed, 08 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.624","similar_paper_uids":["main.624","demo.69","tacl.1906","demo.24","main.18"],"title":"From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains","tldr":"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics an...","track":"NLP Applications"},"forum":"main.624","id":"main.624","presentation_id":"38929059"},{"card_image_alt_text":"A representative figure from paper main.77","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.77.png","content":{"abstract":"Personalized news recommendation is a critical technology to improve users\u2019 online news reading experience. The core of news recommendation is accurate matching between user's interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation. Instead of aggregating user's all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","authors":["Heyuan Wang","Fangzhao Wu","Zheng Liu","Xing Xie"],"demo_url":"","keywords":["Fine-grained Matching","Neural Recommendation","Personalized recommendation","news recommendation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.77.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.77","similar_paper_uids":["main.77","main.331","main.392","main.478","demo.100"],"title":"Fine-grained Interest Matching for Neural News Recommendation","tldr":"Personalized news recommendation is a critical technology to improve users\u2019 online news reading experience. The core of news recommendation is accurate matching between user's interests and candidate news. The same user usually has diverse interests ...","track":"NLP Applications"},"forum":"main.77","id":"main.77","presentation_id":"38929388"},{"card_image_alt_text":"A representative figure from paper tacl.1743","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1743.png","content":{"abstract":"We describe a method for rapidly creating language proficiency assessments, and provide experimental evidence that such tests can be valid, reliable, and secure. Our approach is the first to use machine learning and natural language processing to induce proficiency scales based on a given standard, and then use linguistic models to estimate item difficulty directly for computer-adaptive testing. This alleviates the need for expensive pilot testing with human subjects. We used these methods to develop an online proficiency exam called the Duolingo English Test, and demonstrate that its scores align significantly with other high-stakes English assessments. Furthermore, our approach produces test scores that are highly reliable, while generating item banks large enough to satisfy security requirements.","authors":["Burr Settles","Masato Hagiwara","Geoffrey T. LaFlair"],"demo_url":"","keywords":["Machine Assessment","language assessments","natural processing","computer-adaptive testing"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00310","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00310","similar_paper_uids":["tacl.1743","main.179","main.491","main.442","main.206"],"title":"Machine Learning-Driven Language Assessment","tldr":"We describe a method for rapidly creating language proficiency assessments, and provide experimental evidence that such tests can be valid, reliable, and secure. Our approach is the first to use machine learning and natural language processing to ind...","track":"NLP Applications"},"forum":"tacl.1743","id":"tacl.1743","presentation_id":"38929487"}]
