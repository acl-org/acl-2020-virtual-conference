[{"card_image_alt_text":"A representative figure from paper main.167","card_image_path":"static/images/papers/main.167.png","content":{"abstract":"Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.","authors":["Manuel Mager","Ram\u00f3n Fernandez Astudillo","Tahira Naseem","Md Arafat Sultan","Young-Suk Lee","Radu Florian","Salim Roukos"],"demo_url":"","keywords":["AMR-to-Text Generation","GPT-too","Language-Model-First Approach","AMRs"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.167.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.167","similar_paper_uids":["main.167","tacl.1805","main.67","main.397","main.119"],"title":"GPT-too: A Language-Model-First Approach for AMR-to-Text Generation","tldr":"Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this p...","track":"Generation"},"forum":"main.167","id":"main.167","presentation_id":"38929147"},{"card_image_alt_text":"A representative figure from paper main.164","card_image_path":"static/images/papers/main.164.png","content":{"abstract":"Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-_k_, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.","authors":["Daphne Ippolito","Daniel Duckworth","Chris Callison-Burch","Douglas Eck"],"demo_url":"","keywords":["Automatic Text","detection","humanness systems","neural modelling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.164.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.164","similar_paper_uids":["main.164","main.173","main.516","main.219","main.419"],"title":"Automatic Detection of Generated Text is Easiest when Humans are Fooled","tldr":"Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research i...","track":"Generation"},"forum":"main.164","id":"main.164","presentation_id":"38928914"},{"card_image_alt_text":"A representative figure from paper main.68","card_image_path":"static/images/papers/main.68.png","content":{"abstract":"Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.","authors":["Piji Li","Haisong Zhang","Xiaojiang Liu","Shuming Shi"],"demo_url":"","keywords":["Rigid Generation","Neural generation","text generation","rhyming schemes"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.68.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.68","similar_paper_uids":["main.68","main.9","main.185","main.708","main.358"],"title":"Rigid Formats Controlled Text Generation","tldr":"Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such...","track":"Generation"},"forum":"main.68","id":"main.68","presentation_id":"38928912"},{"card_image_alt_text":"A representative figure from paper main.69","card_image_path":"static/images/papers/main.69.png","content":{"abstract":"Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.","authors":["Kaustubh Dhole","Christopher D. Manning"],"demo_url":"","keywords":["Question Generation","syntactic transformation","crowd-sourced evaluations","generating questions"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.69.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.69","similar_paper_uids":["main.69","main.545","main.21","tacl.1845","main.498"],"title":"Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation","tldr":"Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules levera...","track":"Generation"},"forum":"main.69","id":"main.69","presentation_id":"38929019"},{"card_image_alt_text":"A representative figure from paper main.228","card_image_path":"static/images/papers/main.228.png","content":{"abstract":"Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.","authors":["Nabil Hossain","Marjan Ghazvininejad","Luke Zettlemoyer"],"demo_url":"","keywords":["Retrieve-Edit-Rerank Generation","candidate selection","Retrieve-and-edit methods","post-generation approach"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.228.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.228","similar_paper_uids":["main.228","main.172","tacl.1906","main.17","main.707"],"title":"Simple and Effective Retrieve-Edit-Rerank Text Generation","tldr":"Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our frame...","track":"Generation"},"forum":"main.228","id":"main.228","presentation_id":"38929289"},{"card_image_alt_text":"A representative figure from paper main.639","card_image_path":"static/images/papers/main.639.png","content":{"abstract":"Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.","authors":["Chulun Zhou","Liangyu Chen","Jiachen Liu","Xinyan Xiao","Jinsong Su","Sheng Guo","Hua Wu"],"demo_url":"","keywords":["Exploring Relevance","Contextual Relevance","Unsupervised Transfer","style transfer"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.639.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.639","similar_paper_uids":["main.639","srw.17","main.169","main.673","main.354"],"title":"Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer","tldr":"Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the ...","track":"Generation"},"forum":"main.639","id":"main.639","presentation_id":"38928938"},{"card_image_alt_text":"A representative figure from paper main.25","card_image_path":"static/images/papers/main.25.png","content":{"abstract":"Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.","authors":["Yi Tay","Dara Bahri","Che Zheng","Clifford Brunk","Donald Metzler","Andrew Tomkins"],"demo_url":"","keywords":["Reverse Models","neural modeling","Neural Models","generative models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.25.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.25","similar_paper_uids":["main.25","main.173","main.223","main.646","main.561"],"title":"Reverse Engineering Configurations of Neural Text Generation Models","tldr":"Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental propertie...","track":"Generation"},"forum":"main.25","id":"main.25","presentation_id":"38929268"},{"card_image_alt_text":"A representative figure from paper main.19","card_image_path":"static/images/papers/main.19.png","content":{"abstract":"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model's scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem.","authors":["Ashutosh Baheti","Alan Ritter","Kevin Small"],"demo_url":"","keywords":["Fluent Generation","Conversational Answering","Question answering","Question QA"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.19.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.19","similar_paper_uids":["main.19","main.413","main.652","main.410","main.450"],"title":"Fluent Response Generation for Conversational Question Answering","tldr":"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extrac...","track":"Generation"},"forum":"main.19","id":"main.19","presentation_id":"38928997"},{"card_image_alt_text":"A representative figure from paper main.703","card_image_path":"static/images/papers/main.703.png","content":{"abstract":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.","authors":["Mike Lewis","Yinhan Liu","Naman Goyal","Marjan Ghazvininejad","Abdelrahman Mohamed","Omer Levy","Veselin Stoyanov","Luke Zettlemoyer"],"demo_url":"","keywords":["Natural Generation","Translation","Comprehension","pretraining models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.703.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.703","similar_paper_uids":["main.703","main.740","tacl.1853","main.705","main.247"],"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","tldr":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-...","track":"Generation"},"forum":"main.703","id":"main.703","presentation_id":"38929218"},{"card_image_alt_text":"A representative figure from paper main.18","card_image_path":"static/images/papers/main.18.png","content":{"abstract":"Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG","authors":["Zhiyu Chen","Harini Eavani","Wenhu Chen","Yinyin Liu","William Yang Wang"],"demo_url":"","keywords":["natural generation","NLG","real-world applications","content selection"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.18.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.18","similar_paper_uids":["main.18","main.538","main.436","main.135","main.522"],"title":"Few-Shot NLG with Pre-Trained Language Model","tldr":"Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-sho...","track":"Generation"},"forum":"main.18","id":"main.18","presentation_id":"38928835"},{"card_image_alt_text":"A representative figure from paper main.24","card_image_path":"static/images/papers/main.24.png","content":{"abstract":"Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.","authors":["Yi Liao","Xin Jiang","Qun Liu"],"demo_url":"","keywords":["Autoregressive Generation","natural tasks","natural generation","natural NLG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.24.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.24","similar_paper_uids":["main.24","main.705","main.200","main.36","main.542"],"title":"Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order","tldr":"Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GP...","track":"Generation"},"forum":"main.24","id":"main.24","presentation_id":"38929449"},{"card_image_alt_text":"A representative figure from paper main.26","card_image_path":"static/images/papers/main.26.png","content":{"abstract":"While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.","authors":["Qian Yu","Lidong Bing","Qiong Zhang","Wai Lam","Luo Si"],"demo_url":"","keywords":["Review-based Generation","Adaptive Transfer","Adaptive Augmentation","online services"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.26.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.26","similar_paper_uids":["main.26","main.631","main.175","main.461","main.21"],"title":"Review-based Question Generation with Adaptive Instance Transfer and Augmentation","tldr":"While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new ...","track":"Generation"},"forum":"main.26","id":"main.26","presentation_id":"38929281"},{"card_image_alt_text":"A representative figure from paper main.27","card_image_path":"static/images/papers/main.27.png","content":{"abstract":"Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.","authors":["Ruichu Cai","Zhihao Liang","Boyan Xu","zijian li","Yuexing Hao","Yao Chen"],"demo_url":"","keywords":["Code Generation","code task","adaptive code","TAG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.27.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.27","similar_paper_uids":["main.27","main.282","main.749","main.572","main.449"],"title":"TAG : Type Auxiliary Guiding for Code Comment Generation","tldr":"Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing fram...","track":"Generation"},"forum":"main.27","id":"main.27","presentation_id":"38929371"},{"card_image_alt_text":"A representative figure from paper main.23","card_image_path":"static/images/papers/main.23.png","content":{"abstract":"Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow \"one-to-many'' conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.","authors":["Yu Duan","Canwen Xu","Jiaxin Pei","Jialong Han","Chenliang Li"],"demo_url":"","keywords":["Flexible Generation","Conditional Generation","Natural Generation","Natural NLG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.23.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.23","similar_paper_uids":["main.23","main.235","main.708","main.705","main.354"],"title":"Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders","tldr":"Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emergi...","track":"Generation"},"forum":"main.23","id":"main.23","presentation_id":"38928857"},{"card_image_alt_text":"A representative figure from paper main.664","card_image_path":"static/images/papers/main.664.png","content":{"abstract":"Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available.","authors":["Zhan Shi","Xu Zhou","Xipeng Qiu","Xiaodan Zhu"],"demo_url":"","keywords":["Image Captioning","multimodal problem","natural processing","computer community"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.664.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.664","similar_paper_uids":["main.664","main.731","main.306","main.583","main.683"],"title":"Improving Image Captioning with Better Use of Caption","tldr":"Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available i...","track":"Generation"},"forum":"main.664","id":"main.664","presentation_id":"38929192"},{"card_image_alt_text":"A representative figure from paper main.710","card_image_path":"static/images/papers/main.710.png","content":{"abstract":"Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.","authors":["Xingdi Yuan","Tong Wang","Rui Meng","Khushboo Thaker","Peter Brusilovsky","Daqing He","Adam Trischler"],"demo_url":"","keywords":["modeling perspectives","variable-number generation","keyphrase tasks","neural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.710.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.710","similar_paper_uids":["main.710","main.103","main.105","main.101","main.300"],"title":"One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases","tldr":"Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives....","track":"Generation"},"forum":"main.710","id":"main.710","presentation_id":"38929166"},{"card_image_alt_text":"A representative figure from paper main.704","card_image_path":"static/images/papers/main.704.png","content":{"abstract":"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.","authors":["Thibault Sellam","Dipanjan Das","Ankur Parikh"],"demo_url":"","keywords":["Learning Metrics","Text Generation","WMT task","pre-training scheme"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.704.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.704","similar_paper_uids":["main.704","main.770","main.93","main.693","srw.58"],"title":"BLEURT: Learning Robust Metrics for Text Generation","tldr":"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metr...","track":"Generation"},"forum":"main.704","id":"main.704","presentation_id":"38929170"},{"card_image_alt_text":"A representative figure from paper main.705","card_image_path":"static/images/papers/main.705.png","content":{"abstract":"Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. By leveraging BERT's idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization. Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets.","authors":["Yen-Chun Chen","Zhe Gan","Yu Cheng","Jingzhou Liu","Jingjing Liu"],"demo_url":"","keywords":["Text Generation","language tasks","language generation","generation tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.705.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.705","similar_paper_uids":["main.705","main.247","demo.47","main.24","main.195"],"title":"Distilling Knowledge Learned in BERT for Text Generation","tldr":"Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional ...","track":"Generation"},"forum":"main.705","id":"main.705","presentation_id":"38929146"},{"card_image_alt_text":"A representative figure from paper main.711","card_image_path":"static/images/papers/main.711.png","content":{"abstract":"We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. Human evaluation shows that our system generates sarcasm better than humans 34% of the time, and better than a reinforced hybrid baseline 90% of the time.","authors":["Tuhin Chakrabarty","Debanjan Ghosh","Smaranda Muresan","Nanyun Peng"],"demo_url":"","keywords":["Sarcasm Generation","unsupervised approach","retrieve-and-edit framework","Human evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.711.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.711","similar_paper_uids":["main.711","main.118","main.349","main.515","tacl.1886"],"title":"R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge","tldr":"We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity wit...","track":"Generation"},"forum":"main.711","id":"main.711","presentation_id":"38929144"},{"card_image_alt_text":"A representative figure from paper main.665","card_image_path":"static/images/papers/main.665.png","content":{"abstract":"The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages. In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task. Contrary to the findings of the 2018 shared task, we show, in experiments on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect -- an improvement of almost 8 BLEU points for a previously state-of-the-art system. We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data.","authors":["Henry Elder","Robert Burke","Alexander O'Connor","Jennifer Foster"],"demo_url":"","keywords":["English Realization","Surface Tasks","Natural tasks","2018 task"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.665.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.665","similar_paper_uids":["main.665","main.163","cl.1552","cl.1547","main.177"],"title":"Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization","tldr":"The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages. In the 201...","track":"Generation"},"forum":"main.665","id":"main.665","presentation_id":"38929432"},{"card_image_alt_text":"A representative figure from paper main.22","card_image_path":"static/images/papers/main.22.png","content":{"abstract":"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly \"reorder'' the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.","authors":["Tanya Goyal","Greg Durrett"],"demo_url":"","keywords":["Controlled Generation","Paraphrasing sentences","machine translation","Neural Preordering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.22.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.22","similar_paper_uids":["main.22","main.535","main.545","tacl.1967","main.28"],"title":"Neural Syntactic Preordering for Controlled Paraphrase Generation","tldr":"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggl...","track":"Generation"},"forum":"main.22","id":"main.22","presentation_id":"38928794"},{"card_image_alt_text":"A representative figure from paper main.20","card_image_path":"static/images/papers/main.20.png","content":{"abstract":"One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.","authors":["Dong Bok Lee","Seanie Lee","Woo Tae Jeong","Donghwan Kim","Sung Ju Hwang"],"demo_url":"","keywords":["question answering","QA","QA","Information-Maximizing VAEs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.20.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.20","similar_paper_uids":["main.20","main.413","main.500","main.652","main.772"],"title":"Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs","tldr":"One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to...","track":"Generation"},"forum":"main.20","id":"main.20","presentation_id":"38928851"},{"card_image_alt_text":"A representative figure from paper main.101","card_image_path":"static/images/papers/main.101.png","content":{"abstract":"Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.","authors":["Zhenyi Wang","Xiaoyang Wang","Bang An","Dong Yu","Changyou Chen"],"demo_url":"","keywords":["Faithful Generation","Text generation","table-to-text problem","Transformer-based framework"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.101.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.101","similar_paper_uids":["main.101","main.454","main.712","main.355","main.456"],"title":"Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints","tldr":"Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information th...","track":"Generation"},"forum":"main.101","id":"main.101","presentation_id":"38929079"},{"card_image_alt_text":"A representative figure from paper main.707","card_image_path":"static/images/papers/main.707.png","content":{"abstract":"We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.","authors":["Dhruv Kumar","Lili Mou","Lukasz Golab","Olga Vechtomova"],"demo_url":"","keywords":["Iterative Simplification","unsupervised simplification","iterative approach","word edits"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.707.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.707","similar_paper_uids":["main.707","main.28","main.709","main.460","main.424"],"title":"Iterative Edit-Based Unsupervised Sentence Simplification","tldr":"We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on ...","track":"Generation"},"forum":"main.707","id":"main.707","presentation_id":"38929280"},{"card_image_alt_text":"A representative figure from paper main.712","card_image_path":"static/images/papers/main.712.png","content":{"abstract":"The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.","authors":["Linfeng Song","Ante Wang","Jinsong Su","Yue Zhang","Kun Xu","Yubin Ge","Dong Yu"],"demo_url":"","keywords":["Structural Preserving","Graph-to-Text Generation","multi-task training","meaning graphs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.712.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.712","similar_paper_uids":["main.712","tacl.1805","main.67","main.224","main.640"],"title":"Structural Information Preserving for Graph-to-Text Generation","tldr":"The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generat...","track":"Generation"},"forum":"main.712","id":"main.712","presentation_id":"38928740"},{"card_image_alt_text":"A representative figure from paper main.706","card_image_path":"static/images/papers/main.706.png","content":{"abstract":"Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events. We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach. Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions. Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations. Dataset, code and documentation are available at https://github.com/salesforce/esprit.","authors":["Nazneen Fatema Rajani","Rui Zhang","Yi Chern Tan","Stephan Zheng","Jeremy Weiss","Aadit Vyas","Abhijit Gupta","Caiming Xiong","Richard Socher","Dragomir Radev"],"demo_url":"","keywords":["Physical Tasks","commonsense reasoning","ESPRIT","Neural networks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.706.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.706","similar_paper_uids":["main.706","demo.69","main.522","main.708","main.100"],"title":"ESPRIT: Explaining Solutions to Physical Reasoning Tasks","tldr":"Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that g...","track":"Generation"},"forum":"main.706","id":"main.706","presentation_id":"38928707"},{"card_image_alt_text":"A representative figure from paper main.666","card_image_path":"static/images/papers/main.666.png","content":{"abstract":"We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.","authors":["Daphne Ippolito","David Grangier","Douglas Eck","Chris Callison-Burch"],"demo_url":"","keywords":["unsupervised task","larger-scale tasks","Sentence-Level Models","sentence-level model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.666.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.666","similar_paper_uids":["main.666","main.226","main.83","main.488","tacl.1849"],"title":"Toward Better Storylines with Sentence-Level Language Models","tldr":"We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which a...","track":"Generation"},"forum":"main.666","id":"main.666","presentation_id":"38928916"},{"card_image_alt_text":"A representative figure from paper main.100","card_image_path":"static/images/papers/main.100.png","content":{"abstract":"The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.","authors":["Yixin Cao","Ruihao Shui","Liangming Pan","Min-Yen Kan","Zhiyuan Liu","Tat-Seng Chua"],"demo_url":"","keywords":["Expertise Transfer","style transfer","text simplification","automatic evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.100.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.100","similar_paper_uids":["main.100","main.115","main.169","demo.69","main.105"],"title":"Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen","tldr":"The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only s...","track":"Generation"},"forum":"main.100","id":"main.100","presentation_id":"38929081"},{"card_image_alt_text":"A representative figure from paper main.21","card_image_path":"static/images/papers/main.21.png","content":{"abstract":"Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only capture limited context dependencies. To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works.","authors":["Zi Chai","Xiaojun Wan"],"demo_url":"","keywords":["Semi-Autoregressive Generation","Question Generation","dialog task","generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.21.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.21","similar_paper_uids":["main.21","main.69","main.355","main.498","srw.122"],"title":"Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction","tldr":"Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since th...","track":"Generation"},"forum":"main.21","id":"main.21","presentation_id":"38929330"},{"card_image_alt_text":"A representative figure from paper main.640","card_image_path":"static/images/papers/main.640.png","content":{"abstract":"The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.","authors":["Shaowei Yao","Tianming Wang","Xiaojun Wan"],"demo_url":"","keywords":["Graph-to-Sequence Learning","text generation","AMR-to-text generation","syntax-based translation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.640.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.640","similar_paper_uids":["main.640","main.67","tacl.1805","main.241","main.224"],"title":"Heterogeneous Graph Transformer for Graph-to-Sequence Learning","tldr":"The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect rel...","track":"Generation"},"forum":"main.640","id":"main.640","presentation_id":"38929420"},{"card_image_alt_text":"A representative figure from paper main.708","card_image_path":"static/images/papers/main.708.png","content":{"abstract":"Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t.\\ logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at https://github.com/wenhuchen/LogicNLG.","authors":["Wenhu Chen","Jianshu Chen","Yu Su","Zhiyu Chen","William Yang Wang"],"demo_url":"","keywords":["Logical Generation","neural NLG","surface-level realizations","logical inference"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.708.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.708","similar_paper_uids":["main.708","main.63","main.4","main.769","main.18"],"title":"Logical Natural Language Generation from Open-Domain Tables","tldr":"Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, ...","track":"Generation"},"forum":"main.708","id":"main.708","presentation_id":"38928710"},{"card_image_alt_text":"A representative figure from paper main.709","card_image_path":"static/images/papers/main.709.png","content":{"abstract":"The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.","authors":["Chao Jiang","Mounica Maddela","Wuwei Lan","Yang Zhong","Wei Xu"],"demo_url":"","keywords":["Sentence Alignment","Text Simplification","monolingual task","automatic evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.709.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.709","similar_paper_uids":["main.709","main.424","main.707","srw.137","main.417"],"title":"Neural CRF Model for Sentence Alignment in Text Simplification","tldr":"The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence ali...","track":"Generation"},"forum":"main.709","id":"main.709","presentation_id":"38929317"},{"card_image_alt_text":"A representative figure from paper main.641","card_image_path":"static/images/papers/main.641.png","content":{"abstract":"The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and ``hallucination''. Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. The segmentation and correspondence are jointly learned as latent variables without any human annotations. We further impose a soft statistical constraint to regularize the segmental granularity. The resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost. On both E2E and WebNLG benchmarks, we show the proposed model consistently outperforms its neural attention counterparts.","authors":["Xiaoyu Shen","Ernie Chang","Hui Su","Cheng Niu","Dietrich Klakow"],"demo_url":"","keywords":["Neural Generation","Segmentation","data-to-text tasks","neural model"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.641.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.641","similar_paper_uids":["main.641","main.432","main.419","main.385","main.687"],"title":"Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence","tldr":"The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and ``hallucination''. Due to the black-box natu...","track":"Generation"},"forum":"main.641","id":"main.641","presentation_id":"38929190"},{"card_image_alt_text":"A representative figure from paper main.16","card_image_path":"static/images/papers/main.16.png","content":{"abstract":"Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption. We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset. Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.","authors":["Ashish V. Thapliyal","Radu Soricut"],"demo_url":"","keywords":["Cross-modal Generation","Web-scale Coverage","Cross-modal tasks","Pivot Stabilization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.16.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.16","similar_paper_uids":["main.16","main.583","main.664","main.93","main.469"],"title":"Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage","tldr":"Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solution...","track":"Generation"},"forum":"main.16","id":"main.16","presentation_id":"38929276"},{"card_image_alt_text":"A representative figure from paper main.17","card_image_path":"static/images/papers/main.17.png","content":{"abstract":"We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach.","authors":["Hayate Iso","Chao Qiao","Hang Li"],"demo_url":"","keywords":["Fact-based Editing","text task","text editing","automatically dataset"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.17.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.17","similar_paper_uids":["main.17","main.168","main.546","main.332","main.101"],"title":"Fact-based Text Editing","tldr":"We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecti...","track":"Generation"},"forum":"main.17","id":"main.17","presentation_id":"38929258"},{"card_image_alt_text":"A representative figure from paper main.15","card_image_path":"static/images/papers/main.15.png","content":{"abstract":"Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.","authors":["Yi Ren","Jinglin Liu","Xu Tan","Zhou Zhao","Sheng Zhao","Tie-Yan Liu"],"demo_url":"","keywords":["Sequence Generation","AR","neural translation","automatic recognition"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.15.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.15","similar_paper_uids":["main.15","main.171","main.277","main.350","main.6"],"title":"A Study of Non-autoregressive Model for Sequence Generation","tldr":"Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge dis...","track":"Generation"},"forum":"main.15","id":"main.15","presentation_id":"38929314"},{"card_image_alt_text":"A representative figure from paper main.134","card_image_path":"static/images/papers/main.134.png","content":{"abstract":"We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.","authors":["Xiang Yu","Simon Tannert","Ngoc Thang Vu","Jonas Kuhn"],"demo_url":"","keywords":["dependency task","Traveling TSP","TSP","decoding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.134.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.134","similar_paper_uids":["main.134","main.302","main.362","main.301","main.298"],"title":"Fast and Accurate Non-Projective Dependency Tree Linearization","tldr":"We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the T...","track":"Generation"},"forum":"main.134","id":"main.134","presentation_id":"38928825"},{"card_image_alt_text":"A representative figure from paper main.135","card_image_path":"static/images/papers/main.135.png","content":{"abstract":"This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.","authors":["Liangming Pan","Yuxi Xie","Yansong Feng","Tat-Seng Chua","Min-Yen Kan"],"demo_url":"","keywords":["Generating Questions","Deep Generation","Deep DQG","reasoning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.135.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.135","similar_paper_uids":["main.135","main.141","main.670","main.586","main.88"],"title":"Semantic Graphs for Generating Deep Questions","tldr":"This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document a...","track":"Generation"},"forum":"main.135","id":"main.135","presentation_id":"38929018"},{"card_image_alt_text":"A representative figure from paper main.28","card_image_path":"static/images/papers/main.28.png","content":{"abstract":"We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.","authors":["Xianggen Liu","Lili Mou","Fandong Meng","Hao Zhou","Jie Zhou","Sen Song"],"demo_url":"","keywords":["Unsupervised Paraphrasing","paraphrase generation","optimization problem","Unsupervised Paraphrasing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.28.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.28","similar_paper_uids":["main.28","main.535","main.707","main.654","srw.19"],"title":"Unsupervised Paraphrasing by Simulated Annealing","tldr":"We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression ...","track":"Generation"},"forum":"main.28","id":"main.28","presentation_id":"38928768"},{"card_image_alt_text":"A representative figure from paper main.67","card_image_path":"static/images/papers/main.67.png","content":{"abstract":"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -- A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.","authors":["Yanbin Zhao","Lu Chen","Zhi Chen","Ruisheng Cao","Su Zhu","Kai Yu"],"demo_url":"","keywords":["Line Generation","AMR-to-text generation","graph-to-sequence task","graph-to-sequence modeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.67.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.67","similar_paper_uids":["main.67","tacl.1805","main.640","main.167","main.241"],"title":"Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks","tldr":"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -- A graph-to-sequence task aiming to recover natural language from Abstract Mean...","track":"Generation"},"forum":"main.67","id":"main.67","presentation_id":"38929027"},{"card_image_alt_text":"A representative figure from paper main.224","card_image_path":"static/images/papers/main.224.png","content":{"abstract":"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.","authors":["Chao Zhao","Marilyn Walker","Snigdha Chaturvedi"],"demo_url":"","keywords":["Data-To-Text Generation","faithful generation","Encoding","Decoding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.224.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.224","similar_paper_uids":["main.224","main.67","main.640","main.712","main.539"],"title":"Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation","tldr":"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence model...","track":"Generation"},"forum":"main.224","id":"main.224","presentation_id":"38929169"},{"card_image_alt_text":"A representative figure from paper main.225","card_image_path":"static/images/papers/main.225.png","content":{"abstract":"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling---a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.","authors":["Chris Donahue","Mina Lee","Percy Liang"],"demo_url":"","keywords":["text infilling","predicting text","writing tools","language modeling"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.225.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.225","similar_paper_uids":["main.225","main.11","tacl.1886","tacl.1853","main.25"],"title":"Enabling Language Models to Fill in the Blanks","tldr":"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to l...","track":"Generation"},"forum":"main.225","id":"main.225","presentation_id":"38929175"},{"card_image_alt_text":"A representative figure from paper main.66","card_image_path":"static/images/papers/main.66.png","content":{"abstract":"Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.","authors":["Daniel Kang","Tatsunori Hashimoto"],"demo_url":"","keywords":["Natural Generation","optimization","estimation","distinguishability"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.66.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.66","similar_paper_uids":["main.66","main.428","main.564","main.491","main.432"],"title":"Improved Natural Language Generation via Loss Truncation","tldr":"Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, includi...","track":"Generation"},"forum":"main.66","id":"main.66","presentation_id":"38929231"},{"card_image_alt_text":"A representative figure from paper main.227","card_image_path":"static/images/papers/main.227.png","content":{"abstract":"Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.","authors":["Ruiyi Zhang","Changyou Chen","Zhe Gan","Wenlin Wang","Dinghan Shen","Guoyin Wang","Zheng Wen","Lawrence Carin"],"demo_url":"","keywords":["Adversarial Generation","long generation","next-word prediction","generator optimization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.227.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.227","similar_paper_uids":["main.227","main.52","main.705","main.516","main.354"],"title":"Improving Adversarial Text Generation by Modeling the Distant Future","tldr":"Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rul...","track":"Generation"},"forum":"main.227","id":"main.227","presentation_id":"38928708"},{"card_image_alt_text":"A representative figure from paper main.226","card_image_path":"static/images/papers/main.226.png","content":{"abstract":"Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2. We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.","authors":["Yichen Huang","Yizhe Zhang","Oussama Elachqar","Yu Cheng"],"demo_url":"","keywords":["Sentence Infilling","Missing generation","sentence in-filling","natural generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.226.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.226","similar_paper_uids":["main.226","main.705","main.9","main.666","main.515"],"title":"INSET: Sentence Infilling with INter-SEntential Transformer","tldr":"Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences...","track":"Generation"},"forum":"main.226","id":"main.226","presentation_id":"38929392"},{"card_image_alt_text":"A representative figure from paper main.65","card_image_path":"static/images/papers/main.65.png","content":{"abstract":"Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the ``components'' of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that \\method achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.","authors":["Jiahuan Li","Yu Bao","Shujian Huang","Xinyu Dai","Jiajun Chen"],"demo_url":"","keywords":["Definition Generation","construction dictionaries","under-specific generation","Explicit Decomposition"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.65.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.65","similar_paper_uids":["main.65","main.544","main.235","main.710","main.694"],"title":"Explicit Semantic Decomposition for Definition Generation","tldr":"Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider e...","track":"Generation"},"forum":"main.65","id":"main.65","presentation_id":"38928989"},{"card_image_alt_text":"A representative figure from paper main.168","card_image_path":"static/images/papers/main.168.png","content":{"abstract":"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.","authors":["Sheena Panthaplackel","Pengyu Nie","Milos Gligoric","Junyi Jessy Li","Raymond Mooney"],"demo_url":"","keywords":["automatically comment","making edits","language representations","edits"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.168.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.168","similar_paper_uids":["main.168","main.17","main.27","main.568","main.4"],"title":"Learning to Update Natural Language Comments Based on Code Changes","tldr":"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to g...","track":"Generation"},"forum":"main.168","id":"main.168","presentation_id":"38929204"},{"card_image_alt_text":"A representative figure from paper main.223","card_image_path":"static/images/papers/main.223.png","content":{"abstract":"In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.","authors":["Tim Van de Cruys"],"demo_url":"","keywords":["Automatic Generation","predictive modeling","poetry generation","generation poems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.223.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.223","similar_paper_uids":["main.223","main.535","main.25","srw.17","main.354"],"title":"Automatic Poetry Generation from Prosaic Text","tldr":"In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to pre...","track":"Generation"},"forum":"main.223","id":"main.223","presentation_id":"38929130"},{"card_image_alt_text":"A representative figure from paper main.169","card_image_path":"static/images/papers/main.169.png","content":{"abstract":"This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.","authors":["Aman Madaan","Amrith Setlur","Tanmay Parekh","Barnabas Poczos","Graham Neubig","Yiming Yang","Ruslan Salakhutdinov","Alan W Black","Shrimai Prabhumoye"],"demo_url":"","keywords":["Politeness Transfer","politeness","transfer tasks","content preservation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.169.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.169","similar_paper_uids":["main.169","main.639","main.100","srw.17","main.673"],"title":"Politeness Transfer: A Tag and Generate Approach","tldr":"This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to enc...","track":"Generation"},"forum":"main.169","id":"main.169","presentation_id":"38929267"},{"card_image_alt_text":"A representative figure from paper main.355","card_image_path":"static/images/papers/main.355.png","content":{"abstract":"A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. Code is available at https://github.com/h-shahidi/2birds-gen.","authors":["Hamidreza Shahidi","Ming Li","Jimmy Lin"],"demo_url":"","keywords":["Text Generation","NLP tasks","neural generation","Table-to-text generation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.355.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.355","similar_paper_uids":["main.355","main.21","main.18","main.538","main.708"],"title":"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data","tldr":"A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP ...","track":"Generation"},"forum":"main.355","id":"main.355","presentation_id":"38929197"},{"card_image_alt_text":"A representative figure from paper main.354","card_image_path":"static/images/papers/main.354.png","content":{"abstract":"Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for sequential data. Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.","authors":["Inkit Padhi","Pierre Dognin","Ke Bai","C\u00edcero Nogueira dos Santos","Vijil Chenthamarakshan","Youssef Mroueh","Payel Das"],"demo_url":"","keywords":["Implicit Generation","generation tasks","unconditional generation","class-conditional generation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.354.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.354","similar_paper_uids":["main.354","main.639","main.169","main.673","main.705"],"title":"Learning Implicit Text Generation via Feature Matching","tldr":"Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations ...","track":"Generation"},"forum":"main.354","id":"main.354","presentation_id":"38929180"},{"card_image_alt_text":"A representative figure from paper tacl.1967","card_image_path":"static/images/papers/tacl.1967.png","content":{"abstract":"Given a sentence (e.g., \"I like mangoes\") and a constraint (e.g., negative sentiment), the goal of controlled text generation is to produce a sentence that adapts the input sentence to meet the requirements of the constraint (e.g., \"I hate mangoes\"). Going beyond such simple constraints, recent works have started exploring the incorporation of complex syntactic-guidance as constraints in the task of controlled paraphrase generation. In these methods, syntactic-guidance is sourced from a separate exemplar sentence. However, these prior works have only utilized limited syntactic information available in the parse tree of the exemplar sentence. We address this limitation in the paper and propose Syntax Guided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic paraphrase generation. We find that S GCP can generate syntax-conforming sentences while not compromising on relevance. We perform extensive automated and human evaluations over multiple real-world datasets to demonstrate the efficacy of SGCP over state-of-the-art baselines. To drive future research, we have made SGCP \u2019s source code available.","authors":["Ashutosh Kumar","Kabir Ahuja","Raghuram Vadapalli","Partha Talukdar"],"demo_url":"","keywords":["Syntax-guided Paraphrases","controlled generation","syntactic generation","automated evaluations"],"paper_type":"TACL","pdf_url":"https://arxiv.org/abs/2005.08417","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://arxiv.org/abs/2005.08417","similar_paper_uids":["tacl.1967","main.60","main.22","main.226","main.27"],"title":"Syntax-guided Controlled Generation of Paraphrases","tldr":"Given a sentence (e.g., \"I like mangoes\") and a constraint (e.g., negative sentiment), the goal of controlled text generation is to produce a sentence that adapts the input sentence to meet the requirements of the constraint (e.g., \"I hate mangoes\")....","track":"Generation"},"forum":"tacl.1967","id":"tacl.1967","presentation_id":"38929513"},{"card_image_alt_text":"A representative figure from paper tacl.1849","card_image_path":"static/images/papers/tacl.1849.png","content":{"abstract":"Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion.","authors":["Sascha Rothe","Shashi Narayan and Aliaksei Severyn"],"demo_url":"","keywords":["Sequence Tasks","Natural Processing","Natural tasks","Sequence Generation"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00313","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00313","similar_paper_uids":["tacl.1849","main.247","main.197","main.451","main.518"],"title":"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks","tldr":"Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving ...","track":"Generation"},"forum":"tacl.1849","id":"tacl.1849","presentation_id":"38929500"},{"card_image_alt_text":"A representative figure from paper tacl.1886","card_image_path":"static/images/papers/tacl.1886.png","content":{"abstract":"Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling \ufb02uency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic con\ufb02icts, and lack of long-range coherence in generated stories. We conjecture that this is because of the dif\ufb01culty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during \ufb01ne-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.","authors":["Jian Guan","Fei Huang","Minlie Huang","Zhihao Zhao","Xiaoyan Zhu"],"demo_url":"","keywords":["Commonsense Generation","Story generation","generating story","Automatic evaluation"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00302","sessions":[{"end_time":"Wed, 08 Jul 2020 18:00:00 GMT","session_name":"14A","start_time":"Wed, 08 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00302","similar_paper_uids":["tacl.1886","main.178","main.133","main.515","main.711"],"title":"A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation","tldr":"Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling \ufb02uency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer...","track":"Generation"},"forum":"tacl.1886","id":"tacl.1886","presentation_id":"38929505"}]
