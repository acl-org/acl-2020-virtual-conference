[{"card_image_alt_text":"A representative figure from paper main.46","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.46.png","content":{"abstract":"This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.","authors":["Rapha\u00ebl Bailly","Kata G\u00e1bor"],"demo_url":"","keywords":["Syntax","optimization process","syntax","corpus guidance"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.46.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.46","similar_paper_uids":["main.46","main.234","main.684","cl.1554","main.160"],"title":"Emergence of Syntax Needs Minimal Supervision","tldr":"This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammati...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"main.46","id":"main.46","presentation_id":"38929115"},{"card_image_alt_text":"A representative figure from paper main.47","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.47.png","content":{"abstract":"We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.","authors":["Tatsuki Kuribayashi","Takumi Ito","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["Evaluator Hypotheses","analyzing order","Language Models","neural models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.47.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.47","similar_paper_uids":["main.47","main.389","main.176","main.415","demo.84"],"title":"Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese","tldr":"We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-b...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"main.47","id":"main.47","presentation_id":"38928812"},{"card_image_alt_text":"A representative figure from paper main.45","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.45.png","content":{"abstract":"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the S\u00f8rensen\u2013Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.","authors":["Xiaoya Li","Xiaofei Sun","Yuxian Meng","Junjun Liang","Fei Wu","Jiwei Li"],"demo_url":"","keywords":["Data-imbalanced Tasks","NLP tasks","tagging","machine comprehension"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.45.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.45","similar_paper_uids":["main.45","main.200","main.247","main.590","main.76"],"title":"Dice Loss for Data-imbalanced NLP Tasks","tldr":"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The m...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"main.45","id":"main.45","presentation_id":"38928694"},{"card_image_alt_text":"A representative figure from paper main.44","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.44.png","content":{"abstract":"We present that, the rank-frequency relation in textual data follows f \u221d r^{-\u0251}(r+\u0263)^{-\u03b2}, where f is the token frequency and r is the rank by frequency, with (\u0251, \u03b2, \u0263) as parameters. The formulation is derived based on the empirical observation that d^2 (x+y)/dx^2 is a typical impulse function, where (x,y)=(\\log r, \\log f). The formulation is the power law when \\mbox{\u03b2=0} and the Zipf--Mandelbrot law when \\mbox{\u0251=0}. We illustrate that \u0251 is related to the analytic features of syntax and \u03b2+\u0263 to those of morphology in natural languages from an investigation of multilingual corpora.","authors":["Chenchen Ding","Masao Utiyama","Eiichiro Sumita"],"demo_url":"","keywords":["Three-Parameter Relation","rank-frequency relation","f","token frequency"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.44.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.44","similar_paper_uids":["main.44","main.459","main.176","main.747","main.313"],"title":"A Three-Parameter Rank-Frequency Relation in Natural Languages","tldr":"We present that, the rank-frequency relation in textual data follows f \u221d r^{-\u0251}(r+\u0263)^{-\u03b2}, where f is the token frequency and r is the rank by frequency, with (\u0251, \u03b2, \u0263) as parameters. The formulation is derived based on the empirical observation that...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"main.44","id":"main.44","presentation_id":"38928699"},{"card_image_alt_text":"A representative figure from paper main.43","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.43.png","content":{"abstract":"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN's memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models' expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of ``saturated\" RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. We provide empirical results to support this conjecture. Experimental findings from training unsaturated networks on formal languages support this conjecture.","authors":["William Merrill","Gail Weiss","Yoav Goldberg","Roy Schwartz","Noah A. Smith","Eran Yahav"],"demo_url":"","keywords":["Formal Architectures","RNN architectures","weighted machine","LSTM"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.43.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.43","similar_paper_uids":["main.43","tacl.1709","tacl.1815","main.428","srw.127"],"title":"A Formal Hierarchy of RNN Architectures","tldr":"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN's memory, and rational recurrence, defined as whether the recurrent update can be...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"main.43","id":"main.43","presentation_id":"38928908"},{"card_image_alt_text":"A representative figure from paper tacl.1815","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1815.png","content":{"abstract":"Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.","authors":["Michael Hahn"],"demo_url":"","keywords":["NLP","Self-Attention Models","Neural Models","Transformers"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00306","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00306","similar_paper_uids":["tacl.1815","main.687","srw.115","main.385","main.411"],"title":"Theoretical Limitations of Self-Attention in Neural Sequence Models","tldr":"Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-att...","track":"Theory and Formalism in NLP (Linguistic and Mathematical)"},"forum":"tacl.1815","id":"tacl.1815","presentation_id":"38929496"}]
