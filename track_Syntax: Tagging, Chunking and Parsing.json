[{"card_image_alt_text":"A representative figure from paper main.775","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.775.png","content":{"abstract":"An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (\u201cWells Fargo\u201d) and dates (\u201cJuly 5, 2020\u201d) as well as certain productive constructions (\u201cblow for blow\u201d, \u201cday after day\u201d). Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information. We empirically compare these two common strategies\u2014parsing and tagging\u2014for predicting flat MWEs. Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers.","authors":["Tianze Shi","Lillian Lee"],"demo_url":"","keywords":["parsing","tagging","predicting MWEs","identifying MWEs"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.775.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.775","similar_paper_uids":["main.775","main.587","main.735","main.591","main.259"],"title":"Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches","tldr":"An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (\u201cWells Fargo\u201d) and dates (\u201cJuly 5, 2020\u201d) as well as certai...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.775","id":"main.775","presentation_id":"38928960"},{"card_image_alt_text":"A representative figure from paper main.776","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.776.png","content":{"abstract":"Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree. We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.","authors":["Erick Fonseca","Andr\u00e9 F. T. Martins"],"demo_url":"","keywords":["Higher-Order Parsers","Neural encoders","dependency parsers","higher-order models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.776.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.776","similar_paper_uids":["main.776","demo.46","main.195","main.250","main.89"],"title":"Revisiting Higher-Order Dependency Parsers","tldr":"Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoder...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.776","id":"main.776","presentation_id":"38929446"},{"card_image_alt_text":"A representative figure from paper main.777","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.777.png","content":{"abstract":"Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF). CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures. In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF. Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings.","authors":["Luoxin Chen","Weitong Ruan","Xinyue Liu","Jianhua Lu"],"demo_url":"","keywords":["Semi-Supervised Labeling","supervised settings","image classification","image tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.777.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.777","similar_paper_uids":["main.777","main.138","main.519","srw.123","main.627"],"title":"SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling","tldr":"Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, ...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.777","id":"main.777","presentation_id":"38929132"},{"card_image_alt_text":"A representative figure from paper main.377","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.377.png","content":{"abstract":"A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs). The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules. In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.","authors":["Yajie Ye","Weiwei Sun"],"demo_url":"","keywords":["graph parsing","exact parsing","Graph Parsing","graph-based representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.377.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.377","similar_paper_uids":["main.377","main.67","main.224","main.605","main.640"],"title":"Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis","tldr":"A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. We demonstrate, for the first time, that exact graph parsing can be effici...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.377","id":"main.377","presentation_id":"38929406"},{"card_image_alt_text":"A representative figure from paper main.376","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.376.png","content":{"abstract":"Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)'s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.","authors":["Daniel Fern\u00e1ndez-Gonz\u00e1lez","Carlos G\u00f3mez-Rodr\u00edguez"],"demo_url":"","keywords":["Sequence-to-sequence parsing","Enriched Linearization","Faster Parsing","Top-down linearizations"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.376.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.376","similar_paper_uids":["main.376","main.629","tacl.1805","main.13","main.777"],"title":"Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing","tldr":"Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show t...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.376","id":"main.376","presentation_id":"38928747"},{"card_image_alt_text":"A representative figure from paper main.375","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.375.png","content":{"abstract":"Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD --- with interesting variations across languages and layers --- and that the strength of this preference is correlated with differences in tree shape.","authors":["Artur Kulmizev","Vinit Ravishankar","Mostafa Abdou","Joakim Nivre"],"demo_url":"","keywords":["probing","Neural Models","deep models","linguistic formalism"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.375.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.375","similar_paper_uids":["main.375","tacl.1892","main.109","cl.1482","main.303"],"title":"Do Neural Language Models Show Preferences for Syntactic Formalisms?","tldr":"Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a sing...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.375","id":"main.375","presentation_id":"38929419"},{"card_image_alt_text":"A representative figure from paper main.299","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.299.png","content":{"abstract":"We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.","authors":["Yang Wei","Yuanbin Wu","Man Lan"],"demo_url":"","keywords":["PTB","CTB","Span-based Linearization","linearization tree"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.299.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.299","similar_paper_uids":["main.299","tacl.1853","main.301","main.622","main.571"],"title":"A Span-based Linearization for Constituent Trees","tldr":"We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from ...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.299","id":"main.299","presentation_id":"38928752"},{"card_image_alt_text":"A representative figure from paper main.304","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.304.png","content":{"abstract":"Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student's and the teachers' structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.","authors":["Xinyu Wang","Yong Jiang","Nguyen Bach","Tao Wang","Fei Huang","Kewei Tu"],"demo_url":"","keywords":["Multilingual Labeling","predicting sequences","online serving","Structure-Level Distillation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.304.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.304","similar_paper_uids":["main.304","main.150","main.252","main.148","main.324"],"title":"Structure-Level Knowledge Distillation For Multilingual Sequence Labeling","tldr":"Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easi...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.304","id":"main.304","presentation_id":"38928856"},{"card_image_alt_text":"A representative figure from paper main.300","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.300.png","content":{"abstract":"Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics. In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods. We then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on English and Japanese, two languages with different branching tendencies. We find that recent models do not show a clear advantage over decade-old models in our experiments. We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing.","authors":["Jun Li","Yifan Cao","Jiong Cai","Yong Jiang","Kewei Tu"],"demo_url":"","keywords":["data preprocessing","empirical parsing","unsupervised parsing","Unsupervised Methods"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.300.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.300","similar_paper_uids":["main.300","tacl.1811","main.600","main.301","main.764"],"title":"An Empirical Comparison of Unsupervised Constituency Parsing Methods","tldr":"Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental res...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.300","id":"main.300","presentation_id":"38929344"},{"card_image_alt_text":"A representative figure from paper main.301","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.301.png","content":{"abstract":"We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.","authors":["Thanh-Tung Nguyen","Xuan-Phi Nguyen","Shafiq Joty","Xiaoli Li"],"demo_url":"","keywords":["parsing problem","pointing tasks","English task","SPMRL tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.301.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.301","similar_paper_uids":["main.301","main.299","main.197","tacl.1853","main.119"],"title":"Efficient Constituency Parsing by Pointing","tldr":"We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the b...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.301","id":"main.301","presentation_id":"38928813"},{"card_image_alt_text":"A representative figure from paper main.303","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.303.png","content":{"abstract":"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.","authors":["Michael Lepori","Tal Linzen","R. Thomas McCoy"],"demo_url":"","keywords":["syntactic tasks","subject-verb task","fine-tuning","data augmentation"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.303.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.303","similar_paper_uids":["main.303","tacl.1892","main.158","main.212","main.591"],"title":"Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs","tldr":"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, ...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.303","id":"main.303","presentation_id":"38928898"},{"card_image_alt_text":"A representative figure from paper main.302","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.302.png","content":{"abstract":"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar.","authors":["Yu Zhang","Zhenghua Li","Min Zhang"],"demo_url":"","keywords":["Neural Parsing","deep era","context representation","direct operation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.302.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.302","similar_paper_uids":["main.302","main.358","main.290","tacl.1876","main.135"],"title":"Efficient Second-Order TreeCRF for Neural Dependency Parsing","tldr":"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.302","id":"main.302","presentation_id":"38928756"},{"card_image_alt_text":"A representative figure from paper main.378","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.378.png","content":{"abstract":"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.","authors":["Milo\u0161 Stanojevi\u0107","Mark Steedman"],"demo_url":"","keywords":["Incremental parsing","human processing","ASR","MT"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.378.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.378","similar_paper_uids":["main.378","main.326","main.773","main.629","main.488"],"title":"Max-Margin Incremental CCG Parsing","tldr":"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effo...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.378","id":"main.378","presentation_id":"38928795"},{"card_image_alt_text":"A representative figure from paper main.557","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.557.png","content":{"abstract":"We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word's tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.","authors":["Nikita Kitaev","Dan Klein"],"demo_url":"","keywords":["Tetra-Tagging","Word-Synchronous Parsing","Linear-Time Inference","constituency algorithm"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.557.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.557","similar_paper_uids":["main.557","main.301","main.376","main.364","main.441"],"title":"Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference","tldr":"We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word's tags in parallel, with minimal task-...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.557","id":"main.557","presentation_id":"38929445"},{"card_image_alt_text":"A representative figure from paper main.379","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.379.png","content":{"abstract":"Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology. In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech. In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs). We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech. We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.","authors":["Bich-Ngoc Do","Ines Rehbein"],"demo_url":"","keywords":["Neural Reranking","Dependency Parsing","reranking","neural rerankers"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.379.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.379","similar_paper_uids":["main.379","main.607","main.482","main.142","main.571"],"title":"Neural Reranking for Dependency Parsing: An Evaluation","tldr":"Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configu...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.379","id":"main.379","presentation_id":"38929031"},{"card_image_alt_text":"A representative figure from paper main.778","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/main.778.png","content":{"abstract":"A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time. We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations. We show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set.","authors":["Joachim Wagner","James Barry","Jennifer Foster"],"demo_url":"","keywords":["Out-of-domain Parsing","monolingual parsing","Treebank Vectors","treebank vector"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.778.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.778","similar_paper_uids":["main.778","main.591","tacl.1766","main.198","main.690"],"title":"Treebank Embedding Vectors for Out-of-domain Dependency Parsing","tldr":"A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"main.778","id":"main.778","presentation_id":"38929016"},{"card_image_alt_text":"A representative figure from paper cl.1554","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/cl.1554.png","content":{"abstract":"Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering tools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand-annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.","authors":["Aarne Ranta","Krasimir Angelov","Normunds Gruzitis","Prasanth Kolachina"],"demo_url":"","keywords":["Abstract Syntax","controlled implementations","accurate generation","accurate translation"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00378","sessions":[{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 13:00:00 GMT","session_name":"8A","start_time":"Tue, 07 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/coli_a_00378","similar_paper_uids":["cl.1554","main.46","main.354","srw.144","main.260"],"title":"Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines","tldr":"Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, whe...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"cl.1554","id":"cl.1554","presentation_id":"38929483"},{"card_image_alt_text":"A representative figure from paper tacl.1801","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1801.png","content":{"abstract":"Neural dependency parsing has proven very effective, achieving state-of-the-art results on numerous domains and languages. Unfortunately, it requires large amounts of labeled data, that is costly and laborious to create. In this paper we propose a self-training algorithm that alleviates this annotation bottleneck by training a parser on its own output. Our Deep Contextualized Selftraining (DCST) algorithm utilizes representation models trained on sequence labeling tasks that are derived from the parser\u2019s output when applied to unlabeled data, and integrates these models with the base parserthrough a gating mechanism. We conduct experiments across multiple languages, both in low resource in-domain and in cross-domain setups, and demonstrate that DCST substantially outperforms traditional self-training as well as recent semi-supervised training methods.","authors":["Guy Rotman","Roi Reichart"],"demo_url":"","keywords":["Low Parsing","sequence tasks","Deep Self-training","Neural parsing"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00294","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 22:00:00 GMT","session_name":"15B","start_time":"Wed, 08 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00294","similar_paper_uids":["tacl.1801","main.346","main.272","main.370","main.681"],"title":"Deep Contextualized Self-training for Low Resource Dependency Parsing ","tldr":"Neural dependency parsing has proven very effective, achieving state-of-the-art results on numerous domains and languages. Unfortunately, it requires large amounts of labeled data, that is costly and laborious to create. In this paper we propose a se...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"tacl.1801","id":"tacl.1801","presentation_id":"38929493"},{"card_image_alt_text":"A representative figure from paper tacl.1876","card_image_path":"https://acl2020-public.s3.amazonaws.com/papers/tacl.1876.png","content":{"abstract":"Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined on word-level. Therefore, word segmentation is the precondition of dependency parsing, which makes dependency parsing suffer from error propagation and unable to directly make use of the character-level pre-trained language model (such as BERT). In this paper, we propose a graph-based model to integrate Chinese word segmentation and dependency parsing. Different from previous transition-based joint models, our proposed model is more concise, which results in fewer efforts of feature engineering. Our graph-based joint model achieves better performance than previous joint models and state-of-the-art results in both Chinese word segmentation and dependency parsing. Besides, when BERT is combined, our model can substantially reduce the performance gap of dependency parsing between joint models and gold-segmented word-based models. Our code is publicly available at https://github.com/fastnlp/JointCwsParser.","authors":["Hang Yan","Xipeng Qiu","Xuanjing Huang"],"demo_url":"","keywords":["Joint Segmentation","Joint Parsing","Chinese segmentation","dependency parsing"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00301","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 09:00:00 GMT","session_name":"7A","start_time":"Tue, 07 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00301","similar_paper_uids":["tacl.1876","main.391","main.315","demo.69","demo.67"],"title":"A Graph-based Model for Joint Chinese Word Segmentation and Dependency Parsing","tldr":"Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined on word-level. Therefore, word segmentation is the precondition of dependency parsing, which makes d...","track":"Syntax: Tagging, Chunking and Parsing"},"forum":"tacl.1876","id":"tacl.1876","presentation_id":"38929503"}]
