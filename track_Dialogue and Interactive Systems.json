[{"card_image_alt_text":"A representative figure from paper main.8","card_image_path":"static/images/papers/main.8.png","content":{"abstract":"Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent's persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person's speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor's persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.","authors":["Alex Boyd","Raul Puri","Mohammad Shoeybi","Mostofa Patwary","Bryan Catanzaro"],"demo_url":"","keywords":["Large Modeling","generation","style matching","automatic evaluations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.8.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.8","similar_paper_uids":["main.8","main.516","main.635","demo.37","cl.1508"],"title":"Large Scale Multi-Actor Generative Dialog Modeling","tldr":"Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper ad...","track":"Dialogue and Interactive Systems"},"forum":"main.8","id":"main.8","presentation_id":"38928984"},{"card_image_alt_text":"A representative figure from paper main.52","card_image_path":"static/images/papers/main.52.png","content":{"abstract":"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.","authors":["Lei Shen","Yang Feng"],"demo_url":"","keywords":["Emotion-Controllable Generation","training process","response tasks","CDL"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.52.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.52","similar_paper_uids":["main.52","main.185","main.55","main.288","main.638"],"title":"CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation","tldr":"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cr...","track":"Dialogue and Interactive Systems"},"forum":"main.52","id":"main.52","presentation_id":"38929331"},{"card_image_alt_text":"A representative figure from paper main.563","card_image_path":"static/images/papers/main.563.png","content":{"abstract":"Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).","authors":["Yong Shan","Zekang Li","Jinchao Zhang","Fandong Meng","Yang Feng","Cheng Niu","Jie Zhou"],"demo_url":"","keywords":["Dialogue Tracking","slot problem","Contextual Network","DST"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.563.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.563","similar_paper_uids":["main.563","main.5","main.567","main.53","main.3"],"title":"A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking","tldr":"Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of...","track":"Dialogue and Interactive Systems"},"forum":"main.563","id":"main.563","presentation_id":"38929047"},{"card_image_alt_text":"A representative figure from paper main.166","card_image_path":"static/images/papers/main.166.png","content":{"abstract":"To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent ``what to say'' and ``how to say'', and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation. In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy. Results on two benchmark corpora demonstrate the effectiveness of this framework.","authors":["Jun Xu","Haifeng Wang","Zheng-Yu Niu","Hua Wu","Wanxiang Che","Ting Liu"],"demo_url":"","keywords":["Conversational Learning","Open-Domain Generation","policy learning","dialog planning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.166.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.166","similar_paper_uids":["main.166","main.59","main.60","main.98","demo.79"],"title":"Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation","tldr":"To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and control...","track":"Dialogue and Interactive Systems"},"forum":"main.166","id":"main.166","presentation_id":"38928875"},{"card_image_alt_text":"A representative figure from paper main.9","card_image_path":"static/images/papers/main.9.png","content":{"abstract":"Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.","authors":["Siqi Bao","Huang He","Fan Wang","Hua Wu","Haifeng Wang"],"demo_url":"","keywords":["natural tasks","conversational answering","language generation","one-to-many problem"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.9.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.9","similar_paper_uids":["main.9","main.52","main.226","main.638","main.694"],"title":"PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable","tldr":"Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, know...","track":"Dialogue and Interactive Systems"},"forum":"main.9","id":"main.9","presentation_id":"38928777"},{"card_image_alt_text":"A representative figure from paper main.53","card_image_path":"static/images/papers/main.53.png","content":{"abstract":"Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.","authors":["Sungdong Kim","Sohee Yang","Gyuwan Kim","Sang-Woo Lee"],"demo_url":"","keywords":["Dialogue Tracking","predicting operation","training","open setting"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.53.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.53","similar_paper_uids":["main.53","main.5","main.636","main.563","main.637"],"title":"Efficient Dialogue State Tracking by Selectively Overwriting Memory","tldr":"Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue stat...","track":"Dialogue and Interactive Systems"},"forum":"main.53","id":"main.53","presentation_id":"38929359"},{"card_image_alt_text":"A representative figure from paper main.428","card_image_path":"static/images/papers/main.428.png","content":{"abstract":"Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.","authors":["Margaret Li","Stephen Roller","Ilia Kulikov","Sean Welleck","Y-Lan Boureau","Kyunghyun Cho","Jason Weston"],"demo_url":"","keywords":["dialogue tasks","Unlikelihood Training","Generative models","maximum training"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.428.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.428","similar_paper_uids":["main.428","main.173","main.66","main.499","main.768"],"title":"Don\u2019t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training","tldr":"Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within ut...","track":"Dialogue and Interactive Systems"},"forum":"main.428","id":"main.428","presentation_id":"38929090"},{"card_image_alt_text":"A representative figure from paper main.54","card_image_path":"static/images/papers/main.54.png","content":{"abstract":"The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).","authors":["Donghoon Ham","Jeong-Gwan Lee","Youngsoo Jang","Kee-Eung Kim"],"demo_url":"","keywords":["tracking flow","dialogue systems","human evaluation","End-to-End Pipeline"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.54.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.54","similar_paper_uids":["main.54","demo.49","main.221","main.638","main.62"],"title":"End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2","tldr":"The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pi...","track":"Dialogue and Interactive Systems"},"forum":"main.54","id":"main.54","presentation_id":"38929379"},{"card_image_alt_text":"A representative figure from paper main.565","card_image_path":"static/images/papers/main.565.png","content":{"abstract":"Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.","authors":["Libo Qin","Xiao Xu","Wanxiang Che","Yue Zhang","Ting Liu"],"demo_url":"","keywords":["Multi-Domain Dialog","end-to-end system","navigation","scheduling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.565.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.565","similar_paper_uids":["main.565","main.370","main.165","main.681","main.18"],"title":"Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog","tldr":"Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This ma...","track":"Dialogue and Interactive Systems"},"forum":"main.565","id":"main.565","presentation_id":"38929065"},{"card_image_alt_text":"A representative figure from paper main.564","card_image_path":"static/images/papers/main.564.png","content":{"abstract":"Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.","authors":["Hengyi Cai","Hongshen Chen","Yonghao Song","Cheng Zhang","Xiaofang Zhao","Dawei Yin"],"demo_url":"","keywords":["Data Manipulation","Neural Generation","learning","dialogue generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.564.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.564","similar_paper_uids":["main.564","demo.87","srw.58","main.516","main.173"],"title":"Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight","tldr":"Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended na...","track":"Dialogue and Interactive Systems"},"forum":"main.564","id":"main.564","presentation_id":"38928992"},{"card_image_alt_text":"A representative figure from paper main.55","card_image_path":"static/images/papers/main.55.png","content":{"abstract":"Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.","authors":["Shiki Sato","Reina Akama","Hiroki Ouchi","Jun Suzuki","Kentaro Inui"],"demo_url":"","keywords":["open-domain systems","human evaluation","Dialogue Systems","Response Selection"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.55.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.55","similar_paper_uids":["main.55","main.568","main.638","main.185","main.221"],"title":"Evaluating Dialogue Generation Systems via Response Selection","tldr":"Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response se...","track":"Dialogue and Interactive Systems"},"forum":"main.55","id":"main.55","presentation_id":"38928930"},{"card_image_alt_text":"A representative figure from paper main.57","card_image_path":"static/images/papers/main.57.png","content":{"abstract":"Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.","authors":["Yinpei Dai","Hangyu Li","Chengguang Tang","Yongbin Li","Jian Sun","Xiaodan Zhu"],"demo_url":"","keywords":["real-life services","dialog systems","human-machine collaboration","low-resource learning"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.57.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.57","similar_paper_uids":["main.57","main.60","demo.79","main.98","main.64"],"title":"Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment","tldr":"Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirab...","track":"Dialogue and Interactive Systems"},"forum":"main.57","id":"main.57","presentation_id":"38929454"},{"card_image_alt_text":"A representative figure from paper main.638","card_image_path":"static/images/papers/main.638.png","content":{"abstract":"Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.","authors":["Kai Wang","Junfeng Tian","Rui Wang","Xiaojun Quan","Jianxing Yu"],"demo_url":"","keywords":["Generating responses","task-oriented systems","response generation","automatic evaluations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.638.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.638","similar_paper_uids":["main.638","main.568","main.55","main.221","demo.37"],"title":"Multi-Domain Dialogue Acts and Response Co-Generation","tldr":"Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two s...","track":"Dialogue and Interactive Systems"},"forum":"main.638","id":"main.638","presentation_id":"38928730"},{"card_image_alt_text":"A representative figure from paper main.566","card_image_path":"static/images/papers/main.566.png","content":{"abstract":"Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. Human demonstrations can be used to accelerate learning progress. However, how to effectively leverage demonstrations to learn dialogue policy remains less explored. In this paper, we present S^2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space. Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. The effectiveness of the proposed S^2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.","authors":["Huimin Wang","Baolin Peng","Kam-Fai Wong"],"demo_url":"","keywords":["Demonstrations","learning progress","domain task","human evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.566.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.566","similar_paper_uids":["main.566","main.62","main.59","main.166","tacl.1901"],"title":"Learning Efficient Dialogue Policy from Demonstrations through Shaping","tldr":"Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. Human demonstrations can be used to accelerate learning progress. However, how to effectively ...","track":"Dialogue and Interactive Systems"},"forum":"main.566","id":"main.566","presentation_id":"38928995"},{"card_image_alt_text":"A representative figure from paper main.567","card_image_path":"static/images/papers/main.567.png","content":{"abstract":"Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information\u2019s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.","authors":["Jiaying Hu","Yan Yang","Chencai Chen","liang he","Zhou Yu"],"demo_url":"","keywords":["Dialogue Tracking","long tracking","SAS","Slot Sharing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.567.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.567","similar_paper_uids":["main.567","main.563","main.5","main.637","main.10"],"title":"SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing","tldr":"Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracke...","track":"Dialogue and Interactive Systems"},"forum":"main.567","id":"main.567","presentation_id":"38929216"},{"card_image_alt_text":"A representative figure from paper main.163","card_image_path":"static/images/papers/main.163.png","content":{"abstract":"Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.","authors":["Bo-Hsiang Tseng","Jianpeng Cheng","Yimai Fang","David Vandyke"],"demo_url":"","keywords":["Joint Generation","Natural understanding","natural generation","NLG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.163.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.163","similar_paper_uids":["main.163","main.63","main.559","main.708","main.542"],"title":"A Generative Model for Joint Natural Language Understanding and Generation","tldr":"Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal repre...","track":"Dialogue and Interactive Systems"},"forum":"main.163","id":"main.163","presentation_id":"38929001"},{"card_image_alt_text":"A representative figure from paper main.56","card_image_path":"static/images/papers/main.56.png","content":{"abstract":"Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.","authors":["Yefei Zha","Ruobing Li","Hui Lin"],"demo_url":"","keywords":["Off-topic Detection","automated system","real-world applications","detecting responses"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.56.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.56","similar_paper_uids":["main.56","main.185","main.52","main.568","main.55"],"title":"Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection","tldr":"Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken r...","track":"Dialogue and Interactive Systems"},"forum":"main.56","id":"main.56","presentation_id":"38929413"},{"card_image_alt_text":"A representative figure from paper main.515","card_image_path":"static/images/papers/main.515.png","content":{"abstract":"Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.","authors":["Sixing Wu","Ying Li","Dawei Zhang","Yang Zhou","Zhonghai Wu"],"demo_url":"","keywords":["Diverse Generation","dialogue generation","Context-Specific Awareness","Generative systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.515.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.515","similar_paper_uids":["main.515","main.6","main.637","main.546"],"title":"Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness","tldr":"Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works t...","track":"Dialogue and Interactive Systems"},"forum":"main.515","id":"main.515","presentation_id":"38928749"},{"card_image_alt_text":"A representative figure from paper main.517","card_image_path":"static/images/papers/main.517.png","content":{"abstract":"Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. In our approach, each dialogue model consists of a shared module, a gating module, and a private module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.","authors":["Yiping Song","Zequn Liu","Wei Bi","Rui Yan","Ming Zhang"],"demo_url":"","keywords":["Few-shot Tasks","open-domain systems","generative models","meta-learning framework"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.517.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.517","similar_paper_uids":["main.517","main.638","main.197","main.542","main.127"],"title":"Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks","tldr":"Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine...","track":"Dialogue and Interactive Systems"},"forum":"main.517","id":"main.517","presentation_id":"38928970"},{"card_image_alt_text":"A representative figure from paper main.516","card_image_path":"static/images/papers/main.516.png","content":{"abstract":"Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.","authors":["Haoyu Song","Yan Wang","Wei-Nan Zhang","Xiaojiang Liu","Ting Liu"],"demo_url":"","keywords":["Persona Generation","persona-based task","personality-inconsistent problem","generating responses"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.516.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.516","similar_paper_uids":["main.516","main.7","main.131","main.8","main.568"],"title":"Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation","tldr":"Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by inc...","track":"Dialogue and Interactive Systems"},"forum":"main.516","id":"main.516","presentation_id":"38928804"},{"card_image_alt_text":"A representative figure from paper main.129","card_image_path":"static/images/papers/main.129.png","content":{"abstract":"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user's requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.","authors":["Gabriel Gordon-Hall","Philip John Gorinski","Shay B. Cohen"],"demo_url":"","keywords":["Weak Demonstrations","dialog manager","multi-domain systems","expert demonstrators"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.129.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.129","similar_paper_uids":["main.129","demo.79","main.59","main.166","main.60"],"title":"Learning Dialog Policies from Weak Demonstrations","tldr":"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorit...","track":"Dialogue and Interactive Systems"},"forum":"main.129","id":"main.129","presentation_id":"38928789"},{"card_image_alt_text":"A representative figure from paper main.128","card_image_path":"static/images/papers/main.128.png","content":{"abstract":"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word\u2019s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model \u2013 TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.","authors":["Yutai Hou","Wanxiang Che","Yongkui Lai","Zhihan Zhou","Yijia Liu","Han Liu","Ting Liu"],"demo_url":"","keywords":["slot tagging","Few-shot tagging","fewshot problems","Few-shot Tagging"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.128.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.128","similar_paper_uids":["main.128","main.436","main.426","demo.87","main.148"],"title":"Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network","tldr":"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies bet...","track":"Dialogue and Interactive Systems"},"forum":"main.128","id":"main.128","presentation_id":"38929212"},{"card_image_alt_text":"A representative figure from paper main.10","card_image_path":"static/images/papers/main.10.png","content":{"abstract":"Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.","authors":["Yangming Li","Kaisheng Yao","Libo Qin","Wanxiang Che","Xiaolong Li","Ting Liu"],"demo_url":"","keywords":["Task-oriented Systems","natural generation","natural NLG","NLG"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.10.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.10","similar_paper_uids":["main.10","main.5","main.563","main.3","main.567"],"title":"Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network","tldr":"Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value....","track":"Dialogue and Interactive Systems"},"forum":"main.10","id":"main.10","presentation_id":"38928822"},{"card_image_alt_text":"A representative figure from paper main.131","card_image_path":"static/images/papers/main.131.png","content":{"abstract":"Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose P^2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P^2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.","authors":["Qian Liu","Yihong Chen","Bei Chen","Jian-Guang Lou","Zixuan Chen","Bin Zhou","Dongmei Zhang"],"demo_url":"","keywords":["Dialogue Generation","mimicking responses","cognitive science","understanding"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.131.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.131","similar_paper_uids":["main.131","main.516","main.7","main.219","main.218"],"title":"You Impress Me: Dialogue Generation via Mutual Persona Perception","tldr":"Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between ...","track":"Dialogue and Interactive Systems"},"forum":"main.131","id":"main.131","presentation_id":"38928693"},{"card_image_alt_text":"A representative figure from paper main.130","card_image_path":"static/images/papers/main.130.png","content":{"abstract":"Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.","authors":["Leyang Cui","Yu Wu","Shujie Liu","Yue Zhang","Ming Zhou"],"demo_url":"","keywords":["Multi-Turn Reasoning","conversation reasoning","conversation research","reasoning problems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.130.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.130","similar_paper_uids":["main.130","demo.37","main.135","main.210","main.586"],"title":"MuTual: A Dataset for Multi-Turn Dialogue Reasoning","tldr":"Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent respo...","track":"Dialogue and Interactive Systems"},"forum":"main.130","id":"main.130","presentation_id":"38928987"},{"card_image_alt_text":"A representative figure from paper main.11","card_image_path":"static/images/papers/main.11.png","content":{"abstract":"We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.","authors":["Samuel Coope","Tyler Farghly","Daniela Gerz","Ivan Vuli\u0107","Matthew Henderson"],"demo_url":"","keywords":["Dialog","dialog slot-filling","turn-based task","few-shot scenarios"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.11.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.11","similar_paper_uids":["main.11","tacl.1853","main.505","main.370","main.585"],"title":"Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations","tldr":"We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational ...","track":"Dialogue and Interactive Systems"},"forum":"main.11","id":"main.11","presentation_id":"38929061"},{"card_image_alt_text":"A representative figure from paper main.126","card_image_path":"static/images/papers/main.126.png","content":{"abstract":"Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.","authors":["Weixin Liang","James Zou","Zhou Yu"],"demo_url":"","keywords":["Automatic Evaluation","Open evaluation","dialog research","dialog evaluation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.126.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.126","similar_paper_uids":["main.126","demo.79","main.60","main.64","main.98"],"title":"Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation","tldr":"Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limite...","track":"Dialogue and Interactive Systems"},"forum":"main.126","id":"main.126","presentation_id":"38928690"},{"card_image_alt_text":"A representative figure from paper main.127","card_image_path":"static/images/papers/main.127.png","content":{"abstract":"Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response.","authors":["Wentao Ma","Yiming Cui","Ting Liu","Dong Wang","Shijin Wang","Guoping Hu"],"demo_url":"","keywords":["Conversational Embedding","Retrieval-Based System","single-turn tasks","retrieval-based systems"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.127.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.127","similar_paper_uids":["main.127","main.568","main.635","main.55","main.517"],"title":"Conversational Word Embedding for Retrieval-Based Dialog System","tldr":"Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to lea...","track":"Dialogue and Interactive Systems"},"forum":"main.127","id":"main.127","presentation_id":"38928772"},{"card_image_alt_text":"A representative figure from paper main.12","card_image_path":"static/images/papers/main.12.png","content":{"abstract":"Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.","authors":["Giovanni Campagna","Agata Foryciarz","Mehrad Moradshahi","Monica Lam"],"demo_url":"","keywords":["Multi-Domain Tracking","data acquisition","dialogue tracking","data augmentation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.12.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.12","similar_paper_uids":["main.12","main.524","main.148","main.636","main.625"],"title":"Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking","tldr":"Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking wh...","track":"Dialogue and Interactive Systems"},"forum":"main.12","id":"main.12","presentation_id":"38929160"},{"card_image_alt_text":"A representative figure from paper main.518","card_image_path":"static/images/papers/main.518.png","content":{"abstract":"Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.","authors":["Hung Le","Steven C.H. Hoi"],"demo_url":"","keywords":["downstream tasks","video-grounded tasks","sequence-to-sequence task","Pretrained Models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.518.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.518","similar_paper_uids":["main.518","main.197","tacl.1849","main.214","demo.37"],"title":"Video-Grounded Dialogues with Pretrained Generation Language Models","tldr":"Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained langu...","track":"Dialogue and Interactive Systems"},"forum":"main.518","id":"main.518","presentation_id":"38928971"},{"card_image_alt_text":"A representative figure from paper main.98","card_image_path":"static/images/papers/main.98.png","content":{"abstract":"We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.","authors":["Zeming Liu","Haifeng Wang","Zheng-Yu Niu","Hua Wu","Wanxiang Che","Ting Liu"],"demo_url":"","keywords":["Conversational Recommendation","multi-type dialogs","recommender","non-recommendation dialog"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.98.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.98","similar_paper_uids":["main.98","demo.79","main.60","main.166","main.64"],"title":"Towards Conversational Recommendation over Multi-Type Dialogs","tldr":"We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account ...","track":"Dialogue and Interactive Systems"},"forum":"main.98","id":"main.98","presentation_id":"38929296"},{"card_image_alt_text":"A representative figure from paper main.634","card_image_path":"static/images/papers/main.634.png","content":{"abstract":"Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation. As bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective. In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text. Compared with bilateral conversations, non-conversational text are easier to obtain, more diverse and cover a much broader range of topics. We collect a large-scale non-conversational corpus from multi sources including forum comments, idioms and book snippets. We further present a training paradigm to effectively incorporate these text via iterative back translation. The resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context.","authors":["Hui Su","Xiaoyu Shen","Sanqiang Zhao","Zhou Xiao","Pengwei Hu","Randy Zhong","Cheng Niu","Jie Zhou"],"demo_url":"","keywords":["Diversifying Generation","low-diversity problem","open-domain generation","dialogue generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.634.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.634","similar_paper_uids":["main.634","demo.37","main.515","main.635","main.19"],"title":"Diversifying Dialogue Generation with Non-Conversational Text","tldr":"Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation. As bland and generic utterances usually dominate the frequency distribution in our daily chitc...","track":"Dialogue and Interactive Systems"},"forum":"main.634","id":"main.634","presentation_id":"38929350"},{"card_image_alt_text":"A representative figure from paper main.185","card_image_path":"static/images/papers/main.185.png","content":{"abstract":"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named ``Negative Training\" to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.","authors":["Tianxing He","James Glass"],"demo_url":"","keywords":["Neural Generation","open-domain generation","Negative Training","deep models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.185.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.185","similar_paper_uids":["main.185","main.568","main.55","main.52","main.19"],"title":"Negative Training for Neural Dialogue Response Generation","tldr":"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious respon...","track":"Dialogue and Interactive Systems"},"forum":"main.185","id":"main.185","presentation_id":"38928691"},{"card_image_alt_text":"A representative figure from paper main.218","card_image_path":"static/images/papers/main.218.png","content":{"abstract":"Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.","authors":["Hyundong Cho","Jonathan May"],"demo_url":"","keywords":["Grounding Conversations","dialogue systems","bootstrapped classifier","chit-chat systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.218.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.218","similar_paper_uids":["main.218","main.133","main.131","main.220","demo.37"],"title":"Grounding Conversations with Improvised Dialogues","tldr":"Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important a...","track":"Dialogue and Interactive Systems"},"forum":"main.218","id":"main.218","presentation_id":"38928948"},{"card_image_alt_text":"A representative figure from paper main.219","card_image_path":"static/images/papers/main.219.png","content":{"abstract":"To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).","authors":["Kurt Shuster","Samuel Humeau","Antoine Bordes","Jason Weston"],"demo_url":"","keywords":["large-scale architectures","IGC task","neural architectures","image representations"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.219.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.219","similar_paper_uids":["main.219","main.131","main.93","main.164","main.664"],"title":"Image-Chat: Engaging Grounded Conversations","tldr":"To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a...","track":"Dialogue and Interactive Systems"},"forum":"main.219","id":"main.219","presentation_id":"38928905"},{"card_image_alt_text":"A representative figure from paper main.184","card_image_path":"static/images/papers/main.184.png","content":{"abstract":"Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.","authors":["Houyu Zhang","Zhenghao Liu","Chenyan Xiong","Zhiyuan Liu"],"demo_url":"","keywords":["Grounded Generation","conversation model","ConceptFlow","knowledge-aware models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.184.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.184","similar_paper_uids":["main.184","main.717","main.748","main.635","main.470"],"title":"Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs","tldr":"Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By gr...","track":"Dialogue and Interactive Systems"},"forum":"main.184","id":"main.184","presentation_id":"38928828"},{"card_image_alt_text":"A representative figure from paper main.635","card_image_path":"static/images/papers/main.635.png","content":{"abstract":"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.","authors":["Hao Zhou","Chujie Zheng","Kaili Huang","Minlie Huang","Xiaoyan Zhu"],"demo_url":"","keywords":["Multi-turn Conversation","Multi-turn ","knowledge-driven systems","transfer learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.635.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.635","similar_paper_uids":["main.635","main.8","demo.37","main.6","main.127"],"title":"KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation","tldr":"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain k...","track":"Dialogue and Interactive Systems"},"forum":"main.635","id":"main.635","presentation_id":"38928880"},{"card_image_alt_text":"A representative figure from paper main.99","card_image_path":"static/images/papers/main.99.png","content":{"abstract":"User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection. In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.","authors":["Guangfeng Yan","Lu Fan","Qimai Li","Han Liu","Xiaotong Zhang","Xiao-Ming Wu","Albert Y.S. Lam"],"demo_url":"","keywords":["Unknown Detection","Zero-shot Classification","User classification","dialogue systems"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.99.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.99","similar_paper_uids":["main.99","main.102","main.279","main.336","main.12"],"title":"Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification","tldr":"User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This pap...","track":"Dialogue and Interactive Systems"},"forum":"main.99","id":"main.99","presentation_id":"38929387"},{"card_image_alt_text":"A representative figure from paper main.64","card_image_path":"static/images/papers/main.64.png","content":{"abstract":"The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.","authors":["Shikib Mehri","Maxine Eskenazi"],"demo_url":"","keywords":["Dialog Generation","dialog","open-domain research","USR"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.64.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.64","similar_paper_uids":["main.64","demo.79","main.126","main.60","main.98"],"title":"USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation","tldr":"The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSu...","track":"Dialogue and Interactive Systems"},"forum":"main.64","id":"main.64","presentation_id":"38928829"},{"card_image_alt_text":"A representative figure from paper main.58","card_image_path":"static/images/papers/main.58.png","content":{"abstract":"Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.","authors":["Keqing He","Yuanmeng Yan","Weiran XU"],"demo_url":"","keywords":["slot tagging","Contextual Representation","Neural-based models","knowledge-enhanced model"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.58.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.58","similar_paper_uids":["main.58","main.5","main.734","main.735","main.574"],"title":"Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge","tldr":"Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this ...","track":"Dialogue and Interactive Systems"},"forum":"main.58","id":"main.58","presentation_id":"38928871"},{"card_image_alt_text":"A representative figure from paper main.637","card_image_path":"static/images/papers/main.637.png","content":{"abstract":"Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.","authors":["Jun Quan","Deyi Xiong"],"demo_url":"","keywords":["Task-Oriented Generation","auxiliary task","transferable generator","TRADE"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.637.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.637","similar_paper_uids":["main.637","main.567","main.62","main.53","main.638"],"title":"Modeling Long Context for Task-Oriented Dialogue State Generation","tldr":"Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique an...","track":"Dialogue and Interactive Systems"},"forum":"main.637","id":"main.637","presentation_id":"38928877"},{"card_image_alt_text":"A representative figure from paper main.186","card_image_path":"static/images/papers/main.186.png","content":{"abstract":"The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user's request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains. We propose a recursive, hierarchical frame-based representation and show how to learn it from data. We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template. We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end. We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.","authors":["Rashmi Gangadharaiah","Balakrishnan Narayanaswamy"],"demo_url":"","keywords":["Task Dialog","Natural component","task systems","complex scenarios"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.186.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.186","similar_paper_uids":["main.186","main.3","main.126","main.5","demo.79"],"title":"Recursive Template-based Frame Generation for Task Oriented Dialog","tldr":"The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user's request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This infor...","track":"Dialogue and Interactive Systems"},"forum":"main.186","id":"main.186","presentation_id":"38928834"},{"card_image_alt_text":"A representative figure from paper main.568","card_image_path":"static/images/papers/main.568.png","content":{"abstract":"Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context. Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth. One approach to resolve this problem is to consider the similarity of the generated response with the conversational context. In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus. Our approach considers the speakers in defining the different levels of similar context. We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model. Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores. We also show that our model trained on Twitter can be applied to movie dialogues without any additional training. We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models.","authors":["JinYeong Bak","Alice Oh"],"demo_url":"","keywords":["Speaker Model","Automatic generation","open-domain generation","automatic models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.568.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 07:00:00 GMT","session_name":"11B","start_time":"Wed, 08 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.568","similar_paper_uids":["main.568","main.55","main.638","main.185","main.221"],"title":"Speaker Sensitive Response Evaluation Model","tldr":"Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context. Existing evaluation models merely compare the generated response with the ground truth response and...","track":"Dialogue and Interactive Systems"},"forum":"main.568","id":"main.568","presentation_id":"38929430"},{"card_image_alt_text":"A representative figure from paper main.187","card_image_path":"static/images/papers/main.187.png","content":{"abstract":"We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash_dataset.","authors":["Ahmed Elgohary","Saghar Hosseini","Ahmed Hassan Awadallah"],"demo_url":"","keywords":["semantic correction","one-shot translation","correction task","Parser"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.187.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.187","similar_paper_uids":["main.187","main.100","main.18","main.708","main.677"],"title":"Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback","tldr":"We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. I...","track":"Dialogue and Interactive Systems"},"forum":"main.187","id":"main.187","presentation_id":"38928802"},{"card_image_alt_text":"A representative figure from paper main.636","card_image_path":"static/images/papers/main.636.png","content":{"abstract":"A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator. With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus. Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain. We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation. Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.","authors":["Yi Huang","Junlan Feng","Min Hu","Xiaoting Wu","Xiaoyu Du","Shuo Ma"],"demo_url":"","keywords":["Dialogue Systems","multi-domain task","new adaptation","domain adaptation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.636.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 14:00:00 GMT","session_name":"13B","start_time":"Wed, 08 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.636","similar_paper_uids":["main.636","main.53","main.5","main.12","main.129"],"title":"Meta-Reinforced Multi-Domain State Generator for Dialogue Systems","tldr":"A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-dom...","track":"Dialogue and Interactive Systems"},"forum":"main.636","id":"main.636","presentation_id":"38929369"},{"card_image_alt_text":"A representative figure from paper main.3","card_image_path":"static/images/papers/main.3.png","content":{"abstract":"As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach.","authors":["Zihan Liu","Genta Indra Winata","Peng Xu","Pascale Fung"],"demo_url":"","keywords":["Cross-domain Filling","task-oriented systems","slot filling","data problem"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.3.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.3","similar_paper_uids":["main.3","main.5","main.563","main.10","main.567"],"title":"Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling","tldr":"As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcit...","track":"Dialogue and Interactive Systems"},"forum":"main.3","id":"main.3","presentation_id":"38928771"},{"card_image_alt_text":"A representative figure from paper main.59","card_image_path":"static/images/papers/main.59.png","content":{"abstract":"Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.","authors":["Ryuichi Takanobu","Runze Liang","Minlie Huang"],"demo_url":"","keywords":["pretraining","Multi-Agent Learning","Role-Aware Decomposition","reinforcement learning"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.59.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.59","similar_paper_uids":["main.59","main.566","main.166","main.129","main.98"],"title":"Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition","tldr":"Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorith...","track":"Dialogue and Interactive Systems"},"forum":"main.59","id":"main.59","presentation_id":"38928928"},{"card_image_alt_text":"A representative figure from paper main.7","card_image_path":"static/images/papers/main.7.png","content":{"abstract":"Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.","authors":["Bowen Wu","Mengyuan Li","Zongsheng Wang","Yifu Chen","Derek F. Wong","Qihang Feng","Junhong Huang","Baoxun Wang"],"demo_url":"","keywords":["conversational agents","optimization","persona-aware generation","Variational Generator"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.7.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.7","similar_paper_uids":["main.7","main.516","main.8","main.131","main.55"],"title":"Guiding Variational Response Generator to Exploit Persona","tldr":"Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of th...","track":"Dialogue and Interactive Systems"},"forum":"main.7","id":"main.7","presentation_id":"38928775"},{"card_image_alt_text":"A representative figure from paper main.61","card_image_path":"static/images/papers/main.61.png","content":{"abstract":"Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document. In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. In this paper, we propose to create the document memory with some anticipated responses in mind. This is achieved using a teacher-student framework. The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information. The student learns to construct a response-anticipated document memory from the first two sources, and teacher\u2019s insight on memory creation. Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.","authors":["Zhiliang Tian","Wei Bi","Dongkyu Lee","Lanqing Xue","Yiping Song","Xiaojiang Liu","Nevin L. Zhang"],"demo_url":"","keywords":["On-Demand Integration","Response Generation","Conversing Reading","Conversing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.61.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.61","similar_paper_uids":["main.61","main.481","main.141","main.568","main.670"],"title":"Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation","tldr":"Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a giv...","track":"Dialogue and Interactive Systems"},"forum":"main.61","id":"main.61","presentation_id":"38929327"},{"card_image_alt_text":"A representative figure from paper main.183","card_image_path":"static/images/papers/main.183.png","content":{"abstract":"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.","authors":["Eric Michael Smith","Mary Williamson","Kurt Shuster","Jason Weston","Y-Lan Boureau"],"demo_url":"","keywords":["conversational agent","open-domain agent","model schemes","multi-task training"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.183.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.183","similar_paper_uids":["main.183","main.769","main.222","main.86","main.689"],"title":"Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills","tldr":"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can exp...","track":"Dialogue and Interactive Systems"},"forum":"main.183","id":"main.183","presentation_id":"38929292"},{"card_image_alt_text":"A representative figure from paper main.222","card_image_path":"static/images/papers/main.222.png","content":{"abstract":"We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. We show that such multi-tasking improves over a BERT pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings. We obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge.","authors":["Kurt Shuster","Da JU","Stephen Roller","Emily Dinan","Y-Lan Boureau","Jason Weston"],"demo_url":"","keywords":["text tasks","Dialogue Dodecathlon","Image Agents","dodecaDialogue"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.222.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.222","similar_paper_uids":["main.222","main.86","main.402","main.388","main.183"],"title":"The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents","tldr":"We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perce...","track":"Dialogue and Interactive Systems"},"forum":"main.222","id":"main.222","presentation_id":"38928890"},{"card_image_alt_text":"A representative figure from paper main.182","card_image_path":"static/images/papers/main.182.png","content":{"abstract":"This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. We show that instead of retraining models for this specific purpose, we can capture the original retrieval model's underlying confidence concerning the best prediction using trivial additional computation.","authors":["Yulan Feng","Shikib Mehri","Maxine Eskenazi","Tiancheng Zhao"],"demo_url":"","keywords":["Dialog Retrieval","end-to-end tasks","uncertainty classification","retraining models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.182.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.182","similar_paper_uids":["main.182","demo.79","main.98","main.57","main.60"],"title":"\"None of the Above\": Measure Uncertainty in Dialog Response Retrieval","tldr":"This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. We show that instead of retraining models for this spe...","track":"Dialogue and Interactive Systems"},"forum":"main.182","id":"main.182","presentation_id":"38929205"},{"card_image_alt_text":"A representative figure from paper main.60","card_image_path":"static/images/papers/main.60.png","content":{"abstract":"Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.","authors":["Silin Gao","Yichi Zhang","Zhijian Ou","Zhou Yu"],"demo_url":"","keywords":["Paraphrase Generation","dialog tasks","data process","dialog generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.60.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 13:00:00 GMT","session_name":"3A","start_time":"Mon, 06 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.60","similar_paper_uids":["main.60","demo.79","main.98","main.126","main.166"],"title":"Paraphrase Augmented Task-Oriented Dialog Generation","tldr":"Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world se...","track":"Dialogue and Interactive Systems"},"forum":"main.60","id":"main.60","presentation_id":"38928976"},{"card_image_alt_text":"A representative figure from paper main.6","card_image_path":"static/images/papers/main.6.png","content":{"abstract":"Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. Furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution. Our joint neural conversation model which integrates recurrent Knowledge-Interaction and knowledge Copy (KIC) performs well on generating informative responses. Experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets Wizard-of-Wikipedia(average Bleu +87%; abs.: 0.034) and DuConv(average Bleu +20%; abs.: 0.047)) with different knowledge formats (textual & structured) and different languages (English & Chinese).","authors":["Xiexiong Lin","Weiyu Jian","Jianshan He","Taifeng Wang","Wei Chu"],"demo_url":"","keywords":["Generating Response","Knowledge-driven approaches","response steps","knowledge mechanism"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.6.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.6","similar_paper_uids":["main.6","main.515","main.58","main.635","main.744"],"title":"Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy","tldr":"Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To a...","track":"Dialogue and Interactive Systems"},"forum":"main.6","id":"main.6","presentation_id":"38929055"},{"card_image_alt_text":"A representative figure from paper main.427","card_image_path":"static/images/papers/main.427.png","content":{"abstract":"We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft. The dataset consists of 7K human utterances and their corresponding parses. Given proper world state, the parses can be interpreted and executed in game. We report the performance of baseline models, and analyze their successes and failures.","authors":["Kavya Srinet","Yacine Jernite","Jonathan Gray","Arthur Szlam"],"demo_url":"","keywords":["Voxel-World Assistant","instruction-driven communication","CraftAssist Parsing","Semantic Parsing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.427.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 18:00:00 GMT","session_name":"9A","start_time":"Tue, 07 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 21:00:00 GMT","session_name":"10A","start_time":"Tue, 07 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.427","similar_paper_uids":["main.427","main.232","main.187","main.219","main.644"],"title":"CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant","tldr":"We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft. The dataset consists of 7K human utterances and their corresponding parses. Given proper world state, the parses can be interpreted...","track":"Dialogue and Interactive Systems"},"forum":"main.427","id":"main.427","presentation_id":"38929264"},{"card_image_alt_text":"A representative figure from paper main.4","card_image_path":"static/images/papers/main.4.png","content":{"abstract":"Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing.","authors":["Tianyu Zhao","Divesh Lala","Tatsuya Kawahara"],"demo_url":"","keywords":["human evaluation","Precise Evaluators","Automatic evaluator","reference-free evaluator"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.4.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 10:00:00 GMT","session_name":"2B","start_time":"Mon, 06 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.4","similar_paper_uids":["main.4","main.124","main.19","main.220","demo.115"],"title":"Designing Precise and Robust Dialogue Response Evaluators","tldr":"Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, ...","track":"Dialogue and Interactive Systems"},"forum":"main.4","id":"main.4","presentation_id":"38928816"},{"card_image_alt_text":"A representative figure from paper main.62","card_image_path":"static/images/papers/main.62.png","content":{"abstract":"Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.","authors":["Xinting Huang","Jianzhong Qi","Yu Sun","Rui Zhang"],"demo_url":"","keywords":["Semi-Supervised Learning","generalization function","Stochastic Estimation","Dialogue optimization"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.62.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.62","similar_paper_uids":["main.62","main.566","main.637","main.166","tacl.1901"],"title":"Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation","tldr":"Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogue...","track":"Dialogue and Interactive Systems"},"forum":"main.62","id":"main.62","presentation_id":"38929372"},{"card_image_alt_text":"A representative figure from paper main.221","card_image_path":"static/images/papers/main.221.png","content":{"abstract":"The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.","authors":["Matthew Roddy","Naomi Harte"],"demo_url":"","keywords":["Neural Timings","neural models","incremental system","SDS"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.221.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.221","similar_paper_uids":["main.221","main.638","main.568","main.55","main.54"],"title":"Neural Generation of Dialogue Response Timings","tldr":"The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn a...","track":"Dialogue and Interactive Systems"},"forum":"main.221","id":"main.221","presentation_id":"38928926"},{"card_image_alt_text":"A representative figure from paper main.220","card_image_path":"static/images/papers/main.220.png","content":{"abstract":"Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.","authors":["Koustuv Sinha","Prasanna Parthasarathi","Jasmine Wang","Ryan Lowe","William L. Hamilton","Joelle Pineau"],"demo_url":"","keywords":["Online Evaluation","inference","online setting","Unreferenced Metric"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.220.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 19:00:00 GMT","session_name":"4B","start_time":"Mon, 06 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.220","similar_paper_uids":["main.220","main.4","srw.58","demo.37","main.638"],"title":"Learning an Unreferenced Metric for Online Dialogue Evaluation","tldr":"Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not gene...","track":"Dialogue and Interactive Systems"},"forum":"main.220","id":"main.220","presentation_id":"38928843"},{"card_image_alt_text":"A representative figure from paper main.63","card_image_path":"static/images/papers/main.63.png","content":{"abstract":"In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG.","authors":["Shang-Yu Su","Chao-Wei Huang","Yun-Nung Chen"],"demo_url":"","keywords":["Unsupervised Understanding","Unsupervised Generation","natural understanding","natural generation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.63.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 09:00:00 GMT","session_name":"2A","start_time":"Mon, 06 Jul 2020 08:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.63","similar_paper_uids":["main.63","main.163","main.708","main.542","main.290"],"title":"Towards Unsupervised Language Understanding and Generation by Joint Dual Learning","tldr":"In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentence...","track":"Dialogue and Interactive Systems"},"forum":"main.63","id":"main.63","presentation_id":"38929439"},{"card_image_alt_text":"A representative figure from paper main.5","card_image_path":"static/images/papers/main.5.png","content":{"abstract":"Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets.","authors":["Yawen Ouyang","Moxin Chen","Xinyu Dai","Yinggong Zhao","Shujian Huang","Jiajun Chen"],"demo_url":"","keywords":["Dialogue Tracking","dialogue DST","learning reasoning","Explicit Modeling"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.5.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 06:00:00 GMT","session_name":"1A","start_time":"Mon, 06 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.5","similar_paper_uids":["main.5","main.563","main.567","main.3","main.53"],"title":"Dialogue State Tracking with Explicit Slot Connection Modeling","tldr":"Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domain...","track":"Dialogue and Interactive Systems"},"forum":"main.5","id":"main.5","presentation_id":"38929321"},{"card_image_alt_text":"A representative figure from paper cl.1508","card_image_path":"static/images/papers/cl.1508.png","content":{"abstract":"This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human\u2013machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.","authors":["Li Zhou","Jianfeng Gao","Di Li","Heung-Yeung Shum"],"demo_url":"","keywords":["XiaoIce","Microsoft XiaoIce","system design","Markov Processes"],"paper_type":"CL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00368","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 22:00:00 GMT","session_name":"5B","start_time":"Mon, 06 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/COLI_a_00368","similar_paper_uids":["cl.1508","main.8","main.305","main.635","demo.100"],"title":"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot","tldr":"This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, a...","track":"Dialogue and Interactive Systems"},"forum":"cl.1508","id":"cl.1508","presentation_id":"38929478"},{"card_image_alt_text":"A representative figure from paper tacl.1901","card_image_path":"static/images/papers/tacl.1901.png","content":{"abstract":"To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.","authors":["Qi Zhu","Kaili Huang","Zheng Zhang","Xiaoyan Zhu","Minlie Huang"],"demo_url":"","keywords":["multi-domain modeling","pipelined systems","cross-domain modeling","dialogue tracking"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00314","sessions":[{"end_time":"Mon, 06 Jul 2020 07:00:00 GMT","session_name":"1B","start_time":"Mon, 06 Jul 2020 06:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 14:00:00 GMT","session_name":"3B","start_time":"Mon, 06 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00314","similar_paper_uids":["tacl.1901","main.638","main.62","main.524","main.566"],"title":"CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset","tldr":"To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue ...","track":"Dialogue and Interactive Systems"},"forum":"tacl.1901","id":"tacl.1901","presentation_id":"38929507"}]
