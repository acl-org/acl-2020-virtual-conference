[{"card_image_alt_text":"A representative figure from paper main.488","card_image_path":"static/images/papers/main.488.png","content":{"abstract":"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.","authors":["Paul Pu Liang","Irene Mengze Li","Emily Zheng","Yao Chong Lim","Ruslan Salakhutdinov","Louis-Philippe Morency"],"demo_url":"","keywords":["Debiasing Representations","real-world scenarios","legal systems","debiasing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.488.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.488","similar_paper_uids":["main.488","main.405","main.484","main.431","main.486"],"title":"Towards Debiasing Sentence Representations","tldr":"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes....","track":"Ethics and NLP"},"forum":"main.488","id":"main.488","presentation_id":"38928697"},{"card_image_alt_text":"A representative figure from paper main.265","card_image_path":"static/images/papers/main.265.png","content":{"abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","authors":["Andrew Gaut","Tony Sun","Shirlyn Tang","Yuxin Huang","Jing Qian","Mai ElSherief","Jieyu Zhao","Diba Mirza","Elizabeth Belding","Kai-Wei Chang","William Yang Wang"],"demo_url":"","keywords":["Relation Extraction","Automated Construction","name anonymization","data augmentation"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.265.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.265","similar_paper_uids":["main.265","main.690","main.326","main.468","main.484"],"title":"Towards Understanding Gender Bias in Relation Extraction","tldr":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literatu...","track":"Ethics and NLP"},"forum":"main.265","id":"main.265","presentation_id":"38929244"},{"card_image_alt_text":"A representative figure from paper main.264","card_image_path":"static/images/papers/main.264.png","content":{"abstract":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models' top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","authors":["Shengyu Jia","Tao Meng","Jieyu Zhao","Kai-Wei Chang"],"demo_url":"","keywords":["Mitigating Amplification","natural processing","gender issue","Posterior Regularization"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.264.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.264","similar_paper_uids":["main.264","main.468","main.326","main.773","main.262"],"title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","tldr":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. Howe...","track":"Ethics and NLP"},"forum":"main.264","id":"main.264","presentation_id":"38928917"},{"card_image_alt_text":"A representative figure from paper main.260","card_image_path":"static/images/papers/main.260.png","content":{"abstract":"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.","authors":["Jieyu Zhao","Subhabrata Mukherjee","Saghar Hosseini","Kai-Wei Chang","Ahmed Hassan Awadallah"],"demo_url":"","keywords":["cross-lingual transfer","multilingual embeddings","NLP applications","bias analysis"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.260.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.260","similar_paper_uids":["main.260","main.468","main.264","main.421","main.493"],"title":"Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer","tldr":"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cr...","track":"Ethics and NLP"},"forum":"main.260","id":"main.260","presentation_id":"38928955"},{"card_image_alt_text":"A representative figure from paper main.261","card_image_path":"static/images/papers/main.261.png","content":{"abstract":"As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.","authors":["Kobi Leins","Jey Han Lau","Timothy Baldwin"],"demo_url":"","keywords":["NLP research","automatic sentencing","ethically research","NLP"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.261.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.261","similar_paper_uids":["main.261","main.506","main.332","demo.104","main.699"],"title":"Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?","tldr":"As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with r...","track":"Ethics and NLP"},"forum":"main.261","id":"main.261","presentation_id":"38929312"},{"card_image_alt_text":"A representative figure from paper main.263","card_image_path":"static/images/papers/main.263.png","content":{"abstract":"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.","authors":["Samson Tan","Shafiq Joty","Min-Yen Kan","Richard Socher"],"demo_url":"","keywords":["Linguistic Discrimination","Inflectional Perturbations","pre-trained networks","NLP models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.263.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.263","similar_paper_uids":["main.263","main.383","main.357","main.197","main.191"],"title":"It\u2019s Morphin\u2019 Time! Combating Linguistic Discrimination with Inflectional Perturbations","tldr":"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We ...","track":"Ethics and NLP"},"forum":"main.263","id":"main.263","presentation_id":"38928803"},{"card_image_alt_text":"A representative figure from paper main.262","card_image_path":"static/images/papers/main.262.png","content":{"abstract":"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences -- to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.","authors":["Kawin Ethayarajh"],"demo_url":"","keywords":["Classifier","Bernstein Bounds","classifiers","co-reference system"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.262.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 06:00:00 GMT","session_name":"6A","start_time":"Tue, 07 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.262","similar_paper_uids":["main.262","main.773","main.468","main.264","main.690"],"title":"Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds","tldr":"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protec...","track":"Ethics and NLP"},"forum":"main.262","id":"main.262","presentation_id":"38928838"},{"card_image_alt_text":"A representative figure from paper main.483","card_image_path":"static/images/papers/main.483.png","content":{"abstract":"Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models' inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.","authors":["Brendan Kennedy","Xisen Jin","Aida Mostafazadeh Davani","Morteza Dehghani","Xiang Ren"],"demo_url":"","keywords":["Contextualizing Classifiers","Post-hoc Explanation","Hate classifiers","fine-tuned classifiers"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.483.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.483","similar_paper_uids":["main.483","main.769","main.485","main.380","main.770"],"title":"Contextualizing Hate Speech Classifiers with Post-hoc Explanation","tldr":"Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to model...","track":"Ethics and NLP"},"forum":"main.483","id":"main.483","presentation_id":"38929143"},{"card_image_alt_text":"A representative figure from paper main.484","card_image_path":"static/images/papers/main.484.png","content":{"abstract":"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.","authors":["Tianlu Wang","Xi Victoria Lin","Nazneen Fatema Rajani","Bryan McCann","Vicente Ordonez","Caiming Xiong"],"demo_url":"","keywords":["Tailoring Embeddings","Gender Mitigation","Double-Hard Debias","downstream models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.484.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.484","similar_paper_uids":["main.484","main.690","main.773","main.619","main.488"],"title":"Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation","tldr":"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing proced...","track":"Ethics and NLP"},"forum":"main.484","id":"main.484","presentation_id":"38929050"},{"card_image_alt_text":"A representative figure from paper main.485","card_image_path":"static/images/papers/main.485.png","content":{"abstract":"We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.","authors":["Su Lin Blodgett","Solon Barocas","Hal Daum\u00e9 III","Hanna Wallach"],"demo_url":"","keywords":["NLP","NLP systems","normative reasoning","normative process"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.485.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.485","similar_paper_uids":["main.485","main.468","srw.18","main.486","main.260"],"title":"Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP","tldr":"We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that th...","track":"Ethics and NLP"},"forum":"main.485","id":"main.485","presentation_id":"38929425"},{"card_image_alt_text":"A representative figure from paper main.487","card_image_path":"static/images/papers/main.487.png","content":{"abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","authors":["Ben Hutchinson","Vinodkumar Prabhakaran","Emily Denton","Kellie Webster","Yu Zhong","Stephen Denuyl"],"demo_url":"","keywords":["toxicity prediction","NLP Models","equitable technologies","ML models"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.487.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.487","similar_paper_uids":["main.487","main.405","main.488","main.486","main.418"],"title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","tldr":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from t...","track":"Ethics and NLP"},"forum":"main.487","id":"main.487","presentation_id":"38929358"},{"card_image_alt_text":"A representative figure from paper main.486","card_image_path":"static/images/papers/main.486.png","content":{"abstract":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that \"we shouldn't lower our standards to hire more women,\" most listeners will infer the implicature intended by the speaker - that \"women (candidates) are less qualified.\" Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","authors":["Maarten Sap","Saadia Gabriel","Lianhui Qin","Dan Jurafsky","Noah A. Smith","Yejin Choi"],"demo_url":"","keywords":["Warning","large-scale evaluation","high-level categorization","Social Frames"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.486.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 19:00:00 GMT","session_name":"9B","start_time":"Tue, 07 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.486","similar_paper_uids":["main.486","srw.18","main.488","main.405","main.485"],"title":"Social Bias Frames: Reasoning about Social and Power Implications of Language","tldr":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather th...","track":"Ethics and NLP"},"forum":"main.486","id":"main.486","presentation_id":"38928840"},{"card_image_alt_text":"A representative figure from paper main.418","card_image_path":"static/images/papers/main.418.png","content":{"abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","authors":["Yang Trista Cao","Hal Daum\u00e9 III"],"demo_url":"","keywords":["Gender-Inclusive Resolution","interrogating annotations","coreference systems","systemic biases"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.418.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 22:00:00 GMT","session_name":"10B","start_time":"Tue, 07 Jul 2020 21:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.418","similar_paper_uids":["main.418","main.488","main.484","main.690","main.773"],"title":"Toward Gender-Inclusive Coreference Resolution","tldr":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans...","track":"Ethics and NLP"},"forum":"main.418","id":"main.418","presentation_id":"38929030"},{"card_image_alt_text":"A representative figure from paper main.380","card_image_path":"static/images/papers/main.380.png","content":{"abstract":"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., \"gay\", \"black\") are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like \"She makes me happy to be gay\" as abusive simply because of the word \"gay.\" In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models' generalization ability.","authors":["Guanhua Zhang","Bing Bai","Junqi Zhang","Kun Bai","Conghui Zhu","Tiejun Zhao"],"demo_url":"","keywords":["Mitigating Discrimination","Text Classifications","Discrimination","Instance Weighting"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.380.pdf","sessions":[{"end_time":"Tue, 07 Jul 2020 10:00:00 GMT","session_name":"7B","start_time":"Tue, 07 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Tue, 07 Jul 2020 14:00:00 GMT","session_name":"8B","start_time":"Tue, 07 Jul 2020 13:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.380","similar_paper_uids":["main.380","main.483","main.264","main.485","main.769"],"title":"Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting","tldr":"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., \"gay\", \"black\") ar...","track":"Ethics and NLP"},"forum":"main.380","id":"main.380","presentation_id":"38928723"}]
