[{"card_image_alt_text":"A representative figure from paper main.211","card_image_path":"static/images/papers/main.211.png","content":{"abstract":"Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments. Specifically, we \"occlude\" the majority of a document's text and add context-sensitive commands that reveal \"glimpses\" of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.","authors":["Xingdi Yuan","Jie Fu","Marc-Alexandre C\u00f4t\u00e9","Yi Tay","Chris Pal","Adam Trischler"],"demo_url":"","keywords":["Interactive Comprehension","real-world applications","web-level retrieval","question answering"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.211.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.211","similar_paper_uids":["main.211","main.361","main.413","main.321","main.662"],"title":"Interactive Machine Comprehension with Information Seeking Agents","tldr":"Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are stat...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.211","id":"main.211","presentation_id":"38929112"},{"card_image_alt_text":"A representative figure from paper main.210","card_image_path":"static/images/papers/main.210.png","content":{"abstract":"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.","authors":["Vivek Gupta","Maitrey Mehta","Pegah Nokhiz","Vivek Srikumar"],"demo_url":"","keywords":["INFOTABS","complex reasoning","modeling strategies","meaning fragments"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.210.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.210","similar_paper_uids":["main.210","main.115","main.130","main.441","main.586"],"title":"INFOTABS: Inference on Tables as Semi-structured Data","tldr":"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testin...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.210","id":"main.210","presentation_id":"38929232"},{"card_image_alt_text":"A representative figure from paper main.774","card_image_path":"static/images/papers/main.774.png","content":{"abstract":"We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise. We describe a direct scalar regression modeling approach, and find that existing categorically-labeled NLI data can be used in pre-training. Our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks.","authors":["Tongfei Chen","Zhengping Jiang","Adam Poliak","Keisuke Sakaguchi","Benjamin Van Durme"],"demo_url":"","keywords":["Uncertain Inference","Natural Inference","NLI","UNLI"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.774.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.774","similar_paper_uids":["main.774","main.768","srw.135","main.771","tacl.1780"],"title":"Uncertain Natural Language Inference","tldr":"We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. We demonstrate the fea...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.774","id":"main.774","presentation_id":"38929141"},{"card_image_alt_text":"A representative figure from paper main.548","card_image_path":"static/images/papers/main.548.png","content":{"abstract":"Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.","authors":["Jiemin Wu","Yanghui Rao","Zusheng Zhang","Haoran Xie","Qing Li","Fu Lee Wang","Ziye Chen"],"demo_url":"","keywords":["Dispersed Discovery","mining topics","Neural Models","Mixed models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.548.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.548","similar_paper_uids":["main.548","main.32","srw.5","main.694","main.73"],"title":"Neural Mixed Counting Models for Dispersed Topic Discovery","tldr":"Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the exis...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.548","id":"main.548","presentation_id":"38928980"},{"card_image_alt_text":"A representative figure from paper main.212","card_image_path":"static/images/papers/main.212.png","content":{"abstract":"Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model's limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT's accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.","authors":["Junghyun Min","R. Thomas McCoy","Dipanjan Das","Emily Pitler","Tal Linzen"],"demo_url":"","keywords":["Syntactic Augmentation","natural inference","natural NLI","NLI"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.212.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.212","similar_paper_uids":["main.212","main.429","main.190","main.303","main.309"],"title":"Syntactic Data Augmentation Increases Robustness to Inference Heuristics","tldr":"Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.212","id":"main.212","presentation_id":"38928832"},{"card_image_alt_text":"A representative figure from paper main.549","card_image_path":"static/images/papers/main.549.png","content":{"abstract":"Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.","authors":["Wanjun Zhong","Jingjing Xu","Duyu Tang","Zenan Xu","Nan Duan","Ming Zhou","Jiahai Wang","Jian Yin"],"demo_url":"","keywords":["Reasoning Graph","Fact Checking","string concatenation","semantic labeling"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.549.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.549","similar_paper_uids":["main.549","main.655","main.761","main.97","main.539"],"title":"Reasoning Over Semantic-Level Graph for Fact Checking","tldr":"Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.549","id":"main.549","presentation_id":"38928866"},{"card_image_alt_text":"A representative figure from paper main.773","card_image_path":"static/images/papers/main.773.png","content":{"abstract":"While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.","authors":["Xiang Zhou","Mohit Bansal"],"demo_url":"","keywords":["Natural Inference","data augmentation","Robustifying Models","deep models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.773.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.773","similar_paper_uids":["main.773","main.468","main.262","main.264","main.769"],"title":"Towards Robustifying NLI Models Against Lexical Dataset Biases","tldr":"While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.773","id":"main.773","presentation_id":"38929270"},{"card_image_alt_text":"A representative figure from paper main.772","card_image_path":"static/images/papers/main.772.png","content":{"abstract":"Question-answering (QA) data often encodes essential information in many facets. This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the question-answer driven sentence encoding (QuASE) framework. QuASE learns representations from QA data, using BERT or other state-of-the-art contextual language models. In particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single- or multi-sentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks. This work may point out an alternative way to supervise NLP tasks.","authors":["Hangfeng He","Qiang Ning","Dan Roth"],"demo_url":"","keywords":["named recognition","NLP tasks","QuASE","QAMR"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.772.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.772","similar_paper_uids":["main.772","main.413","main.20","main.19","main.600"],"title":"QuASE: Question-Answer Driven Sentence Encoding","tldr":"Question-answering (QA) data often encodes essential information in many facets. This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.772","id":"main.772","presentation_id":"38929339"},{"card_image_alt_text":"A representative figure from paper main.770","card_image_path":"static/images/papers/main.770.png","content":{"abstract":"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.","authors":["Prasetya Ajie Utama","Nafise Sadat Moosavi","Iryna Gurevych"],"demo_url":"","keywords":["Debiasing Models","natural tasks","NLU tasks","debiasing methods"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.770.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.770","similar_paper_uids":["main.770","main.769","main.773","main.690","main.704"],"title":"Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance","tldr":"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be ve...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.770","id":"main.770","presentation_id":"38929067"},{"card_image_alt_text":"A representative figure from paper main.771","card_image_path":"static/images/papers/main.771.png","content":{"abstract":"The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model's internal decision making. In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation. We demonstrate NILE's effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations. Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed. We discuss the faithfulness of NILE's explanations in terms of sensitivity of the decisions to the corresponding explanations. We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model's explanations. Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.","authors":["Sawan Kumar","Partha Talukdar"],"demo_url":"","keywords":["Natural Inference","NLP tasks","internal making","NLI"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.771.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.771","similar_paper_uids":["main.771","main.494","main.382","main.491","main.387"],"title":"NILE : Natural Language Inference with Faithful Natural Language Explanations","tldr":"The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explan...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.771","id":"main.771","presentation_id":"38929362"},{"card_image_alt_text":"A representative figure from paper main.657","card_image_path":"static/images/papers/main.657.png","content":{"abstract":"The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement. We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks. This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection.","authors":["Deborah Ferreira","Andr\u00e9 Freitas"],"demo_url":"","keywords":["Premise Selection","complex problems","semantically task","natural processing"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.657.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.657","similar_paper_uids":["main.657","main.224","main.67","main.549","main.210"],"title":"Premise Selection in Natural Language Mathematical Texts","tldr":"The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The natural language premise selection ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.657","id":"main.657","presentation_id":"38929402"},{"card_image_alt_text":"A representative figure from paper main.656","card_image_path":"static/images/papers/main.656.png","content":{"abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process -- generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","authors":["Pepa Atanasova","Jakob Grue Simonsen","Christina Lioma","Isabelle Augenstein"],"demo_url":"","keywords":["Generating Explanations","automated checking","predicting claims","generating justifications"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.656.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.656","similar_paper_uids":["main.656","main.761","main.97","main.771","main.494"],"title":"Generating Fact Checking Explanations","tldr":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puz...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.656","id":"main.656","presentation_id":"38929094"},{"card_image_alt_text":"A representative figure from paper main.655","card_image_path":"static/images/papers/main.655.png","content":{"abstract":"Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT's effectiveness. All source codes of this work are available at https://github.com/thunlp/KernelGAT.","authors":["Zhenghao Liu","Chenyan Xiong","Maosong Sun","Zhiyuan Liu"],"demo_url":"","keywords":["Fine-grained Verification","Fact Verification","fine-grained propagation","Kernel Network"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.655.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.655","similar_paper_uids":["main.655","main.549","main.761","main.97","main.572"],"title":"Fine-grained Fact Verification with Kernel Graph Attention Network","tldr":"Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.655","id":"main.655","presentation_id":"38928682"},{"card_image_alt_text":"A representative figure from paper main.679","card_image_path":"static/images/papers/main.679.png","content":{"abstract":"Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.","authors":["Mostafa Abdou","Vinit Ravishankar","Maria Barrett","Yonatan Belinkov","Desmond Elliott","Anders S\u00f8gaard"],"demo_url":"","keywords":["human understanding","Language Models","Winograd Perturbations","Large-scale models"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.679.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.679","similar_paper_uids":["main.679","main.769","main.244","main.690","main.457"],"title":"The Sensitivity of Language Models and Humans to Winograd Schema Perturbations","tldr":"Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, t...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.679","id":"main.679","presentation_id":"38929042"},{"card_image_alt_text":"A representative figure from paper main.678","card_image_path":"static/images/papers/main.678.png","content":{"abstract":"Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.","authors":["Ben Zhou","Qiang Ning","Daniel Khashabi","Dan Roth"],"demo_url":"","keywords":["Temporal Acquisition","human annotation","temporal NLP","Minimal Supervision"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.678.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.678","similar_paper_uids":["main.678","main.680","main.95","main.423","main.668"],"title":"Temporal Common Sense Acquisition with Minimal Supervision","tldr":"Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.678","id":"main.678","presentation_id":"38929393"},{"card_image_alt_text":"A representative figure from paper main.768","card_image_path":"static/images/papers/main.768.png","content":{"abstract":"Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by \"some\" as entailments. For some presupposition triggers like \"only\", BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.","authors":["Paloma Jeretic","Alex Warstadt","Suvrat Bhooshan","Adina Williams"],"demo_url":"","keywords":["Natural inference","NLI","natural understanding","pragmatic inferences"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.768.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.768","similar_paper_uids":["main.768","main.479","tacl.1780","srw.135","main.543"],"title":"Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition","tldr":"Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudie...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.768","id":"main.768","presentation_id":"38929367"},{"card_image_alt_text":"A representative figure from paper main.542","card_image_path":"static/images/papers/main.542.png","content":{"abstract":"With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.","authors":["Benfeng Xu","Licheng Zhang","Zhendong Mao","Quan Wang","Hongtao Xie","Yongdong Zhang"],"demo_url":"","keywords":["Curriculum Learning","Natural Understanding","natural tasks","NLU tasks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.542.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.542","similar_paper_uids":["main.542","main.63","main.517","main.111","main.197"],"title":"Curriculum Learning for Natural Language Understanding","tldr":"With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a co...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.542","id":"main.542","presentation_id":"38929322"},{"card_image_alt_text":"A representative figure from paper main.543","card_image_path":"static/images/papers/main.543.png","content":{"abstract":"Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.","authors":["Hitomi Yanaka","Koji Mineshima","Daisuke Bekki","Kentaro Inui"],"demo_url":"","keywords":["Systematicity Inference","inferences","generalization composition","monotonicity inferences"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.543.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.543","similar_paper_uids":["main.543","main.177","main.158","main.768","main.479"],"title":"Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?","tldr":"Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn s...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.543","id":"main.543","presentation_id":"38928821"},{"card_image_alt_text":"A representative figure from paper main.769","card_image_path":"static/images/papers/main.769.png","content":{"abstract":"Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models' predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.","authors":["Rabeeh Karimi Mahabadi","Yonatan Belinkov","James Henderson"],"demo_url":"","keywords":["End-to-End Mitigation","real-world scenarios","training","large-scale benchmarks"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.769.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 21:00:00 GMT","session_name":"15A","start_time":"Wed, 08 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.769","similar_paper_uids":["main.769","main.770","main.773","main.522","main.586"],"title":"End-to-End Bias Mitigation by Modelling Biases in Corpora","tldr":"Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.769","id":"main.769","presentation_id":"38929068"},{"card_image_alt_text":"A representative figure from paper main.541","card_image_path":"static/images/papers/main.541.png","content":{"abstract":"Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse. We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects. First, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world StackOverflow posts. Second, to obtain linguistically diverse natural language descriptions, we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see, rather than having them paraphrase synthetic language. Third, we augment each regex example with a collection of strings that are and are not matched by the ground truth regex, similar to how real users give examples. Our quantitative and qualitative analysis demonstrates the advantages of StructuredRegex over prior datasets. Further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset, including non-local constraints and multi-modal inputs.","authors":["Xi Ye","Qiaochu Chen","Isil Dillig","Greg Durrett"],"demo_url":"","keywords":["Multimodal Synthesis","regular generation","regex tasks","StackOverflow"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.541.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 19:00:00 GMT","session_name":"14B","start_time":"Wed, 08 Jul 2020 18:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.541","similar_paper_uids":["main.541","main.587","main.168","main.18","main.644"],"title":"Benchmarking Multimodal Regex Synthesis with Complex Structures","tldr":"Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.541","id":"main.541","presentation_id":"38929291"},{"card_image_alt_text":"A representative figure from paper main.544","card_image_path":"static/images/papers/main.544.png","content":{"abstract":"Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation. To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts. Our approach works in an encoderdecoder manner and is equipped with Vector Quantised-Variational Autoencoder, where the encoder outputs representations from a distribution over discrete variables. Such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation. Our approach provides state-of-the-art performance on both Event2mind and Atomic datasets. More importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts.","authors":["Daya Guo","Duyu Tang","Nan Duan","Jian Yin","Daxin Jiang","Ming Zhou"],"demo_url":"","keywords":["Evidence-Aware Generation","Generating texts","generation","generation texts"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.544.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.544","similar_paper_uids":["main.544","main.549","main.65","srw.42","main.97"],"title":"Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder","tldr":"Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent sema...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.544","id":"main.544","presentation_id":"38928852"},{"card_image_alt_text":"A representative figure from paper main.545","card_image_path":"static/images/papers/main.545.png","content":{"abstract":"Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human's paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder. On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG. Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge.","authors":["Xin Jia","Wenjie Zhou","Xu Sun","Yunfang Wu"],"demo_url":"","keywords":["question generation(QG","sentence-level generation","diversity training","Paraphrases"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.545.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 10:00:00 GMT","session_name":"12B","start_time":"Wed, 08 Jul 2020 09:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.545","similar_paper_uids":["main.545","main.69","main.535","main.22","main.500"],"title":"How to Ask Good Questions? Try to Leverage Paraphrases","tldr":"Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human's paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.545","id":"main.545","presentation_id":"38928935"},{"card_image_alt_text":"A representative figure from paper main.547","card_image_path":"static/images/papers/main.547.png","content":{"abstract":"Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.","authors":["Lu Chen","Yanbin Zhao","Boer Lv","Lesheng Jin","Zhi Chen","Su Zhu","Kai Yu"],"demo_url":"","keywords":["Chinese Matching","Chinese segmentation","matching","Neural Networks"],"paper_type":"Short","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.547.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.547","similar_paper_uids":["main.547","main.274","main.224","main.315","main.712"],"title":"Neural Graph Matching Networks for Chinese Short Text Matching","tldr":"Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performa...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.547","id":"main.547","presentation_id":"38929442"},{"card_image_alt_text":"A representative figure from paper main.209","card_image_path":"static/images/papers/main.209.png","content":{"abstract":"Open Information Extraction systems extract (\u201csubject text\u201d, \u201crelation text\u201d, \u201cobject text\u201d) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (\u201csubject text\u201d, \u201crelation text\u201d, ?) questions. An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained. For example, facts can appear in different paraphrased textual variants, which can lead to test leakage. To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench. We performed experiments with a prototypical knowledge graph embedding model for openlink prediction. While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.","authors":["Samuel Broscheit","Kiril Gashteovski","Yanjie Wang","Rainer Gemulla"],"demo_url":"","keywords":["Open Prediction","open task","predicting facts","openlink prediction"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.209.pdf","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.209","similar_paper_uids":["main.209","main.649","main.515","main.546","main.717"],"title":"Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction","tldr":"Open Information Extraction systems extract (\u201csubject text\u201d, \u201crelation text\u201d, \u201cobject text\u201d) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.209","id":"main.209","presentation_id":"38929433"},{"card_image_alt_text":"A representative figure from paper main.546","card_image_path":"static/images/papers/main.546.png","content":{"abstract":"Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. However, researchers have mainly poured attention to knowledge inference on binary facts. The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world. Therefore, this paper addresses knowledge inference on n-ary facts. We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s). We further propose a neural network model, NeuInfer, for knowledge inference on n-ary facts. Besides handling the common task to infer an unknown element in a whole fact, NeuInfer can cope with a new type of task, flexible knowledge inference. It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s). Experimental results demonstrate the remarkable superiority of NeuInfer.","authors":["Saiping Guan","Xiaolong Jin","Jiafeng Guo","Yuanzhuo Wang","Xueqi Cheng"],"demo_url":"","keywords":["Knowledge Inference","NeuInfer","neural model","N-ary Facts"],"paper_type":"Long","pdf_url":"https://www.aclweb.org/anthology/2020.acl-main.546.pdf","sessions":[{"end_time":"Wed, 08 Jul 2020 06:00:00 GMT","session_name":"11A","start_time":"Wed, 08 Jul 2020 05:00:00 GMT","zoom_link":""},{"end_time":"Wed, 08 Jul 2020 13:00:00 GMT","session_name":"13A","start_time":"Wed, 08 Jul 2020 12:00:00 GMT","zoom_link":""}],"share_url":"https://www.aclweb.org/anthology/2020.acl-main.546","similar_paper_uids":["main.546","main.515","main.572","main.17","main.209"],"title":"NeuInfer: Knowledge Inference on N-ary Facts","tldr":"Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. However, researchers have m...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"main.546","id":"main.546","presentation_id":"38928724"},{"card_image_alt_text":"A representative figure from paper tacl.1780","card_image_path":"static/images/papers/tacl.1780.png","content":{"abstract":"We analyze human\u2019s disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation \u201cnoise\u201d, but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective which requires models to explicitly capture the full distribution of plausible human judgments.","authors":["Ellie Pavlick","Tom Kwiatkowski"],"demo_url":"","keywords":["Human Inferences","natural inferences","natural inference","evaluation objective"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00293","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00293","similar_paper_uids":["tacl.1780","main.768","tacl.1915","main.220","main.173"],"title":"Inherent Disagreements in Human Textual Inferences","tldr":"We analyze human\u2019s disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation \u201cnoise\u201d, but rather persist as we collect more ratings and as we vary the amount of context ...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"tacl.1780","id":"tacl.1780","presentation_id":"38929492"},{"card_image_alt_text":"A representative figure from paper tacl.1720","card_image_path":"static/images/papers/tacl.1720.png","content":{"abstract":"We present a novel semantic framework for modeling linguistic expressions of generalization\u2014 generic, habitual, and episodic statements\u2014as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information\u2014including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings\u2014for predicting expressions of generalization.","authors":["Venkata Subrahmanyan Govindarajan","Benjamin Van Durme","Aaron Steven White"],"demo_url":"","keywords":["linguistic generalization\u2014","predicting generalization","expressions generalization","Decomposing Generalization"],"paper_type":"TACL","pdf_url":"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00285","sessions":[{"end_time":"Mon, 06 Jul 2020 18:00:00 GMT","session_name":"4A","start_time":"Mon, 06 Jul 2020 17:00:00 GMT","zoom_link":""},{"end_time":"Mon, 06 Jul 2020 21:00:00 GMT","session_name":"5A","start_time":"Mon, 06 Jul 2020 20:00:00 GMT","zoom_link":""}],"share_url":"https://www.mitpressjournals.org/doi/10.1162/tacl_a_00285","similar_paper_uids":["tacl.1720","main.236","main.434","main.552","main.431"],"title":"Decomposing Generalization: Models of Generic, Habitual and Episodic Statements","tldr":"We present a novel semantic framework for modeling linguistic expressions of generalization\u2014 generic, habitual, and episodic statements\u2014as combinations of simple, real-valued referential properties of predicates and their arguments. We use this frame...","track":"Semantics: Textual Inference and Other Areas of Semantics"},"forum":"tacl.1720","id":"tacl.1720","presentation_id":"38929485"}]
